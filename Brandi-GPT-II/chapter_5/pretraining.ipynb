{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc788ee9",
   "metadata": {},
   "source": [
    "# Pretraining On Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61eb852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "# Add the parent directory to sys.path so chapter_4 can be imported\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from chapter_4.chapter04 import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "688e783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 small\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of transformer blocks\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "376f1ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c696ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from chapter_4.chapter04 import generate_text_sample\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # unsqueeze adds batch dimensions\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_sample(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6a8f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example inputs and targets\n",
    "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\",\n",
    "                      [40, 1107, 588]])      # \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],      # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])    # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "012c9dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Feed examples into model to get probilities\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af258b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "# greedily pick next token\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True) # keepdim retains number of dimensions. False squeezes dim \n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2c41f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs back into text\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0c50822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "# Print initial softmax probabilities\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3212f",
   "metadata": {},
   "source": [
    "## Calculate Loss\n",
    "logits -> probabilities -> target probabilities -> log probabilities -> average log probability -> negative average log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "937c0a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9921bda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17833af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas) # Easier to push negative log down to 1 with gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a54160",
   "metadata": {},
   "source": [
    "Note: Cross Entropy Loss measures the difference between two probability distributions\n",
    "    cross entropy for discrete outcomes is synonymous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e302770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Target shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Target shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77847872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Flatten over batch for cross entropy\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b568689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9e844",
   "metadata": {},
   "source": [
    "Note: Perplexity measures how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset.\n",
    "\n",
    "The number represents how many of the tokens in the vocabulary it is unsure about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddf2d58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c54764",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c4dad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The Verdict\n",
    "file_path = \"../the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e69d4da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# Get stats\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00d346be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training split\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "221fb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "from chapter_2.chapter02 import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    txt=train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    txt=val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0c40e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Double check loaders\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb0efe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the cross entropy loss of a given batch\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b272ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute training and validation loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        # Get loss over each batch\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    # Average loss per batch\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bfcecf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583690219456\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss over training and validation data loaders\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56290375",
   "metadata": {},
   "source": [
    "## Training an LLM\n",
    "iterate over training epochs --> iterate over batches in each training epoch --> reset loss gradients from previous batch iteration --> calculate loss on current batch --> backward pass to calculate loss gradients --> update model weights using loss gradients --> print training and validation set losses --> generate sample text for visual inspection --> beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cbf167a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c1fa6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_sample(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42461dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement training flow\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                        optimizer, device, num_epochs,\n",
    "                        eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize tracking\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # reset loss gradients\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward() # calculate loss gradients\n",
    "            optimizer.step() # updates model weights using loss grads\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validate\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        \n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91a569",
   "metadata": {},
   "source": [
    "Note: AdamW improves weight decay to minimize model complexity and prevent overfitting by penalizing larger weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a small training\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=0.0004,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51d69a59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     fig.tight_layout()\n\u001b[32m     18\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m epochs_tensor = torch.linspace(\u001b[32m0\u001b[39m, \u001b[43mnum_epochs\u001b[49m, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[32m     21\u001b[39m plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
      "\u001b[31mNameError\u001b[39m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f28ae7",
   "metadata": {},
   "source": [
    "For a larger dataset, see the Project Gutenberg with 60,000 public domain books\n",
    "\n",
    "Note: It is common to train for 1 epoch with very large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae3702",
   "metadata": {},
   "source": [
    "## Decoding (Generation) Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c3f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to cpu if it was one GPU and turn off training\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the ax.\n",
      "\n",
      "\"I had the last word.\n",
      "\n",
      "\n",
      "\n",
      "\"I didn't\n"
     ]
    }
   ],
   "source": [
    "# Put model through text generation\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_sample(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619ad5d",
   "metadata": {},
   "source": [
    "### Temperature Scaling\n",
    "Adds a probabilistic selection process to the next-token generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8513bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vocab\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example logits\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e6cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Greedily select\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac7f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Implement probabilistic sampling process\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc79845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "# Illustrate multinomial sampling\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "              for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample)) # counts the frequency of each value\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlling the distribution with temperature scaling\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0acec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Illustrate distributions\n",
    "temperatures = [5, 1, 0.1]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                                          for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2186767",
   "metadata": {},
   "source": [
    "### Top-k Sampling\n",
    "Restricts the sampled tokens to the top-k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7bb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# Get top_k probability tokens\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits) # descending order\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# Set all lower probability tokens to -inf\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1], # logits less than  the minimum of the top 3\n",
    "    input=torch.tensor(float('-inf')), # assign -inf to lower logits\n",
    "    other=next_token_logits # retrain original logits for top tokens\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Get new token probability\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b764e",
   "metadata": {},
   "source": [
    "## Better Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85446c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d43f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you. Of work up surprise. It is to face watching me by his knees\n"
     ]
    }
   ],
   "source": [
    "# Example text generation \n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd46da",
   "metadata": {},
   "source": [
    "## Loading and Saving Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ecde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save state_dict to file\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebde81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load state_dict\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()\n",
    "# Should save optimizer state as well to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model and optimizer\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a687c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restoring model and optimizer states\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554142bc",
   "metadata": {},
   "source": [
    "## Loading Pretrained OpenAI Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e6a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\Code\\Brandi-GPT-II\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f96e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x27433ecdbb0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download downloader from repo\n",
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b917e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: ../gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: ../gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: ../gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: ../gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: ../gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Get architecture setting and params\n",
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"../gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "# Inspect setting and parameters\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys()) # holds weight tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "# View weights\n",
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model size configs\n",
    "# Each weights are open sourced\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024}) # Change from 256 \n",
    "NEW_CONFIG.update({\"qkv_bias\": True}) # usually not used, but is for these weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ddf73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model instance\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight tensor loading function\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                         f\"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ab06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full weight loading function\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].attn.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].attn.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].attn.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].attn.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239ebe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use loaded weights\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0af80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " I beieve God exists because he did not destroy it. You don't have to accept it as such at all. There is very little if any that you get in exchange for one of its elements. Therefore that they are a sign that the world would not change.\" The reason I was intrigued then was that my teacher said things to him of a sort we'd heard all, and while many people had claimed he might have done quite simply to protect her, he hadn't said what he might have sought to suggest in order to prove it on such a grand scale that such a miracle would ever occur. For a long time I just couldn't take on feeling the same urge that he had. I found it quite comforting to believe such a thing though what else we might find out about something much greater can probably serve as testimony from our personal memory to the point one finds yourself compelled to question. After about another 10 minutes when he looked back, some sort of sudden sense of utter horror gripped me and I let my stomach sink down\n"
     ]
    }
   ],
   "source": [
    "# Generating text with pretrained model\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"I beieve God exists because\", tokenizer).to(device),\n",
    "    max_new_tokens=200,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1755220",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate gpt 2 on small dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m evaluate_model(gpt, \u001b[43mtrain_loader\u001b[49m, val_loader, device, \u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate gpt 2 on small dataset\n",
    "evaluate_model(gpt, train_loader, val_loader, device, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brandi-GPT-II",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
