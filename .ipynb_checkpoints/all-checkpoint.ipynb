{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f156432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.adViT.critic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdversarialVisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vit_encoder: nn.Module,\n",
    "            z_dim: int | None = None,  # proposal dimension\n",
    "            c_dim: int | None = None,  # context dimension\n",
    "            hidden_dim: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vit = vit_encoder\n",
    "        self.z_dim = z_dim\n",
    "        self.c_dim = c_dim\n",
    "\n",
    "        embed_dim = self.vit.c_token.size(-1)\n",
    "\n",
    "        # Total feature dimension (one long vector)\n",
    "        in_dim = embed_dim\n",
    "        if z_dim is not None:\n",
    "            in_dim += z_dim\n",
    "        if c_dim is not None:\n",
    "            in_dim += c_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            I_in: torch.Tensor,        # (B, C_in, H, W)\n",
    "            O_pred: torch.Tensor,      # (B, C_out, H, W) or (B, T, C_out, H, W)\n",
    "            mask_in: torch.Tensor | None = None,\n",
    "            mask_out: torch.Tensor | None = None,\n",
    "            z: torch.Tensor | None = None,\n",
    "            C: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   P = num proposals         #\n",
    "        #   C_in = input channels     #\n",
    "        #   C_out = output channels   #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "\n",
    "        B, C_in, H, W = I_in.shape\n",
    "\n",
    "        #################################\n",
    "        #   MULTI-PROPOSAL BRANCH       #\n",
    "        #################################\n",
    "\n",
    "        if O_pred.dim() == 5:\n",
    "            B, T, C_out, H, W = O_pred.shape\n",
    "\n",
    "            # Expand inputs\n",
    "            I_exp = I_in.unsqueeze(1).expand(B, T, C_in, H, W)\n",
    "            I_flat = I_exp.reshape(B*T, C_in, H, W)\n",
    "            O_flat = O_pred.reshape(B*T, C_out, H, W)\n",
    "\n",
    "            # Convert O_flat to 1 channel if needed\n",
    "            if O_flat.size(1) > 1:\n",
    "                O_flat = torch.argmax(O_flat, dim=1, keepdim=True).float()\n",
    "\n",
    "            # Combine masks\n",
    "            if mask_in is not None or mask_out is not None:\n",
    "                if mask_in is None:\n",
    "                    mask_in = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "                if mask_out is None:\n",
    "                    mask_out = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "                mask = torch.logical_or(mask_in, mask_out)\n",
    "                mask = mask.unsqueeze(1).expand(B, T, H, W).reshape(B*T, H, W)\n",
    "            else:\n",
    "                mask = None\n",
    "\n",
    "            # Concatenate input + output\n",
    "            x = torch.cat([I_flat, O_flat], dim=1)  # (B*T, 2, H, W)\n",
    "\n",
    "            # Encode with ViT\n",
    "            h_flat = self.vit.forward_grid(x, mask=mask)\n",
    "            h = h_flat.view(B, T, -1)\n",
    "\n",
    "            # Collect features\n",
    "            feats = [h]\n",
    "\n",
    "            if z is not None:\n",
    "                if z.dim() == 2:\n",
    "                    z = z.unsqueeze(1).expand(B, T, -1)\n",
    "                feats.append(z)\n",
    "\n",
    "            if C is not None:\n",
    "                feats.append(C.unsqueeze(1).expand(B, T, -1))\n",
    "\n",
    "            feat = torch.cat(feats, dim=-1)\n",
    "            return self.mlp(feat).squeeze(-1)\n",
    "\n",
    "        #################################\n",
    "        #   SINGLE-PROPOSAL BRANCH      #\n",
    "        #################################\n",
    "\n",
    "        B, C_out, H, W = O_pred.shape\n",
    "\n",
    "        # Convert O_pred to 1 channel if needed\n",
    "        if O_pred.size(1) > 1:          # multi-channel class logits\n",
    "            classes = torch.arange(O_pred.size(1), device=O_pred.device).view(1, -1, 1, 1)\n",
    "            probs = O_pred.softmax(dim=1)\n",
    "            O_pred = (probs * classes).sum(dim=1, keepdim=True)   # differentiable\n",
    "\n",
    "\n",
    "        # Combine masks\n",
    "        if mask_in is not None or mask_out is not None:\n",
    "            if mask_in is None:\n",
    "                mask_in = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "            if mask_out is None:\n",
    "                mask_out = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "            mask = torch.logical_or(mask_in, mask_out)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # Concatenate input + output\n",
    "        x = torch.cat([I_in, O_pred], dim=1)  # (B, 2, H, W)\n",
    "\n",
    "        # Encode with ViT\n",
    "        h = self.vit.forward_grid(x, mask=mask)\n",
    "\n",
    "        # Combine features for MLP\n",
    "        feats = [h]\n",
    "        if z is not None:\n",
    "            feats.append(z)\n",
    "        if C is not None:\n",
    "            feats.append(C)\n",
    "        feat = torch.cat(feats, dim=-1)\n",
    "\n",
    "        return self.mlp(feat).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.context_encoding.example_pair_encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ExamplePairEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encods single example pair (I_i, O_i) into vector h_i\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vit_pair: nn.Module\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vit = vit_pair # generic\n",
    "        self.norm = nn.LayerNorm(self.vit.c_token.size(-1)) # normalize h_i\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            I_i: torch.Tensor,\n",
    "            O_i: torch.Tensor,\n",
    "            mask_I: torch.Tensor,\n",
    "            mask_O: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        B, _, H, W = I_i.shape\n",
    "        \n",
    "        # Concatenate input-output as different channels\n",
    "        x = torch.cat([I_i, O_i], dim=1)\n",
    "\n",
    "        # Combine masks\n",
    "        mask = torch.logical_or(mask_I, mask_O)\n",
    "        key_padded_mask = ~mask\n",
    "\n",
    "        # Pass through ViT for context embedding\n",
    "        h_i = self.vit.forward_grid(x, mask=key_padded_mask)  # (B, embed_dim)\n",
    "\n",
    "        # Normalize\n",
    "        h_i = self.norm(h_i)\n",
    "\n",
    "        return h_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714752eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.context_encoding.example_pair_aggregator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ExamplePairAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregates the context vectors from k example pairs (I_i, O_i) into a single embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            embed_dim: int,\n",
    "            hidden_dim: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # MLP to weigh context vectors\n",
    "        self.score_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.Tanh(), # [-1,1]\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Normalize context vector\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            h: torch.Tensor,  # all embeddings \n",
    "            mask: torch.Tensor | None = None  # where the tokens are valid\n",
    "    ) -> torch.Tensor:\n",
    "        ###############################\n",
    "        #   B = batch size            #\n",
    "        #   K = num example pairs     #\n",
    "        #   D = embedding dimension   #\n",
    "        ###############################\n",
    "\n",
    "        B, K, D = h.shape\n",
    "\n",
    "        # Infer scores\n",
    "        scores = self.score_mlp(h)\n",
    "\n",
    "        # Mask = where the tokens are\n",
    "        if mask is not None:\n",
    "\n",
    "            # Ensure boolean\n",
    "            mask = mask.to(dtype=torch.bool)\n",
    "\n",
    "            # Match to score shape\n",
    "            mask_expanded = mask.unsqueeze(-1) # (B, K, 1)\n",
    "\n",
    "            # Set where the mask is not to very negative\n",
    "            scores = scores.masked_fill(~mask_expanded, float(\"-inf\"))\n",
    "\n",
    "        # Attention weights over k example pairs\n",
    "        attn = F.softmax(scores, dim=1)\n",
    "\n",
    "        # Weighted sum\n",
    "        C = torch.sum(attn * h, dim=1)\n",
    "\n",
    "        # Final normalization\n",
    "        C = self.norm(C)\n",
    "\n",
    "        return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab898f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.context_encoding.conditional_encoder.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConditionalTestInputEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vit_test: nn.Module\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   D = token embedding dim   #\n",
    "        #   S = num tokens            #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "\n",
    "        super().__init__()\n",
    "        self.vit = vit_test\n",
    "        self.embed_dim = self.vit.c_token.size(-1)\n",
    "\n",
    "        # Project context vector to embedding dim\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            I_test,  # (B, 1, H, W)\n",
    "            mask_test,  # (B, H, W) or None\n",
    "            C  # (B, D)\n",
    "    ):\n",
    "\n",
    "        B, _, H, W = I_test.shape\n",
    "\n",
    "        ######################\n",
    "        #   Encode with ViT  #\n",
    "        ######################\n",
    "\n",
    "        tokens = self.vit.patch_embedding(I_test)  # (B, S, D)\n",
    "\n",
    "        #######################\n",
    "        #   Build Test Mask   #\n",
    "        #######################\n",
    "\n",
    "        if mask_test is not None:\n",
    "            flat_mask = mask_test.reshape(B, -1)  # (B, S)\n",
    "            key_padding_mask = ~flat_mask         # True = pad\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "        \n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "        ##########################\n",
    "        #   Add Context Vector   #\n",
    "        ##########################\n",
    "\n",
    "        C_token = self.c_proj(C).unsqueeze(1)  # (B,1,D)\n",
    "        tokens = torch.cat([C_token, tokens], dim=1)  # (B,1+S,D)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # Add context to mask\n",
    "            c_pad = torch.zeros(B, 1, dtype=torch.bool, device=key_padding_mask.device)\n",
    "            key_padding_mask = torch.cat([c_pad, key_padding_mask], dim=1)  # (B,1+S)\n",
    "\n",
    "        #####################################\n",
    "        #   Positional Encoding + Dropout   #\n",
    "        #####################################\n",
    "\n",
    "        tokens = self.vit.pos_encoding(tokens)\n",
    "        tokens = self.vit.dropout(tokens)\n",
    "\n",
    "        return tokens, key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.attention.py\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "\n",
    "        out, _ = self.attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.CNNBlock\n",
    "\n",
    "import torch.nn as nn\n",
    "from src.architecture.executor.FiLM import FiLM\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels: int,\n",
    "            z_dim: int | None = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            channels,  # in\n",
    "            channels,  # out\n",
    "            kernel_size=3, \n",
    "            padding=1\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(channels, channels)  # normalize each channel with respect to itself\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self.film = FiLM(channels, z_dim) if z_dim is not None else None\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            z=None\n",
    "    ):\n",
    "        # Feature extraction\n",
    "        x = self.norm(self.conv(x))\n",
    "\n",
    "        # Proposed feature modulation\n",
    "        if self.film is not None:\n",
    "            x = self.film(x, z)\n",
    "\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.executor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.architecture.executor.CNNBlock import CNNBlock\n",
    "from src.architecture.ViT.body import TransformerEncoderBlock\n",
    "\n",
    "\n",
    "# Hybrid ViT and CNN\n",
    "class Executor(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a latent transformation z to an input grid\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            mlp_dim,\n",
    "            depth,\n",
    "            z_dim,\n",
    "            hidden_channels=64,\n",
    "            num_classes=10  # ARC colors\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        ############################\n",
    "        #   CNN Feature Enricher   #\n",
    "        ############################\n",
    "\n",
    "        self.enricher = nn.Sequential(\n",
    "            nn.Conv2d(1, hidden_channels, 3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        ######################################\n",
    "        #   CNN Proposal Feature Detection   #\n",
    "        ######################################\n",
    "\n",
    "        self.cnn_blocks = nn.ModuleList([\n",
    "            CNNBlock(hidden_channels, z_dim=z_dim)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        ##################\n",
    "        #   Tokenizers   #\n",
    "        ##################\n",
    "\n",
    "        self.to_embedding = nn.Linear(hidden_channels, embed_dim)\n",
    "\n",
    "        # Interpret proposal \n",
    "        self.z_token = nn.Linear(z_dim, embed_dim)\n",
    "\n",
    "        ##################\n",
    "        #   ViT Layers   #\n",
    "        ##################\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        #######################\n",
    "        #   CNN Discretizer   #\n",
    "        #######################\n",
    "\n",
    "        self.discretizer = nn.Sequential(\n",
    "            nn.Conv2d(  # detect features in token\n",
    "                embed_dim, \n",
    "                hidden_channels, \n",
    "                kernel_size=3, \n",
    "                padding=1\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(  # convert token features to a classification\n",
    "                hidden_channels, \n",
    "                num_classes, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            grid, \n",
    "            z\n",
    "    ):\n",
    "        ###########################\n",
    "        #   grid = (B, 1, H, W)   #\n",
    "        #   z = (B, z_dim)        #\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   D = embedding dimension   #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "\n",
    "        B, _, H, W = grid.shape\n",
    "\n",
    "        #############################################\n",
    "        #   Enricher + Proposed Feature Modulator   #\n",
    "        #############################################\n",
    "\n",
    "        x = self.enricher(grid)\n",
    "\n",
    "        for block in self.cnn_blocks:\n",
    "            x = block(x, z)\n",
    "\n",
    "        ################\n",
    "        #   Tokenize   #\n",
    "        ################\n",
    "\n",
    "        # (B, C, H, W) -> (B, H*W, C)\n",
    "        x_flat = x.permute(0, 2, 3, 1).reshape(B, H*W, -1)\n",
    "\n",
    "        tokens = self.to_embedding(x_flat)  # (B, S, D)\n",
    "\n",
    "        # Add proposal z token\n",
    "        z_token = self.z_token(z).unsqueeze(1)  # (B, 1, D) one for each batch\n",
    "        tokens = torch.cat([z_token, tokens], dim=1)  # (B, 1+S, D)\n",
    "\n",
    "        ################################\n",
    "        #   ViT for Global Reasoning   #\n",
    "        ################################\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, None)\n",
    "\n",
    "        ###################\n",
    "        #   Un-tokenize   #\n",
    "        ###################\n",
    "\n",
    "        # Remove z token\n",
    "        x_tokens = tokens[:, 1:, :]  # (B, S, D)\n",
    "\n",
    "        # Reshape to (B, D, H, W)\n",
    "        x_feats = x_tokens.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "        ##################\n",
    "        #   Discretize   #\n",
    "        ##################\n",
    "\n",
    "        # Compute on the embedding dimension\n",
    "        logits = self.discretizer(x_feats)  # (B, num_classes, H, W)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.FiLM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Feature-wise modulation\n",
    "class FiLM(nn.Module): \n",
    "    def __init__(\n",
    "            self,\n",
    "            feature_dim,\n",
    "            z_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.to_gamma = nn.Linear(z_dim, feature_dim)  # scale factor\n",
    "        self.to_beta = nn.Linear(z_dim, feature_dim)  # shift factor\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            z: torch.Tensor\n",
    "    ):\n",
    "        ########################\n",
    "        #   x = (B, C, H, W)   #\n",
    "        #   z = (B, z_dim)     #\n",
    "        ########################\n",
    "        #   B = batch size     #       \n",
    "        #   C = channels       #\n",
    "        #   H = height         #\n",
    "        #   W = width          #\n",
    "        ########################\n",
    "\n",
    "        ############################\n",
    "        #   Compute Coefficients   #\n",
    "        ############################\n",
    "\n",
    "        # Expand across input\n",
    "        gamma = self.to_gamma(z).unsqueeze(-1).unsqueeze(-1)  # (B, C, 1, 1)\n",
    "        beta = self.to_beta(z).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        ###############################\n",
    "        #   Apply Feture Modulation   #\n",
    "        ###############################\n",
    "\n",
    "        return x * (1 + gamma) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb723ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.LViTM.attention\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "\n",
    "        out, _ = self.attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a15b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.LViTM.attention\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "\n",
    "        out, _ = self.attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b268128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.LViTM.body\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.architecture.LViTM.attention import MultiHeadAttention\n",
    "\n",
    "\n",
    "class LViTMBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embed_dim, \n",
    "            num_heads, \n",
    "            mlp_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        ###########################\n",
    "        #   Attention Mechanism   #\n",
    "        ###########################\n",
    "\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "        ###########\n",
    "        #   MLP   #\n",
    "        ###########\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim), # increase dimensionality\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim) # return to embedding dimension\n",
    "        )\n",
    "\n",
    "        #################\n",
    "        #   Normalize   #\n",
    "        #################\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x,  # (B, L, D)\n",
    "            key_padding_mask=None  # (B, L)\n",
    "    ):\n",
    "        # Apply attention to normalized input\n",
    "        x = x + self.attn(self.norm1(x), key_padding_mask=key_padding_mask)\n",
    "        # Normalize and run through MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LargeVisionTransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            mlp_dim,\n",
    "            depth,\n",
    "            num_proposals,\n",
    "            z_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        #################\n",
    "        #   Variables   #\n",
    "        #################\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_proposals = num_proposals\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        #######################\n",
    "        #   Proposal Tokens   #\n",
    "        #######################\n",
    "\n",
    "        self.proposal_tokens = nn.Parameter(\n",
    "            torch.randn(1, num_proposals, embed_dim)\n",
    "        )\n",
    "\n",
    "        # No positional encoding\n",
    "        self.pos_encoding = None\n",
    "\n",
    "        ##########################\n",
    "        #   Transformer Blocks   #\n",
    "        ##########################\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [LViTMBlock(embed_dim, num_heads, mlp_dim) for _ in range(depth)]\n",
    "        )\n",
    "\n",
    "        ##################################\n",
    "        #   Project to Latent Proposal   #\n",
    "        ##################################\n",
    "\n",
    "        self.proposal_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, z_dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            C,\n",
    "            test_tokens,\n",
    "            key_padding_mask=None\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = num batches           #\n",
    "        #   S = sequence length       #\n",
    "        #   D = embedding dimension   #\n",
    "        #   T = num proposal tokens   #\n",
    "        ###############################\n",
    "\n",
    "        B, S, D = test_tokens.shape\n",
    "        T = self.num_proposals\n",
    "\n",
    "        ##############################\n",
    "        #   Prepare Special Tokens   #\n",
    "        ##############################\n",
    "        \n",
    "        # Expand context to a single token\n",
    "        C_token = C.unsqueeze(1)\n",
    "\n",
    "        # Place proposal tokens in each batch\n",
    "        proposal_tok = self.proposal_tokens.expand(B, self.num_proposals, self.embed_dim)\n",
    "\n",
    "        ###################\n",
    "        #   Build Input   #\n",
    "        ###################\n",
    "\n",
    "        x = torch.cat([C_token, proposal_tok, test_tokens], dim=1)  # (B, 1+T+S, D)\n",
    "\n",
    "        ##################\n",
    "        #   Build Mask   #\n",
    "        ##################\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # context token padding: (B, 1)\n",
    "            c_pad = torch.zeros(B, 1, dtype=torch.bool, device=key_padding_mask.device)\n",
    "            # proposal tokens padding: (B, T)\n",
    "            p_pad = torch.zeros(B, T, dtype=torch.bool, device=key_padding_mask.device)\n",
    "            # full mask: (B, 1 + T + S)\n",
    "            full_mask = torch.cat([c_pad, p_pad, key_padding_mask], dim=1)\n",
    "        else:\n",
    "            full_mask = None\n",
    "\n",
    "        \n",
    "        # Optional positional encoding here\n",
    "\n",
    "        ####################\n",
    "        #   LViTM Blocks   #\n",
    "        ####################\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, key_padding_mask=full_mask)\n",
    "        \n",
    "        ##########################\n",
    "        #   Retrieve Proposals   #\n",
    "        ##########################\n",
    "\n",
    "        proposal_outs = x[:, 1:1+T, :]  # (B, 1+T+S, D) -> (B, T, D)\n",
    "\n",
    "        ###############################\n",
    "        #   Project to Latent Space   #\n",
    "        ###############################\n",
    "\n",
    "        Z = self.proposal_head(proposal_outs)\n",
    "\n",
    "        return Z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.ViT.attention\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "\n",
    "        out, _ = self.attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d41356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.ViT.body\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.architecture.ViT.attention import MultiHeadAttention\n",
    "from src.architecture.ViT.preprocessing import PatchEmbedding, PositionalEncoding    \n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            img_size=30, \n",
    "            patch_size=1, \n",
    "            embed_dim=128, \n",
    "            num_heads=4, \n",
    "            depth=6, \n",
    "            mlp_dim=256,\n",
    "            in_channels=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        ########################\n",
    "        #   Patch Embeddings   #\n",
    "        ########################\n",
    "        \n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            img_size=img_size, \n",
    "            patch_size=patch_size, \n",
    "            in_channels=in_channels, \n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        ########################\n",
    "        #    Context Token     #\n",
    "        ########################\n",
    "\n",
    "        self.c_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        #############################\n",
    "        #    Positional Encoding    #\n",
    "        #############################\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1) # customizeable?\n",
    "\n",
    "        ####################################\n",
    "        #    Transformer Encoder Blocks    #\n",
    "        ####################################\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim) \n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        #########################\n",
    "        #    Final LayerNorm    #\n",
    "        #########################\n",
    "\n",
    "        self.fLayerNorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x, \n",
    "            mask\n",
    "    ):\n",
    "        #################################\n",
    "        #    B = batch size             #\n",
    "        #    N = number of patchs       #\n",
    "        #    D = embedding dimension    #\n",
    "        #################################\n",
    "\n",
    "        B = x.size(0) \n",
    "\n",
    "        x = self.patch_embedding(x) # (B, N, D)\n",
    "\n",
    "        # Flatten mask \n",
    "        if mask is not None:\n",
    "            mask = mask.reshape(x.size(0), -1)  # (B, H, W) -> (B, N)\n",
    "\n",
    "        # Prepend context token mask\n",
    "        if mask is not None:\n",
    "            c_mask = torch.zeros((mask.size(0), 1), dtype=torch.bool, device=mask.device)\n",
    "            mask = torch.cat([c_mask, mask], dim=1)  # (B, 1+N)\n",
    "            mask = mask.to(torch.bool)\n",
    "\n",
    "        # Convert mask to padding mask\n",
    "        # mask: (B, H, W) or None\n",
    "        if mask is not None:\n",
    "            # Convert spatial mask (B,H,W) â†’ token mask (B,N)\n",
    "            mask_flat = mask.flatten(1)      # (B, H*W)\n",
    "            key_padding_mask = ~mask_flat    # invert boolean mask\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "\n",
    "\n",
    "\n",
    "        # Prepend context\n",
    "        c = self.c_token.expand(B, -1, -1)\n",
    "        x = torch.cat([c, x], dim=1)\n",
    "\n",
    "        x = self.pos_encoding(x) \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.transformer_blocks: # run through sequence of vision transformer blocks\n",
    "            x = block(x, key_padding_mask)\n",
    "        \n",
    "        x = self.fLayerNorm(x)\n",
    "\n",
    "        return x # (B, 1+N, D)\n",
    "    \n",
    "    def forward_grid(\n",
    "            self, \n",
    "            x, \n",
    "            mask=None\n",
    "    ):\n",
    "        tokens = self.forward(x, mask)\n",
    "        return tokens[:, 0] # context embedding\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embed_dim, \n",
    "            num_heads, \n",
    "            mlp_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        ###########################\n",
    "        #   Attention Mechanism   #\n",
    "        ###########################\n",
    "\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "        ###########\n",
    "        #   MLP   #\n",
    "        ###########\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim), # increase dimensionality\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim) # return to embedding dimension\n",
    "        )\n",
    "        \n",
    "        ###################\n",
    "        #   Layer Norms   #\n",
    "        ###################\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # Apply attention to normalized input\n",
    "        x = x + self.attn(self.norm1(x), key_padding_mask=mask)\n",
    "        # Normalize and run through MLP\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ConditionalTransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes test input grid using shared VisionTransformer \n",
    "\n",
    "    Conditioned on aggregate context vector C from example pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vit_encoder: nn.Module\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vit = vit_encoder\n",
    "        self.embed_dim = vit_encoder.c_token.size(-1)\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            I_test, \n",
    "            mask_test, \n",
    "            C\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = batch size            #\n",
    "        #   N = sequence length       #\n",
    "        #   D = embedding dimension   #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "        \n",
    "        B = I_test.size(0)\n",
    "\n",
    "        ####################\n",
    "        #   Flatten Mask   #\n",
    "        ####################\n",
    "\n",
    "        if mask_test is not None:\n",
    "            mask_test = mask_test.reshape(B, -1)  # (B, H*W=N)\n",
    "        \n",
    "        # Project C -> (B, 1, D) (one per batch)\n",
    "        C_proj = self.c_proj(C).unsqueeze(1)\n",
    "\n",
    "        #############\n",
    "        #   Embed   #\n",
    "        #############\n",
    "\n",
    "        patches = self.vit.patch_embedding(I_test)  # (B, N, D)\n",
    "\n",
    "        ###################\n",
    "        #   Build Input   #\n",
    "        ###################\n",
    "\n",
    "        tokens = torch.cat([C_proj, patches], dim=1)  # (B, 1+N, D)\n",
    "\n",
    "        ##################\n",
    "        #   Build Mask   #\n",
    "        ##################\n",
    "\n",
    "        if mask_test is not None:\n",
    "            c_mask = torch.zeros(B, 1, dtype=torch.bool, device=mask_test.device)  # (B, 1)\n",
    "            key_padding_mask = torch.cat([c_mask, ~mask_test], dim=1)  # (B, 1+N)\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "\n",
    "        ###########################\n",
    "        #   Positional Encoding   #\n",
    "        #   and Dropout           #\n",
    "        ###########################\n",
    "\n",
    "        tokens = self.vit.pos_encoding(tokens)\n",
    "        tokens = self.vit.dropout(tokens)\n",
    "\n",
    "        ####################################\n",
    "        #   Inference Transformer Blocks   #\n",
    "        ####################################\n",
    "\n",
    "        for block in self.vit.transformer_blocks:\n",
    "            tokens = block(tokens, key_padding_mask)\n",
    "\n",
    "        #####################\n",
    "        #   Normalization   #\n",
    "        #####################\n",
    "\n",
    "        tokens = self.vit.fLayerNorm(tokens)\n",
    "\n",
    "        return tokens  # (B, 1+N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.ViT.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 2. Adding Positional Embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, S, D)\n",
    "        Returns x + sinusoidal positional encoding of shape (1, S, D)\n",
    "        \"\"\"\n",
    "        B, S, D = x.size()\n",
    "        pe = self.build_sinusoidal_pe(S, D, x.device)\n",
    "        return x + pe\n",
    "\n",
    "    def build_sinusoidal_pe(self, seq_len, dim, device):\n",
    "        \"\"\"\n",
    "        seq_len: dynamic token count (patches)\n",
    "        dim: embedding dimension\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(seq_len, dim, device=device)\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim, 2, device=device).float() * (-math.log(10000.0) / dim)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)     # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)     # odd indices\n",
    "\n",
    "        return pe.unsqueeze(0)  # (1, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.data_pipeline.dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # masks\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,\n",
    "            \"train_output_masks\": train_output_masks,\n",
    "            \"test_input_masks\": test_input_masks,\n",
    "            \"test_output_masks\": test_output_masks,\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9830da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.data_pipeline.dataloader\n",
    "\n",
    "from pathlib import Path\n",
    "from src.data_pipeline.utils import load_jsons_from_folder, _add_one_to_all_values_in_place, pad_data, build_sample_level_dataset, arc_collate_fn_bs1\n",
    "from src.data_pipeline.dataset import ARCSampleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ARCDataModule:\n",
    "    \"\"\"\n",
    "    Simple wrapper to produce a DataLoader from your folder.\n",
    "    Usage:\n",
    "        dm = ARCDataModule(\"~/path/to/training\").prepare()\n",
    "        loader = dm.get_loader()\n",
    "        for batch in loader: ...\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ):\n",
    "        self.dir_path = Path(dir_path).expanduser().resolve()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.dataset = None\n",
    "        self._loader = None\n",
    "\n",
    "    def prepare(self):\n",
    "        # load + preprocess\n",
    "        data = load_jsons_from_folder(self.dir_path)\n",
    "        _add_one_to_all_values_in_place(data)\n",
    "\n",
    "        # pad each sample independently (metric_dict unused)\n",
    "        padded = pad_data(data, metric_dict=None, pad_value=self.pad_value)\n",
    "        sample_list = build_sample_level_dataset(padded, pad_value=self.pad_value)\n",
    "\n",
    "        # build dataset + loader\n",
    "        self.dataset = ARCSampleDataset(sample_list=sample_list)\n",
    "        self._loader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            collate_fn=arc_collate_fn_bs1,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "        return self  # allow chaining\n",
    "\n",
    "    def get_loader(self):\n",
    "        if self._loader is None:\n",
    "            self.prepare()\n",
    "        return self._loader\n",
    "\n",
    "    # convenience so the module itself is iterable\n",
    "    def __iter__(self):\n",
    "        return iter(self.get_loader())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) if self.dataset is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e649b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.data_pipeline.utils\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Load all JSONs (recursive)\n",
    "# ----------------------------\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Read every .json file under dir_path (recursively) and return a dict\n",
    "    keyed by the file's relative path (without the .json extension).\n",
    "    \"\"\"\n",
    "    root = Path(dir_path).expanduser().resolve()\n",
    "    files = sorted(p for p in root.rglob(\"*.json\") if p.is_file())\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found under: {root}\")\n",
    "\n",
    "    data = {}\n",
    "    for p in files:\n",
    "        key = str(p.relative_to(root).with_suffix(\"\"))  # e.g. \"subdir/file\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data[key] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"Unable to load any .json files under: {root}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocess helpers\n",
    "# ----------------------------\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample so that *both* train and test grids\n",
    "    share the same square size per sample.\n",
    "\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # -----------------------------------\n",
    "        # 1. Compute per-sample global maxima\n",
    "        #    across BOTH train and test\n",
    "        # -----------------------------------\n",
    "        max_input_height = 0\n",
    "        max_input_width  = 0\n",
    "        max_output_height = 0\n",
    "        max_output_width  = 0\n",
    "\n",
    "        # Look at TRAIN split\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            if len(pairs['input']) > max_input_height:\n",
    "                max_input_height = len(pairs['input'])\n",
    "            for row in pairs['input']:\n",
    "                if len(row) > max_input_width:\n",
    "                    max_input_width = len(row)\n",
    "\n",
    "            # output\n",
    "            if len(pairs['output']) > max_output_height:\n",
    "                max_output_height = len(pairs['output'])\n",
    "            for row in pairs['output']:\n",
    "                if len(row) > max_output_width:\n",
    "                    max_output_width = len(row)\n",
    "\n",
    "        # Look at TEST split\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            if len(pairs['input']) > max_input_height:\n",
    "                max_input_height = len(pairs['input'])\n",
    "            for row in pairs['input']:\n",
    "                if len(row) > max_input_width:\n",
    "                    max_input_width = len(row)\n",
    "\n",
    "            # output\n",
    "            if len(pairs['output']) > max_output_height:\n",
    "                max_output_height = len(pairs['output'])\n",
    "            for row in pairs['output']:\n",
    "                if len(row) > max_output_width:\n",
    "                    max_output_width = len(row)\n",
    "\n",
    "        # Single square size for this sample\n",
    "        max_size = max(\n",
    "            max_input_height,\n",
    "            max_input_width,\n",
    "            max_output_height,\n",
    "            max_output_width,\n",
    "        )\n",
    "\n",
    "        # -----------------------------------\n",
    "        # 2. Pad TRAIN grids to max_size\n",
    "        # -----------------------------------\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_size:\n",
    "                pairs['input'].append([pad_value] * max_size)\n",
    "            for row in pairs['input']:\n",
    "                while len(row) < max_size:\n",
    "                    row.append(pad_value)\n",
    "\n",
    "            # output\n",
    "            while len(pairs['output']) < max_size:\n",
    "                pairs['output'].append([pad_value] * max_size)\n",
    "            for row in pairs['output']:\n",
    "                while len(row) < max_size:\n",
    "                    row.append(pad_value)\n",
    "\n",
    "        # -----------------------------------\n",
    "        # 3. Pad TEST grids to max_size\n",
    "        # -----------------------------------\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_size:\n",
    "                pairs['input'].append([pad_value] * max_size)\n",
    "            for row in pairs['input']:\n",
    "                while len(row) < max_size:\n",
    "                    row.append(pad_value)\n",
    "\n",
    "            # output\n",
    "            while len(pairs['output']) < max_size:\n",
    "                pairs['output'].append([pad_value] * max_size)\n",
    "            for row in pairs['output']:\n",
    "                while len(row) < max_size:\n",
    "                    row.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    # \"\"\"\n",
    "    # Pads each sample independently to its own max square size.\n",
    "    # metric_dict is ignored (kept for backward compatibility).\n",
    "    # \"\"\"\n",
    "    # for sample in data.values():\n",
    "    #     # ----- compute per-sample maxima for TRAIN -----\n",
    "    #     max_train_input_height = 0\n",
    "    #     max_train_input_width  = 0\n",
    "    #     max_train_output_height = 0\n",
    "    #     max_train_output_width  = 0\n",
    "\n",
    "    #     for pairs in sample.get('train', []):\n",
    "    #         if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "    #         if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "    #         for inp in pairs['input']:\n",
    "    #             if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "    #         for outp in pairs['output']:\n",
    "    #             if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "    #     # ----- compute per-sample maxima for TEST -----\n",
    "    #     max_test_input_height = 0\n",
    "    #     max_test_input_width  = 0\n",
    "    #     max_test_output_height = 0\n",
    "    #     max_test_output_width  = 0\n",
    "\n",
    "    #     for pairs in sample.get('test', []):\n",
    "    #         if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "    #         if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "    #         for inp in pairs['input']:\n",
    "    #             if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "    #         for outp in pairs['output']:\n",
    "    #             if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "    #     # ----- per-sample square sizes -----\n",
    "    #     max_train_size = max(\n",
    "    #         max_train_input_height,\n",
    "    #         max_train_input_width,\n",
    "    #         max_train_output_height,\n",
    "    #         max_train_output_width\n",
    "    #     )\n",
    "    #     max_test_size = max(\n",
    "    #         max_test_input_height,\n",
    "    #         max_test_input_width,\n",
    "    #         max_test_output_height,\n",
    "    #         max_test_output_width\n",
    "    #     )\n",
    "\n",
    "    #     # ----- pad TRAIN for this sample -----\n",
    "    #     for pairs in sample.get('train', []):\n",
    "    #         # input\n",
    "    #         while len(pairs['input']) < max_train_size:\n",
    "    #             pairs['input'].append([pad_value] * max_train_size)\n",
    "    #         for inp in pairs['input']:\n",
    "    #             while len(inp) < max_train_size:\n",
    "    #                 inp.append(pad_value)\n",
    "    #         # output\n",
    "    #         while len(pairs['output']) < max_train_size:\n",
    "    #             pairs['output'].append([pad_value] * max_train_size)\n",
    "    #         for outp in pairs['output']:\n",
    "    #             while len(outp) < max_train_size:\n",
    "    #                 outp.append(pad_value)\n",
    "\n",
    "    #     # ----- pad TEST for this sample -----\n",
    "    #     for pairs in sample.get('test', []):\n",
    "    #         # input\n",
    "    #         while len(pairs['input']) < max_test_size:\n",
    "    #             pairs['input'].append([pad_value] * max_test_size)\n",
    "    #         for inp in pairs['input']:\n",
    "    #             while len(inp) < max_test_size:\n",
    "    #                 inp.append(pad_value)\n",
    "    #         # output\n",
    "    #         while len(pairs['output']) < max_test_size:\n",
    "    #             pairs['output'].append([pad_value] * max_test_size)\n",
    "    #         for outp in pairs['output']:\n",
    "    #             while len(outp) < max_test_size:\n",
    "    #                 outp.append(pad_value)\n",
    "\n",
    "    # return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.data_pipeline.dataset_io\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def save_sample_level(sample_list, file_path):\n",
    "    file_path = os.path.expanduser(file_path)\n",
    "    folder = os.path.dirname(file_path)\n",
    "    if folder and (not os.path.isdir(folder)):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    torch.save(sample_list, file_path)\n",
    "\n",
    "def load_sample_level(file_path, map_location=\"cpu\"):\n",
    "    file_path = os.path.expanduser(file_path)\n",
    "    obj = torch.load(file_path, map_location=map_location)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b780086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.inference.generator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class ARCGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    For each sample:\n",
    "      - Uses ALL training example pairs (I_i, O_i) to compute a context vector C.\n",
    "      - Then, for EACH test input in that sample:\n",
    "          * encodes the test input conditioned on C,\n",
    "          * uses LViTM to propose latent transformation vectors Z,\n",
    "          * chooses a single z (here: the first proposal),\n",
    "          * runs the Executor to produce logits for that test.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        example_pair_encoder: nn.Module,\n",
    "        aggregator: nn.Module,\n",
    "        cond_encoder: nn.Module,\n",
    "        lvitm: nn.Module,\n",
    "        executor: nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.example_pair_encoder = example_pair_encoder\n",
    "        self.aggregator = aggregator\n",
    "        self.cond_encoder = cond_encoder\n",
    "        self.lvitm = lvitm\n",
    "        self.executor = executor\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        train_inputs: torch.Tensor,        # (K_train, H, W)   or (B, K_train, H, W)\n",
    "        train_outputs: torch.Tensor,       # (K_train, H, W)   or (B, K_train, H, W)\n",
    "        train_input_masks: torch.Tensor,   # (K_train, H, W)   or (B, K_train, H, W)\n",
    "        train_output_masks: torch.Tensor,  # (K_train, H, W)   or (B, K_train, H, W)\n",
    "        test_inputs: torch.Tensor,         # (K_test, H, W)    or (B, K_test, H, W)\n",
    "        test_input_masks: torch.Tensor,    # (K_test, H, W)    or (B, K_test, H, W)\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        ##################################\n",
    "        #   Normalize Batch Dimensions   #\n",
    "        ##################################\n",
    "\n",
    "        # If no batch dimension, assume B=1 and add it.\n",
    "        if train_inputs.dim() == 3:\n",
    "            train_inputs = train_inputs.unsqueeze(0)\n",
    "            train_outputs = train_outputs.unsqueeze(0)\n",
    "            train_input_masks = train_input_masks.unsqueeze(0)\n",
    "            train_output_masks = train_output_masks.unsqueeze(0)\n",
    "\n",
    "        if test_inputs.dim() == 3:\n",
    "            test_inputs = test_inputs.unsqueeze(0)\n",
    "            test_input_masks = test_input_masks.unsqueeze(0)\n",
    "\n",
    "        B, K_train, H, W = train_inputs.shape\n",
    "        _, K_test, H_t, W_t = test_inputs.shape\n",
    "        assert H == H_t and W == W_t, \"Train and test grids must share padded size per sample.\"\n",
    "\n",
    "        # Ensure masks are boolean\n",
    "        train_input_masks = train_input_masks.bool()\n",
    "        train_output_masks = train_output_masks.bool()\n",
    "        test_input_masks = test_input_masks.bool()\n",
    "\n",
    "        #########################################\n",
    "        #   Encode ALL training example pairs   #\n",
    "        #########################################\n",
    "\n",
    "        # For each training pair (I_i, O_i), we get a context embedding h_i.\n",
    "        h_list = []\n",
    "\n",
    "        for k in range(K_train):\n",
    "            # Shapes: (B, 1, H, W)\n",
    "            I_k = train_inputs[:, k].unsqueeze(1).float()\n",
    "            O_k = train_outputs[:, k].unsqueeze(1).float()\n",
    "\n",
    "            mask_I_k = train_input_masks[:, k]   # (B, H, W)\n",
    "            mask_O_k = train_output_masks[:, k]  # (B, H, W)\n",
    "\n",
    "            h_k = self.example_pair_encoder(\n",
    "                I_i=I_k,\n",
    "                O_i=O_k,\n",
    "                mask_I=mask_I_k,\n",
    "                mask_O=mask_O_k\n",
    "            )  # (B, D)\n",
    "            h_list.append(h_k)\n",
    "\n",
    "        # Stack: (B, K_train, D)\n",
    "        h = torch.stack(h_list, dim=1)\n",
    "\n",
    "        # Optional: pair_mask could be used if we ever have invalid pairs.\n",
    "        pair_mask = None  # (B, K_train) if needed\n",
    "\n",
    "        ##############################\n",
    "        #   Aggregate to context C   #\n",
    "        ##############################\n",
    "\n",
    "        # Single context vector per sample, shared by all test pairs.\n",
    "        C = self.aggregator(h, mask=pair_mask)  # (B, D)\n",
    "\n",
    "        # ----------------------------------\n",
    "        #   Loop over ALL test inputs\n",
    "        # ----------------------------------\n",
    "        all_logits = []\n",
    "        all_Z = []\n",
    "        all_z_chosen = []\n",
    "\n",
    "        for j in range(K_test):\n",
    "            # Test grid j: (B, 1, H, W)\n",
    "            I_test_j = test_inputs[:, j].unsqueeze(1).float()\n",
    "            mask_test_j = test_input_masks[:, j]   # (B, H, W)\n",
    "\n",
    "            # Encode test input with context C\n",
    "            test_tokens_j, key_padding_mask_j = self.cond_encoder(\n",
    "                I_test_j,      # (B,1,H,W)\n",
    "                mask_test_j,   # (B,H,W)\n",
    "                C              # (B,D)\n",
    "            )\n",
    "            # test_tokens_j: (B, S, D)\n",
    "            # key_padding_mask_j: (B, S) or None\n",
    "\n",
    "            # LViTM proposes latent transformation vectors for this test input\n",
    "            Z_j = self.lvitm(\n",
    "                C=C,\n",
    "                test_tokens=test_tokens_j,\n",
    "                key_padding_mask=key_padding_mask_j\n",
    "            )  # (B, P, z_dim)\n",
    "\n",
    "            Bz, P, z_dim = Z_j.shape\n",
    "            assert Bz == B, \"Batch size mismatch between context and proposals.\"\n",
    "\n",
    "            # Choose a single proposal z for this test input\n",
    "            # Minimal baseline: pick first proposal\n",
    "            z_chosen_j = Z_j[:, 0, :]  # (B, z_dim)\n",
    "\n",
    "            # Executor predicts the output grid logits\n",
    "            logits_j = self.executor(\n",
    "                grid=I_test_j,   # (B,1,H,W)\n",
    "                z=z_chosen_j     # (B,z_dim)\n",
    "            )  # (B, num_classes, H, W)\n",
    "\n",
    "            all_logits.append(logits_j)          # each: (B, C_out, H, W)\n",
    "            all_Z.append(Z_j)                    # each: (B, P, z_dim)\n",
    "            all_z_chosen.append(z_chosen_j)      # each: (B, z_dim)\n",
    "\n",
    "        # -----------------------------------------\n",
    "        #   Stack results across ALL test inputs\n",
    "        # -----------------------------------------\n",
    "        # logits: (B, K_test, num_classes, H, W)\n",
    "        logits = torch.stack(all_logits, dim=1)\n",
    "\n",
    "        # Z_all: (B, K_test, P, z_dim)\n",
    "        Z_all = torch.stack(all_Z, dim=1)\n",
    "\n",
    "        # z_chosen: (B, K_test, z_dim)\n",
    "        z_chosen = torch.stack(all_z_chosen, dim=1)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,         # (B, K_test, C_out, H, W)\n",
    "            \"Z_all\": Z_all,           # (B, K_test, P, z_dim)\n",
    "            \"z_chosen\": z_chosen,     # (B, K_test, z_dim)\n",
    "            \"C\": C                    # (B, D)\n",
    "        }\n",
    "\n",
    "\"\"\"\n",
    "Usage:\n",
    "\n",
    "batch = next(iter(arc_loader))\n",
    "\n",
    "out = arc_generator(\n",
    "    train_inputs=batch[\"train_inputs\"],         # (K_train, H, W)\n",
    "    train_outputs=batch[\"train_outputs\"],       # (K_train, H, W)\n",
    "    train_input_masks=batch[\"train_input_masks\"],\n",
    "    train_output_masks=batch[\"train_output_masks\"],\n",
    "    test_inputs=batch[\"test_inputs\"],           # (K_test, H, W)\n",
    "    test_input_masks=batch[\"test_input_masks\"],\n",
    ")\n",
    "\n",
    "logits = out[\"logits\"]  # (1, K_test, num_classes, H, W)\n",
    "\n",
    "\n",
    "Loss:\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# target: (1, K_test, H, W) â†’ (1 * K_test, H, W)\n",
    "target = batch[\"test_outputs\"].unsqueeze(0) if batch[\"test_outputs\"].dim() == 3 else batch[\"test_outputs\"]\n",
    "B, K_test, H, W = target.shape\n",
    "target_flat = target.view(B * K_test, H, W)\n",
    "\n",
    "# logits: (1, K_test, C_out, H, W) â†’ (1 * K_test, C_out, H, W)\n",
    "logits_flat = logits.view(B * K_test, logits.size(2), H, W)\n",
    "\n",
    "loss = F.cross_entropy(logits_flat, target_flat)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.inference.execution_controller\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.architecture.LViTM.body import LargeVisionTransformerModel\n",
    "from src.architecture.executor.executor import Executor\n",
    "from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "from src.architecture.context_encoding.conditional_encoder import ConditionalTestInputEncoder\n",
    "\n",
    "\n",
    "class HybridExecuteController(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lvitm: LargeVisionTransformerModel,\n",
    "            executor: Executor, \n",
    "            cond_encoder: ConditionalTestInputEncoder,\n",
    "            critic: AdversarialVisionTransformer | None = None\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   D = token embedding dim   #\n",
    "        #   P = num proposals         #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "\n",
    "        super().__init__()\n",
    "        self.lvitm = lvitm\n",
    "        self.executor = executor\n",
    "        self.cond_encoder = cond_encoder\n",
    "        self.critic = critic\n",
    "\n",
    "    ################################\n",
    "    #   Parallel Mode (Training)   #\n",
    "    ################################\n",
    "\n",
    "    def apply_parallel(\n",
    "        self,\n",
    "        I_test: torch.Tensor,  # (B, 1, H, W)\n",
    "        mask_test: torch.Tensor,  # (B, H, W)\n",
    "        C: torch.Tensor,  # (B, D)\n",
    "        examples=None\n",
    "    ):\n",
    "        B, _, H, W = I_test.shape\n",
    "\n",
    "        # Encode test input with context\n",
    "        tokens, key_padding_mask = self.cond_encoder(I_test, mask_test, C)\n",
    "\n",
    "        #######################\n",
    "        #   Reasoning Model   #\n",
    "        #######################\n",
    "\n",
    "        # Compute proposals\n",
    "        Z = self.lvitm(C, tokens, key_padding_mask)\n",
    "\n",
    "        B, P, z_dim = Z.shape\n",
    "\n",
    "        ################\n",
    "        #   Executor   #\n",
    "        ################\n",
    "\n",
    "        # Flatten input for executor\n",
    "        grid_expansion = I_test.unsqueeze(1).expand(B, P, 1, H, W).reshape(B*P, 1, H, W)\n",
    "        z_flat = Z.reshape(B*P, z_dim)\n",
    "\n",
    "        # Execute proposals\n",
    "        out_flat = self.executor(grid_expansion, z_flat)\n",
    "        num_classes = out_flat.size(1)\n",
    "        outputs = out_flat.view(B, P, num_classes, H, W)\n",
    "\n",
    "        ##############\n",
    "        #   Critic   #\n",
    "        ##############\n",
    "\n",
    "        scores = None\n",
    "        best_idx = None\n",
    "        if self.critic is not None and examples is not None:\n",
    "            scores = self.critic(\n",
    "                I_in=I_test,            # (B, 1, H, W)\n",
    "                O_pred=outputs,         # (B, P, num_classes, H, W)\n",
    "                mask_in=mask_test,      # (B, H, W)\n",
    "                mask_out=mask_test,     # assume same valid region as input\n",
    "                z=Z,                    # (B, P, z_dim)\n",
    "                C=C                     # (B, D)\n",
    "            )     \n",
    "            best_idx = scores.argmax(dim=1)  # (B,)\n",
    "\n",
    "        return outputs, scores, best_idx\n",
    "    \n",
    "    ###################################\n",
    "    #   Sequential Mode (Inference)   #\n",
    "    ###################################\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply_sequential(\n",
    "        self,\n",
    "        init_grid: torch.Tensor,  # (B, 1, H, W)\n",
    "        init_mask: torch.Tensor,  # (B, H, W)\n",
    "        C,                        # (B, D)\n",
    "        examples=None,\n",
    "        num_steps=3\n",
    "    ):\n",
    "        grid = init_grid\n",
    "        mask = init_mask\n",
    "        history: list[torch.Tensor] = [grid.clone()]\n",
    "\n",
    "        #########################\n",
    "        #   Iterate Proposals   #\n",
    "        #########################\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            B, _, H, W = grid.shape\n",
    "\n",
    "            # Encode test input with context\n",
    "            tokens, key_padding_mask = self.cond_encoder(grid, mask, C)\n",
    "\n",
    "            #######################\n",
    "            #   Reasoning Model   #\n",
    "            #######################\n",
    "\n",
    "            # Compute proposal\n",
    "            Z = self.lvitm(C, tokens, key_padding_mask)  # (B, P, z_dim)\n",
    "            B, P, z_dim = Z.shape\n",
    "\n",
    "            ################\n",
    "            #   Executor   #\n",
    "            ################\n",
    "\n",
    "            # Flatten input for executor\n",
    "            grid_rep = grid.unsqueeze(1).expand(B, P, 1, H, W).reshape(B * P, 1, H, W)\n",
    "            z_flat = Z.reshape(B * P, z_dim)\n",
    "\n",
    "            # Execute proposals\n",
    "            out_flat = self.executor(grid_rep, z_flat)  # (B*T, num_classes, H, W)\n",
    "            num_classes = out_flat.size(1)\n",
    "            outputs = out_flat.view(B, P, num_classes, H, W)\n",
    "\n",
    "            ##############\n",
    "            #   Critic   #\n",
    "            ##############\n",
    "\n",
    "            # Choose proposal\n",
    "            if self.critic is not None:\n",
    "                scores = self.critic(\n",
    "                    I_in=grid,\n",
    "                    O_pred=outputs,\n",
    "                    mask_in=mask,\n",
    "                    mask_out=mask,\n",
    "                    z=Z,\n",
    "                    C=C\n",
    "                )  # (B, P)\n",
    "                best_idx = scores.argmax(dim=1)  # (B,)\n",
    "            else:\n",
    "                # Or take first proposal\n",
    "                best_idx = torch.zeros(B, dtype=torch.long, device=grid.device)\n",
    "\n",
    "            # Gather best output per batch\n",
    "            idx = best_idx.view(B, 1, 1, 1, 1).expand(B, 1, num_classes, H, W)\n",
    "            best_out_logits = outputs.gather(dim=1, index=idx).squeeze(1)  # (B, C_out, H, W)\n",
    "\n",
    "            # Discretize logits\n",
    "            best_out_grid = best_out_logits.argmax(dim=1, keepdim=True)    # (B, 1, H, W)\n",
    "\n",
    "            grid = best_out_grid\n",
    "            # mask stays the same spatially\n",
    "            history.append(grid.clone())\n",
    "\n",
    "        final_grid_logits = best_out_logits  \n",
    "        return final_grid_logits, history\n",
    "\n",
    "        ############################################################\n",
    "    #   PPO Rollout + (stub) Update for Phase 4                #\n",
    "    ############################################################\n",
    "    def ppo_rollout_and_update(\n",
    "        self,\n",
    "        init_grid: torch.Tensor,       # (B, 1, H, W)\n",
    "        init_mask: torch.Tensor,       # (B, H, W)\n",
    "        C: torch.Tensor,               # (B, D)\n",
    "        ppo_refiner,                   # PPORefiner object (currently unused in stub)\n",
    "        num_steps: int,\n",
    "        gamma: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Minimal implementation to satisfy Phase 4:\n",
    "\n",
    "        - Encodes the test grid with cond_encoder\n",
    "        - Runs LViTM once to get latent proposals z\n",
    "        - Picks the first proposal\n",
    "        - Runs Executor to get final logits\n",
    "        - Returns logits and a dummy PPO stats dict\n",
    "\n",
    "        This does NOT yet perform real PPO; it is a safe stub\n",
    "        that keeps all dimensions consistent and lets training run.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Encode test grid into tokens\n",
    "        #    ConditionalTestInputEncoder.forward: (I_test, mask_I) -> (tokens, key_padding_mask)\n",
    "        test_tokens, key_padding_mask = self.cond_encoder(\n",
    "            I_test=init_grid,\n",
    "            mask_test=init_mask,\n",
    "            C=C\n",
    "        )\n",
    "\n",
    "\n",
    "        # 2. Get latent proposals z from LViTM\n",
    "        #    LargeVisionTransformerModel.forward: (tokens, key_padding_mask, C) -> Z\n",
    "        Z = self.lvitm(\n",
    "            C=C,\n",
    "            test_tokens=test_tokens,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        # typically (B, T, z_dim) or (B, z_dim)\n",
    "\n",
    "        # 3. Choose a single z per sample (e.g., first proposal)\n",
    "        if Z.dim() == 3:  # (B, T, z_dim)\n",
    "            Z0 = Z[:, 0, :]           # (B, z_dim)\n",
    "        else:             # already (B, z_dim)\n",
    "            Z0 = Z\n",
    "\n",
    "        # 4. Run Executor to get logits over ARC colors\n",
    "        #    Executor.forward is expected to look like:\n",
    "        #       forward(I_test, Z, key_padding_mask=None)\n",
    "        logits = self.executor(init_grid, Z0)\n",
    "        # (B, num_classes, H, W)\n",
    "\n",
    "        # 5. Package PPO stats (stubbed out for now)\n",
    "        ppo_stats = {\n",
    "            \"loss\": 0.0,\n",
    "            \"policy_loss\": 0.0,\n",
    "            \"value_loss\": 0.0,\n",
    "            \"entropy\": 0.0,\n",
    "        }\n",
    "\n",
    "        # Phase 4 expects: (final_logits, ppo_stats)\n",
    "        return logits, ppo_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.checkpoints\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Checkpoint Utilities      #\n",
    "###############################\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    \"\"\"\n",
    "    Creates directory if it does not exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        obj,            # nn.Module or dict\n",
    "        path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves a module or state dict to the specified path.\n",
    "    \"\"\"\n",
    "\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    torch.save(obj, path)\n",
    "    print(f\"[checkpoint] Saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "        path: str,\n",
    "        map_location=None,\n",
    "        strict: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a full checkpoint (.pt or .pth). If strict=False and it is a state_dict,\n",
    "    caller can partially load it.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[checkpoint] Not found: {path}\")\n",
    "        return None\n",
    "\n",
    "    ckpt = torch.load(path, map_location=map_location)\n",
    "    print(f\"[checkpoint] Loaded: {path}\")\n",
    "    return ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12899b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.evaluate_final\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.training.checkpoints import load_checkpoint\n",
    "\n",
    "from src.inference.generator import ARCGenerator\n",
    "from src.training.ppo_actor import PPOActor\n",
    "from src.training.ppo_value import PPOValuer\n",
    "from src.training.ppo_refiner import PPORefiner\n",
    "\n",
    "from src.architecture.ViT.body import VisionTransformer\n",
    "from src.architecture.context_encoding.example_pair_encoder import ExamplePairEncoder\n",
    "from src.architecture.context_encoding.example_pair_aggregator import ExamplePairAggregator\n",
    "from src.architecture.context_encoding.conditional_encoder import ConditionalTestInputEncoder\n",
    "from src.architecture.LViTM.body import LargeVisionTransformerModel\n",
    "from src.architecture.executor.executor import Executor\n",
    "from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "from src.data_pipeline.dataloader import ARCDataModule\n",
    "\n",
    "from src.inference.execution_controller import HybridExecuteController\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Device + Constants        #\n",
    "###############################\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Generator Builder         #\n",
    "###############################\n",
    "\n",
    "def build_generator():\n",
    "    vit_gen = VisionTransformer(\n",
    "        img_size=30,\n",
    "        patch_size=1,\n",
    "        embed_dim=128,\n",
    "        num_heads=4,\n",
    "        depth=6,\n",
    "        mlp_dim=256,\n",
    "        in_channels=2\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    example_encoder = ExamplePairEncoder(vit_gen).to(DEVICE)\n",
    "    aggregator = ExamplePairAggregator(embed_dim=vit_gen.c_token.size(-1)).to(DEVICE)\n",
    "    cond_encoder = ConditionalTestInputEncoder(vit_gen).to(DEVICE)\n",
    "\n",
    "    lvitm = LargeVisionTransformerModel(\n",
    "        embed_dim=vit_gen.c_token.size(-1),\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=8,\n",
    "        num_proposals=4,\n",
    "        z_dim=64\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    executor = Executor(\n",
    "        embed_dim=vit_gen.c_token.size(-1),\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=4,\n",
    "        z_dim=64,\n",
    "        hidden_channels=64,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    generator = ARCGenerator(\n",
    "        example_pair_encoder=example_encoder,\n",
    "        aggregator=aggregator,\n",
    "        cond_encoder=cond_encoder,\n",
    "        lvitm=lvitm,\n",
    "        executor=executor\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return generator, example_encoder, aggregator, cond_encoder, lvitm, executor\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Critic + PPO Builders     #\n",
    "###############################\n",
    "\n",
    "def build_critic():\n",
    "    vit_critic = VisionTransformer(\n",
    "        img_size=30,\n",
    "        patch_size=1,\n",
    "        embed_dim=128,\n",
    "        num_heads=4,\n",
    "        depth=6,\n",
    "        mlp_dim=256,\n",
    "        in_channels=1 + NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    critic = AdversarialVisionTransformer(\n",
    "        vit_encoder=vit_critic,\n",
    "        z_dim=None,\n",
    "        c_dim=None,\n",
    "        hidden_dim=256\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return critic\n",
    "\n",
    "\n",
    "def build_ppo(z_dim=64):\n",
    "    actor = PPOActor(z_dim=z_dim, embed_dim=256).to(DEVICE)\n",
    "    valuer = PPOValuer(z_dim=z_dim, embed_dim=256).to(DEVICE)\n",
    "    refiner = PPORefiner(actor=actor, value_fn=valuer, lr=1e-4)\n",
    "    return actor, valuer, refiner\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Context Computation       #\n",
    "###############################\n",
    "\n",
    "def compute_context_C(\n",
    "        example_encoder,\n",
    "        aggregator,\n",
    "        train_inputs,\n",
    "        train_outputs,\n",
    "        train_input_masks,\n",
    "        train_output_masks\n",
    "):\n",
    "    if train_inputs.dim() == 3:\n",
    "        train_inputs = train_inputs.unsqueeze(0)\n",
    "        train_outputs = train_outputs.unsqueeze(0)\n",
    "        train_input_masks = train_input_masks.unsqueeze(0)\n",
    "        train_output_masks = train_output_masks.unsqueeze(0)\n",
    "\n",
    "    B, K_train, H, W = train_inputs.shape\n",
    "\n",
    "    h_list = []\n",
    "\n",
    "    for k in range(K_train):\n",
    "        I_k = train_inputs[:, k].unsqueeze(1).float()\n",
    "        O_k = train_outputs[:, k].unsqueeze(1).float()\n",
    "\n",
    "        mask_I_k = train_input_masks[:, k]\n",
    "        mask_O_k = train_output_masks[:, k]\n",
    "\n",
    "        h_k = example_encoder(\n",
    "            I_i=I_k,\n",
    "            O_i=O_k,\n",
    "            mask_I=mask_I_k,\n",
    "            mask_O=mask_O_k\n",
    "        )  # (B,D)\n",
    "        h_list.append(h_k)\n",
    "\n",
    "    h = torch.stack(h_list, dim=1)\n",
    "    pair_mask = None\n",
    "\n",
    "    C = aggregator(h, mask=pair_mask)\n",
    "    return C\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Evaluation Loop           #\n",
    "###############################\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_final():\n",
    "    \"\"\"\n",
    "    Evaluates:\n",
    "        - baseline generator (direct logits)\n",
    "        - sequential + PPO refinement (first test input)\n",
    "    Prints exact match and pixel accuracy for both.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build models\n",
    "    generator, example_encoder, aggregator, cond_encoder, lvitm, executor = build_generator()\n",
    "    critic = build_critic()\n",
    "    actor, valuer, ppo_refiner = build_ppo(z_dim=64)\n",
    "\n",
    "    # Load checkpoints if they exist\n",
    "    gen_p3 = load_checkpoint(\"checkpoints/generator_phase3_adv.pt\", map_location=DEVICE)\n",
    "    if gen_p3 is not None:\n",
    "        generator.load_state_dict(gen_p3, strict=False)\n",
    "\n",
    "    crit_p3 = load_checkpoint(\"checkpoints/critic_phase3_adv.pt\", map_location=DEVICE)\n",
    "    if crit_p3 is not None:\n",
    "        critic.load_state_dict(crit_p3, strict=False)\n",
    "\n",
    "    actor_p4 = load_checkpoint(\"checkpoints/ppo_actor_phase4.pt\", map_location=DEVICE)\n",
    "    if actor_p4 is not None:\n",
    "        actor.load_state_dict(actor_p4, strict=False)\n",
    "\n",
    "    val_p4 = load_checkpoint(\"checkpoints/ppo_valuer_phase4.pt\", map_location=DEVICE)\n",
    "    if val_p4 is not None:\n",
    "        valuer.load_state_dict(val_p4, strict=False)\n",
    "\n",
    "    controller = HybridExecuteController(\n",
    "        lvitm=lvitm,\n",
    "        executor=executor,\n",
    "        cond_encoder=cond_encoder,\n",
    "        critic=critic\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Metrics\n",
    "    total_exact_baseline = 0\n",
    "    total_exact_ppo = 0\n",
    "    total_tests = 0\n",
    "\n",
    "    total_pix_baseline = 0\n",
    "    total_pix_ppo = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    for batch in ARCDataModule:\n",
    "        for k, v in batch.items():\n",
    "            if torch.is_tensor(v):\n",
    "                batch[k] = v.to(DEVICE)\n",
    "\n",
    "        train_inputs       = batch[\"train_inputs\"]\n",
    "        train_outputs      = batch[\"train_outputs\"]\n",
    "        train_input_masks  = batch[\"train_input_masks\"]\n",
    "        train_output_masks = batch[\"train_output_masks\"]\n",
    "\n",
    "        test_inputs        = batch[\"test_inputs\"]\n",
    "        test_outputs       = batch[\"test_outputs\"]\n",
    "        test_input_masks   = batch[\"test_input_masks\"]\n",
    "        test_output_masks  = batch.get(\"test_output_masks\", test_input_masks)\n",
    "\n",
    "        # Baseline forward (all tests)\n",
    "        gen_out = generator(\n",
    "            train_inputs=train_inputs,\n",
    "            train_outputs=train_outputs,\n",
    "            train_input_masks=train_input_masks,\n",
    "            train_output_masks=train_output_masks,\n",
    "            test_inputs=test_inputs,\n",
    "            test_input_masks=test_input_masks,\n",
    "        )\n",
    "\n",
    "        logits = gen_out[\"logits\"]  # (1,K_test,C_out,H,W)\n",
    "        preds = logits.argmax(dim=2).squeeze(0)  # (K_test,H,W)\n",
    "        targets = test_outputs                    # (K_test,H,W)\n",
    "\n",
    "        K_test, H, W = targets.shape\n",
    "\n",
    "        for j in range(K_test):\n",
    "            pred_b = preds[j]\n",
    "            target_b = targets[j]\n",
    "\n",
    "            # If original sizes exist, crop; else assume full\n",
    "            if \"test_original_size\" in batch:\n",
    "                orig_h, orig_w = batch[\"test_original_size\"].tolist()\n",
    "                pred_b = pred_b[:orig_h, :orig_w]\n",
    "                target_b = target_b[:orig_h, :orig_w]\n",
    "\n",
    "            exact_b = (pred_b == target_b).all().item()\n",
    "            total_exact_baseline += exact_b\n",
    "            total_tests += 1\n",
    "\n",
    "            total_pix_baseline += (pred_b == target_b).sum().item()\n",
    "            total_pixels += target_b.numel()\n",
    "\n",
    "        # PPO sequential evaluation on first test input only\n",
    "        if test_inputs.dim() == 3:\n",
    "            test_inputs = test_inputs.unsqueeze(0)\n",
    "            test_input_masks = test_input_masks.unsqueeze(0)\n",
    "            test_outputs = test_outputs.unsqueeze(0)\n",
    "\n",
    "        B, K_test, H, W = test_inputs.shape\n",
    "        init_grid = test_inputs[:, 0].unsqueeze(1).float()      # (B,1,H,W)\n",
    "        init_mask = test_input_masks[:, 0]                       # (B,H,W)\n",
    "        target_ppo = test_outputs[:, 0]                          # (B,H,W)\n",
    "\n",
    "        C = compute_context_C(\n",
    "            example_encoder,\n",
    "            aggregator,\n",
    "            train_inputs,\n",
    "            train_outputs,\n",
    "            train_input_masks,\n",
    "            train_output_masks\n",
    "        )\n",
    "\n",
    "        final_logits, _ = controller.ppo_rollout_and_update(\n",
    "            init_grid=init_grid,\n",
    "            init_mask=init_mask,\n",
    "            C=C,\n",
    "            ppo_refiner=ppo_refiner,\n",
    "            num_steps=3,\n",
    "            gamma=0.99\n",
    "        )\n",
    "\n",
    "        pred_ppo = final_logits.argmax(dim=1)  # (B,H,W)\n",
    "\n",
    "        pred_ppo_b = pred_ppo[0]\n",
    "        target_ppo_b = target_ppo[0]\n",
    "\n",
    "        if \"test_original_size\" in batch:\n",
    "            orig_h, orig_w = batch[\"test_original_size\"].tolist()\n",
    "            pred_ppo_b = pred_ppo_b[:orig_h, :orig_w]\n",
    "            target_ppo_b = target_ppo_b[:orig_h, :orig_w]\n",
    "\n",
    "        exact_ppo = (pred_ppo_b == target_ppo_b).all().item()\n",
    "        total_exact_ppo += exact_ppo\n",
    "\n",
    "        total_pix_ppo += (pred_ppo_b == target_ppo_b).sum().item()\n",
    "\n",
    "    exact_acc_baseline = total_exact_baseline / total_tests if total_tests > 0 else 0.0\n",
    "    exact_acc_ppo = total_exact_ppo / total_tests if total_tests > 0 else 0.0\n",
    "\n",
    "    pix_acc_baseline = total_pix_baseline / total_pixels if total_pixels > 0 else 0.0\n",
    "    pix_acc_ppo = total_pix_ppo / total_pixels if total_pixels > 0 else 0.0\n",
    "\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"        FINAL EVALUATION       \")\n",
    "    print(\"===============================\")\n",
    "    print(f\"Baseline Exact Match: {exact_acc_baseline * 100:.2f}%\")\n",
    "    print(f\"PPO Exact Match:      {exact_acc_ppo * 100:.2f}%\")\n",
    "    print(f\"Baseline Pixel Acc:   {pix_acc_baseline * 100:.2f}%\")\n",
    "    print(f\"PPO Pixel Acc:        {pix_acc_ppo * 100:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_final()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c851cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.ppo_actor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PPOActor(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes the mean and log_std for latent Z proposals.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            z_dim, \n",
    "            embed_dim=256\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   D = token embedding dim   #\n",
    "        #   P = num proposals         #\n",
    "        ###############################\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Mean network\n",
    "        self.mu_net = nn.Sequential(\n",
    "            nn.LayerNorm(z_dim),\n",
    "            nn.Linear(z_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, z_dim)\n",
    "        )\n",
    "\n",
    "        # Learnable log_std (per-dimension, shared over B and P)\n",
    "        self.log_std = nn.Parameter(torch.zeros(z_dim))\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            Z   # (B, P, z_dim)\n",
    "    ):\n",
    "        # Mean of Gaussian\n",
    "        mu = self.mu_net(Z)  # (B, P, z_dim)\n",
    "\n",
    "        # Expand log_std to match (B, P, z_dim)\n",
    "        log_std = self.log_std.expand_as(mu)  # (B, P, z_dim)\n",
    "\n",
    "        return mu, log_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.ppo_refiner\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.training.ppo_actor import PPOActor\n",
    "from src.training.ppo_value import PPOValuer\n",
    "\n",
    "\n",
    "class PPORefiner:\n",
    "    \"\"\"\n",
    "    Performs PPO updates on latent Z proposals.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor: PPOActor,\n",
    "        value_fn: PPOValuer,\n",
    "        clip_ratio=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        lr=1e-4,\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   P = num proposals         #\n",
    "        ###############################\n",
    "\n",
    "        self.actor = actor\n",
    "        self.value_fn = value_fn\n",
    "\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        self.opt = torch.optim.Adam(\n",
    "            list(actor.parameters()) + list(value_fn.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "    #######################\n",
    "    #   Gaussian log-prob #\n",
    "    #######################\n",
    "    def _log_prob(self, actions, mu, log_std):\n",
    "        # std\n",
    "        std = torch.exp(log_std)  # (B, P, z_dim)\n",
    "\n",
    "        # Gaussian log prob\n",
    "        # sum over z_dim (the action dimensions)\n",
    "        logp = -0.5 * (((actions - mu) / std) ** 2 + 2 * log_std + torch.log(2 * torch.pi))\n",
    "        logp = logp.sum(dim=-1)  # (B, P)\n",
    "\n",
    "        return logp\n",
    "\n",
    "    #############################\n",
    "    #   Refinement step (TTA)   #\n",
    "    #############################\n",
    "    @torch.no_grad()\n",
    "    def refine(self, Z, steps=1, scale=0.1):\n",
    "        \"\"\"\n",
    "        Test-time heuristic refinement:\n",
    "        small shifts along actor mean.\n",
    "        \"\"\"\n",
    "        Z_new = Z.clone()\n",
    "\n",
    "        for _ in range(steps):\n",
    "            mu, _ = self.actor(Z_new)\n",
    "            Z_new = Z_new + scale * mu\n",
    "\n",
    "        return Z_new\n",
    "\n",
    "    ############################\n",
    "    #        PPO UPDATE        #\n",
    "    ############################\n",
    "    def update(self, Z, actions, old_logp, returns, advantages):\n",
    "        \"\"\"\n",
    "        Z:        (B, P, z_dim)\n",
    "        actions:  (B, P, z_dim)\n",
    "        old_logp: (B, P)\n",
    "        returns:  (B, P)\n",
    "        advantages: (B, P)\n",
    "        \"\"\"\n",
    "\n",
    "        # New policy\n",
    "        mu, log_std = self.actor(Z)  # (B, P, z_dim)\n",
    "\n",
    "        # New log prob\n",
    "        logp = self._log_prob(actions, mu, log_std)  # (B, P)\n",
    "\n",
    "        # PPO ratio\n",
    "        ratio = torch.exp(logp - old_logp)  # (B, P)\n",
    "\n",
    "        # Clipped surrogate objective\n",
    "        clipped_ratio = torch.clamp(\n",
    "            ratio,\n",
    "            1 - self.clip_ratio,\n",
    "            1 + self.clip_ratio\n",
    "        )\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = clipped_ratio * advantages\n",
    "\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # Value loss\n",
    "        values = self.value_fn(Z)  # (B, P)\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "\n",
    "        # Entropy bonus\n",
    "        entropy = 0.5 * (log_std.exp() * torch.sqrt(torch.tensor(2 * torch.pi * torch.e))).mean()\n",
    "        entropy_loss = -self.entropy_coef * entropy\n",
    "\n",
    "        loss = policy_loss + self.value_coef * value_loss + entropy_loss\n",
    "\n",
    "        # Update step\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"policy_loss\": policy_loss.item(),\n",
    "            \"value_loss\": value_loss.item(),\n",
    "            \"entropy\": entropy.item(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.ppo_value\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PPOValuer(nn.Module):\n",
    "    \"\"\"\n",
    "    Value function for latent proposals Z\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            z_dim, \n",
    "            embed_dim=256\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   P = num proposals         #\n",
    "        ###############################\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(z_dim),\n",
    "            nn.Linear(z_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            Z   # (B, P, z_dim)\n",
    "    ):\n",
    "        B, T, D = Z.shape\n",
    "        values = self.net(Z.view(B*T, D)).view(B, T)\n",
    "        return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d618011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.validation_phase1\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import your generator\n",
    "from src.inference.generator import ARCGenerator\n",
    "\n",
    "# Probably in the same file as ARCSampleDataset\n",
    "from src.data_pipeline.dataloader import ARCDataModule\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_phase1(generator, val_loader, device):\n",
    "    generator.eval()\n",
    "\n",
    "    total_exact = 0\n",
    "    total_tests = 0\n",
    "\n",
    "    total_pixels = 0\n",
    "    total_pixels_correct = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "\n",
    "        # Move tensors to device\n",
    "        for k, v in batch.items():\n",
    "            if torch.is_tensor(v):\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "        K_test = batch[\"test_inputs\"].shape[0]\n",
    "\n",
    "        # Forward pass\n",
    "        out = generator(\n",
    "            train_inputs=batch[\"train_inputs\"],\n",
    "            train_outputs=batch[\"train_outputs\"],\n",
    "            train_input_masks=batch[\"train_input_masks\"],\n",
    "            train_output_masks=batch[\"train_output_masks\"],\n",
    "            test_inputs=batch[\"test_inputs\"],\n",
    "            test_input_masks=batch[\"test_input_masks\"],\n",
    "        )\n",
    "\n",
    "        logits = out[\"logits\"]  # (1, K_test, C_out, H, W)\n",
    "        preds = logits.argmax(dim=2)  # (1, K_test, H, W)\n",
    "        preds = preds.squeeze(0)      # (K_test, H, W)\n",
    "\n",
    "        targets = batch[\"test_outputs\"]  # (K_test, H, W)\n",
    "        H, W = targets.shape[1], targets.shape[2]\n",
    "\n",
    "        # For pixel accuracy\n",
    "        for j in range(K_test):\n",
    "            pred = preds[j]\n",
    "            target = targets[j]\n",
    "\n",
    "            # -------------------------------\n",
    "            # Crop prediction back to original size\n",
    "            # -------------------------------\n",
    "            # Provided by your dataset\n",
    "            orig_h, orig_w = batch[\"test_original_size\"].tolist()\n",
    "            pred_cropped = pred[:orig_h, :orig_w]\n",
    "            target_cropped = target[:orig_h, :orig_w]\n",
    "\n",
    "            # ---- Exact Match ----\n",
    "            exact = (pred_cropped == target_cropped).all().item()\n",
    "            total_exact += exact\n",
    "            total_tests += 1\n",
    "\n",
    "            # ---- Pixel Accuracy ----\n",
    "            total_pixels += orig_h * orig_w\n",
    "            total_pixels_correct += (pred_cropped == target_cropped).sum().item()\n",
    "\n",
    "    exact_acc = total_exact / total_tests if total_tests > 0 else 0\n",
    "    pixel_acc = total_pixels_correct / total_pixels if total_pixels > 0 else 0\n",
    "\n",
    "    print(\"\\n=== Validation Results ===\")\n",
    "    print(f\"Exact Match Accuracy: {exact_acc * 100:.2f}%\")\n",
    "    print(f\"Pixel Accuracy:       {pixel_acc * 100:.2f}%\\n\")\n",
    "\n",
    "    return exact_acc, pixel_acc\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Example usage (in a validation script)\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load your trained generator\n",
    "    generator = ARCGenerator(...)  # build same as train script\n",
    "    generator.load_state_dict(torch.load(\"phase1_generator.pt\", map_location=DEVICE))\n",
    "    generator.to(DEVICE)\n",
    "\n",
    "    # Use same loader or a separate validation loader\n",
    "    validate_phase1(generator, ARCDataModule, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.phase1\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your modules\n",
    "from src.inference.generator import ARCGenerator\n",
    "\n",
    "# Encoders & components\n",
    "from src.architecture.context_encoding.example_pair_encoder import ExamplePairEncoder\n",
    "from src.architecture.context_encoding.example_pair_aggregator import ExamplePairAggregator\n",
    "from src.architecture.context_encoding.conditional_encoder import ConditionalTestInputEncoder\n",
    "from src.architecture.LViTM.body import LargeVisionTransformerModel\n",
    "from src.architecture.executor.executor import Executor\n",
    "from src.architecture.ViT.body import VisionTransformer\n",
    "\n",
    "# Training constants\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#   Build MODEL for PHASE 1 using TWO separate ViTs          #\n",
    "##############################################################\n",
    "def build_model():\n",
    "\n",
    "    ##############################################################\n",
    "    #   1. Vision Transformers                                   #\n",
    "    #      vit_pair = (I, O) example pairs, 2 channels           #\n",
    "    #      vit_test = I_test only, 1 channel                     #\n",
    "    ##############################################################\n",
    "    img_size   = 30\n",
    "    patch_size = 1\n",
    "    embed_dim  = 128\n",
    "    num_heads  = 4\n",
    "    depth_vit  = 6\n",
    "    mlp_dim    = 256\n",
    "    z_dim      = 64\n",
    "    num_props  = 4\n",
    "\n",
    "    # Example pair ViT: (I_i, O_i) â†’ 2 channels\n",
    "    vit_pair = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=2\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Test grid ViT: (I_test) â†’ 1 channel\n",
    "    vit_test = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ##############################################################\n",
    "    #   2. Context encoding components                           #\n",
    "    ##############################################################\n",
    "    example_encoder = ExamplePairEncoder(vit_pair).to(DEVICE)\n",
    "\n",
    "    aggregator = ExamplePairAggregator(\n",
    "        embed_dim=vit_pair.c_token.size(-1)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    cond_encoder = ConditionalTestInputEncoder(vit_test).to(DEVICE)\n",
    "\n",
    "    ##############################################################\n",
    "    #   3. LVITM (reasoning / proposal generation)               #\n",
    "    ##############################################################\n",
    "    lvitm = LargeVisionTransformerModel(\n",
    "        embed_dim=vit_pair.c_token.size(-1),\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=8,\n",
    "        num_proposals=num_props,\n",
    "        z_dim=z_dim\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ##############################################################\n",
    "    #   4. Executor                                              #\n",
    "    ##############################################################\n",
    "    executor = Executor(\n",
    "        embed_dim=vit_pair.c_token.size(-1),\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=4,\n",
    "        z_dim=z_dim,\n",
    "        hidden_channels=64,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ##############################################################\n",
    "    #   5. Wrap everything into ARCGenerator                     #\n",
    "    ##############################################################\n",
    "    generator = ARCGenerator(\n",
    "        example_pair_encoder=example_encoder,\n",
    "        aggregator=aggregator,\n",
    "        cond_encoder=cond_encoder,\n",
    "        lvitm=lvitm,\n",
    "        executor=executor\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#   Phase 1 Supervised Training                              #\n",
    "##############################################################\n",
    "def train_phase1(arc_loader):\n",
    "\n",
    "    generator = build_model()\n",
    "    optimizer = Adam(generator.parameters(), lr=LR)\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS), \"Epoch:\"):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
    "\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for batch in tqdm(arc_loader, \"Batch:\"):\n",
    "            ############################################################\n",
    "            #   Move batch to device                                  #\n",
    "            ############################################################\n",
    "            for k, v in tqdm(batch.items(), \"Sample:\"):\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = v.to(DEVICE)\n",
    "\n",
    "            train_inputs       = batch[\"train_inputs\"]\n",
    "            train_outputs      = batch[\"train_outputs\"]\n",
    "            train_input_masks  = batch[\"train_input_masks\"]\n",
    "            train_output_masks = batch[\"train_output_masks\"]\n",
    "\n",
    "            test_inputs        = batch[\"test_inputs\"]\n",
    "            test_outputs       = batch[\"test_outputs\"]\n",
    "            test_input_masks   = batch[\"test_input_masks\"]\n",
    "\n",
    "            ############################################################\n",
    "            #   Forward through ARCGenerator                           #\n",
    "            ############################################################\n",
    "            out = generator(\n",
    "                train_inputs=train_inputs,\n",
    "                train_outputs=train_outputs,\n",
    "                train_input_masks=train_input_masks,\n",
    "                train_output_masks=train_output_masks,\n",
    "                test_inputs=test_inputs,\n",
    "                test_input_masks=test_input_masks,\n",
    "            )\n",
    "\n",
    "            logits = out[\"logits\"]  # (B, K_test, C_out, H, W)\n",
    "            B, K_test, C_out, H, W = logits.shape\n",
    "\n",
    "            ############################################################\n",
    "            #   CE loss: reshape logits & targets                     #\n",
    "            ############################################################\n",
    "            logits_flat = logits.view(B * K_test, C_out, H, W)\n",
    "            target_flat = test_outputs.view(B * K_test, H, W)\n",
    "\n",
    "            loss = F.cross_entropy(logits_flat, target_flat, ignore_index=0)\n",
    "\n",
    "            ###########################\n",
    "            #   Backprop & optimize   #\n",
    "            ###########################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        avg_loss = total_loss / count\n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#   Script Entry Point                                        #\n",
    "# ##############################################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     from src.data_pipeline.dataloader import ARCDataModule\n",
    "\n",
    "#     model = train_phase1(ARCDataModule)\n",
    "#     torch.save(model.state_dict(), \"phase1_generator.pt\")\n",
    "#     print(\"Saved Phase 1 generator.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01419435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.phase2\n",
    "import torch\n",
    "from torch import autograd\n",
    "\n",
    "from src.architecture.ViT.body import VisionTransformer\n",
    "from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "from src.data_pipeline.dataloader import ARCDataModule\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Hyperparameters           #\n",
    "###############################\n",
    "\n",
    "CRITIC_LR = 1e-4\n",
    "CRITIC_EPOCHS = 5\n",
    "LAMBDA_GP = 10.0\n",
    "NUM_CLASSES = 10  # ARC colors\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Build Critic + ViT        #\n",
    "###############################\n",
    "\n",
    "def build_critic():\n",
    "    from src.architecture.ViT.body import VisionTransformer\n",
    "    from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "\n",
    "    img_size   = 30\n",
    "    patch_size = 1\n",
    "    embed_dim  = 128\n",
    "    num_heads  = 4\n",
    "    depth_vit  = 6\n",
    "    mlp_dim    = 256\n",
    "\n",
    "    # IMPORTANT: ALWAYS 2 CHANNELS\n",
    "    #   ch1 = I_test  (1 channel)\n",
    "    #   ch2 = O_real or O_fake (1 channel)\n",
    "    vit_critic = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=2       \n",
    "    ).to(DEVICE)\n",
    "\n",
    "    critic = AdversarialVisionTransformer(\n",
    "        vit_encoder=vit_critic,\n",
    "        z_dim=None,\n",
    "        c_dim=None,\n",
    "        hidden_dim=256\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return critic\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Fake Output Generator     #\n",
    "###############################\n",
    "\n",
    "def make_fake_outputs(\n",
    "        real_outputs,   # (K_test, H, W)\n",
    "        mask            # (K_test, H, W)\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates fake outputs by sampling random colors where mask == 1.\n",
    "    \"\"\"\n",
    "\n",
    "    K_test, H, W = real_outputs.shape\n",
    "    fake = torch.randint(\n",
    "        low=0,\n",
    "        high=NUM_CLASSES,\n",
    "        size=(K_test, H, W),\n",
    "        device=real_outputs.device\n",
    "    )\n",
    "\n",
    "    # Keep padding as 0; only randomize valid cells\n",
    "    fake = torch.where(mask.bool(), fake, torch.zeros_like(fake))\n",
    "\n",
    "    return fake\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Gradient Penalty (GP)     #\n",
    "###############################\n",
    "\n",
    "def gradient_penalty(\n",
    "        critic,\n",
    "        I_real,    # (B, 1, H, W)\n",
    "        O_real,    # (B, 1, H, W)\n",
    "        O_fake,    # (B, 1, H, W)\n",
    "        mask_in,   # (B, H, W)\n",
    "        mask_out   # (B, H, W)\n",
    "):\n",
    "    \"\"\"\n",
    "    WGAN-GP gradient penalty on interpolated outputs.\n",
    "    Only interpolates in the output space for simplicity.\n",
    "    \"\"\"\n",
    "\n",
    "    B, _, H, W = O_real.shape\n",
    "\n",
    "    # Interpolate between real and fake outputs\n",
    "    epsilon = torch.rand(B, 1, 1, 1, device=O_real.device)\n",
    "    O_interp = epsilon * O_real + (1.0 - epsilon) * O_fake\n",
    "\n",
    "    # Enable gradient tracking\n",
    "    O_interp.requires_grad_(True)\n",
    "\n",
    "    # Input grid stays fixed (we could also interpolate I, but ARC rules are usually about O)\n",
    "    I_interp = I_real\n",
    "\n",
    "    # Critic score on interpolated pair\n",
    "    scores = critic(\n",
    "        I_in=I_interp,\n",
    "        O_pred=O_interp,\n",
    "        mask_in=mask_in,\n",
    "        mask_out=mask_out,\n",
    "        z=None,\n",
    "        C=None\n",
    "    )  # (B,)\n",
    "\n",
    "    # Compute gradients of scores w.r.t O_interp\n",
    "    grads = autograd.grad(\n",
    "        outputs=scores.sum(),\n",
    "        inputs=O_interp,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]  # (B, 1, H, W)\n",
    "\n",
    "    grads = grads.view(B, -1)  # (B, H*W)\n",
    "    grad_norm = grads.norm(2, dim=1)  # (B,)\n",
    "\n",
    "    gp = ((grad_norm - 1.0) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Critic Warmup Loop        #\n",
    "###############################\n",
    "\n",
    "def train_critic_phase2(critic, data_loader):\n",
    "\n",
    "    critic.train()\n",
    "    optimizer = torch.optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "    for epoch in range(CRITIC_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        print(f\"\\n=== Critic Warmup Epoch {epoch + 1}/{CRITIC_EPOCHS} ===\")\n",
    "\n",
    "        for batch in data_loader:\n",
    "\n",
    "            ###############################\n",
    "            #   Move tensors to device    #\n",
    "            ###############################\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = v.to(DEVICE)\n",
    "\n",
    "            test_inputs = batch[\"test_inputs\"]          # (K_test, H, W)\n",
    "            test_outputs = batch[\"test_outputs\"]        # (K_test, H, W)\n",
    "            test_input_masks = batch[\"test_input_masks\"]    # (K_test, H, W)\n",
    "            test_output_masks = batch[\"test_output_masks\"]  # (K_test, H, W)\n",
    "\n",
    "            K_test, H, W = test_inputs.shape\n",
    "            if K_test == 0:\n",
    "                continue\n",
    "\n",
    "            ################################\n",
    "            #   Prepare real / fake pairs  #\n",
    "            ################################\n",
    "\n",
    "            # Real input/output (B = K_test)\n",
    "            I_real = test_inputs.unsqueeze(1).float()       # (B, 1, H, W)\n",
    "            O_real = test_outputs.unsqueeze(1).float()      # (B, 1, H, W)\n",
    "\n",
    "            mask_in = test_input_masks.bool()               # (B, H, W)\n",
    "            mask_out = test_output_masks.bool()             # (B, H, W)\n",
    "\n",
    "            # Fake outputs\n",
    "            O_fake_raw = make_fake_outputs(\n",
    "                real_outputs=test_outputs,\n",
    "                mask=test_output_masks\n",
    "            )  # (B, H, W)\n",
    "            O_fake = O_fake_raw.unsqueeze(1).float()        # (B, 1, H, W)\n",
    "\n",
    "            ###############################\n",
    "            #   Critic Scores             #\n",
    "            ###############################\n",
    "\n",
    "            # Real scores\n",
    "            score_real = critic(\n",
    "                I_in=I_real,\n",
    "                O_pred=O_real,\n",
    "                mask_in=mask_in,\n",
    "                mask_out=mask_out,\n",
    "                z=None,\n",
    "                C=None\n",
    "            )  # (B,)\n",
    "\n",
    "            # Fake scores\n",
    "            score_fake = critic(\n",
    "                I_in=I_real,\n",
    "                O_pred=O_fake,\n",
    "                mask_in=mask_in,\n",
    "                mask_out=mask_out,\n",
    "                z=None,\n",
    "                C=None\n",
    "            )  # (B,)\n",
    "\n",
    "            wasserstein = score_fake.mean() - score_real.mean()\n",
    "\n",
    "            # Gradient penalty\n",
    "            gp = gradient_penalty(\n",
    "                critic=critic,\n",
    "                I_real=I_real,\n",
    "                O_real=O_real,\n",
    "                O_fake=O_fake,\n",
    "                mask_in=mask_in,\n",
    "                mask_out=mask_out\n",
    "            )\n",
    "\n",
    "            loss = wasserstein + LAMBDA_GP * gp\n",
    "\n",
    "            ###############################\n",
    "            #   Optimize Critic           #\n",
    "            ###############################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / max(total_batches, 1)\n",
    "        print(f\"Epoch {epoch + 1}: Critic Warmup Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    return critic\n",
    "\n",
    "\n",
    "#######################\n",
    "#   Main Entrypoint   #\n",
    "#######################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    critic = build_critic()\n",
    "    critic = train_critic_phase2(critic, ARCDataModule)\n",
    "    torch.save(critic.state_dict(), \"critic_phase2_warmup.pt\")\n",
    "    print(\"\\nSaved critic after Phase 2 warmup: critic_phase2_warmup.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.phase3\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from src.inference.generator import ARCGenerator\n",
    "\n",
    "from src.architecture.context_encoding.example_pair_encoder import ExamplePairEncoder\n",
    "from src.architecture.context_encoding.example_pair_aggregator import ExamplePairAggregator\n",
    "from src.architecture.context_encoding.conditional_encoder import ConditionalTestInputEncoder\n",
    "from src.architecture.ViT.body import VisionTransformer\n",
    "from src.architecture.LViTM.body import LargeVisionTransformerModel\n",
    "from src.architecture.executor.executor import Executor\n",
    "from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "from src.data_pipeline.dataloader import ARCDataModule\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Hyperparameters           #\n",
    "###############################\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "GEN_LR = 1e-4\n",
    "CRITIC_LR = 1e-4\n",
    "\n",
    "LAMBDA_GP = 10.0\n",
    "LAMBDA_ADV = 0.1\n",
    "\n",
    "N_CRITIC = 5  # critic steps per generator step\n",
    "\n",
    "NUM_CLASSES = 11  # ARC colors\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Helper: grad toggle       #\n",
    "###############################\n",
    "\n",
    "def set_requires_grad(module, flag: bool):\n",
    "    \"\"\"\n",
    "    Enables or disables gradients for a module.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Build Generator           #\n",
    "###############################\n",
    "\n",
    "def build_generator():\n",
    "    from src.architecture.ViT.body import VisionTransformer\n",
    "\n",
    "    img_size   = 30\n",
    "    patch_size = 1\n",
    "    embed_dim  = 128\n",
    "    num_heads  = 4\n",
    "    depth_vit  = 6\n",
    "    mlp_dim    = 256\n",
    "    z_dim      = 64\n",
    "    num_props  = 4\n",
    "    NUM_CLASSES = 11  \n",
    "\n",
    "    # Two separate ViTs\n",
    "    vit_pair = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=2          # (I, O)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    vit_test = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=1          # I_test\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    example_encoder = ExamplePairEncoder(vit_pair).to(DEVICE)\n",
    "\n",
    "    aggregator = ExamplePairAggregator(embed_dim=embed_dim).to(DEVICE)\n",
    "\n",
    "    cond_encoder = ConditionalTestInputEncoder(vit_test).to(DEVICE)\n",
    "\n",
    "    lvitm = LargeVisionTransformerModel(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=8,\n",
    "        num_proposals=num_props,\n",
    "        z_dim=z_dim\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    executor = Executor(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=4,\n",
    "        z_dim=z_dim,\n",
    "        hidden_channels=64,\n",
    "        num_classes=NUM_CLASSES  \n",
    "    ).to(DEVICE)\n",
    "\n",
    "    generator = ARCGenerator(\n",
    "        example_pair_encoder=example_encoder,\n",
    "        aggregator=aggregator,\n",
    "        cond_encoder=cond_encoder,\n",
    "        lvitm=lvitm,\n",
    "        executor=executor\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Build Critic              #\n",
    "###############################\n",
    "\n",
    "def build_critic():\n",
    "    from src.architecture.ViT.body import VisionTransformer\n",
    "    from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "\n",
    "    img_size   = 30\n",
    "    patch_size = 1\n",
    "    embed_dim  = 128\n",
    "    num_heads  = 4\n",
    "    depth_vit  = 6\n",
    "    mlp_dim    = 256\n",
    "\n",
    "    # IMPORTANT: ALWAYS 2 CHANNELS\n",
    "    #   ch1 = I_test  (1 channel)\n",
    "    #   ch2 = O_real or O_fake (1 channel)\n",
    "    vit_critic = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=2       \n",
    "    ).to(DEVICE)\n",
    "\n",
    "    critic = AdversarialVisionTransformer(\n",
    "        vit_encoder=vit_critic,\n",
    "        z_dim=None,\n",
    "        c_dim=None,\n",
    "        hidden_dim=256\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return critic\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "#   One-hot encoder           #\n",
    "###############################\n",
    "\n",
    "def one_hot_from_int(\n",
    "        grid,       # (B, H, W) int\n",
    "        num_classes # scalar\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts integer grid to one-hot channels: (B, num_classes, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    B, H, W = grid.shape\n",
    "    # (B, H, W, C)\n",
    "    oh = F.one_hot(grid.long().clamp(min=0, max=num_classes - 1), num_classes=num_classes)\n",
    "    oh = oh.permute(0, 3, 1, 2).float()  # (B, C, H, W)\n",
    "    return oh\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Gradient Penalty (GP)     #\n",
    "###############################\n",
    "\n",
    "def gradient_penalty(critic, I_real, O_real, I_fake, O_fake, device):\n",
    "    \"\"\"\n",
    "    Computes WGAN-GP gradient penalty for the critic.\n",
    "    All inputs are (B,1,H,W).\n",
    "    \"\"\"\n",
    "\n",
    "    # Interpolate input grid\n",
    "    epsilon = torch.rand(I_real.size(0), 1, 1, 1, device=device)\n",
    "    I_interp = epsilon * I_real + (1 - epsilon) * I_fake\n",
    "    I_interp.requires_grad_(True)\n",
    "\n",
    "    # Interpolate output grid\n",
    "    epsilon2 = torch.rand(O_real.size(0), 1, 1, 1, device=device)\n",
    "    O_interp = epsilon2 * O_real + (1 - epsilon2) * O_fake\n",
    "    O_interp.requires_grad_(True)\n",
    "\n",
    "    # Forward pass through critic\n",
    "    scores = critic(I_interp, O_interp)  # (B,)\n",
    "\n",
    "    # Compute grad w.r.t BOTH I and O\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=scores,\n",
    "        inputs=[I_interp, O_interp],\n",
    "        grad_outputs=torch.ones_like(scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )\n",
    "\n",
    "    # grads is a tuple: (grad_I, grad_O)\n",
    "    grad_I, grad_O = grads\n",
    "\n",
    "    # Combine the norms\n",
    "    grad_norm = (\n",
    "        grad_I.reshape(grad_I.size(0), -1).norm(2, dim=1) +\n",
    "        grad_O.reshape(grad_O.size(0), -1).norm(2, dim=1)\n",
    "    )\n",
    "\n",
    "    gp = ((grad_norm - 1) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Phase 3 Training Loop     #\n",
    "###############################\n",
    "\n",
    "def train_phase3_adversarial(\n",
    "        generator,\n",
    "        critic,\n",
    "        data_loader\n",
    "):\n",
    "    \"\"\"\n",
    "    Joint adversarial training of generator + critic.\n",
    "    Supervised CE + WGAN-GP adversarial loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Optionally load checkpoints from Phase 1 and Phase 2\n",
    "    try:\n",
    "        generator.load_state_dict(torch.load(\"phase1_generator.pt\", map_location=DEVICE))\n",
    "        print(\"Loaded Phase 1 generator checkpoint.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Phase 1 generator checkpoint not found; training generator from scratch.\")\n",
    "\n",
    "    try:\n",
    "        critic.load_state_dict(torch.load(\"critic_phase2_warmup.pt\", map_location=DEVICE))\n",
    "        print(\"Loaded Phase 2 critic checkpoint.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Phase 2 critic checkpoint not found; training critic from scratch.\")\n",
    "\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=GEN_LR)\n",
    "    critic_opt = torch.optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n=== Phase 3 Epoch {epoch + 1}/{EPOCHS} ===\")\n",
    "        total_gen_loss = 0.0\n",
    "        total_critic_loss = 0.0\n",
    "        n_gen_steps = 0\n",
    "        n_critic_steps = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "            ###############################\n",
    "            #   Move batch to device      #\n",
    "            ###############################\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = v.to(DEVICE)\n",
    "\n",
    "            train_inputs       = batch[\"train_inputs\"]        # (K_train, H, W)\n",
    "            train_outputs      = batch[\"train_outputs\"]       # (K_train, H, W)\n",
    "            train_input_masks  = batch[\"train_input_masks\"]   # (K_train, H, W)\n",
    "            train_output_masks = batch[\"train_output_masks\"]  # (K_train, H, W)\n",
    "\n",
    "            test_inputs        = batch[\"test_inputs\"]         # (K_test, H, W)\n",
    "            test_outputs       = batch[\"test_outputs\"]        # (K_test, H, W)\n",
    "            test_input_masks   = batch[\"test_input_masks\"]    # (K_test, H, W)\n",
    "            test_output_masks  = batch[\"test_output_masks\"]   # (K_test, H, W)\n",
    "\n",
    "            ###############################\n",
    "            #   Forward: Generator        #\n",
    "            ###############################\n",
    "\n",
    "            # ARCGenerator handles B=1 internally\n",
    "            gen_out = generator(\n",
    "                train_inputs=train_inputs,\n",
    "                train_outputs=train_outputs,\n",
    "                train_input_masks=train_input_masks,\n",
    "                train_output_masks=train_output_masks,\n",
    "                test_inputs=test_inputs,\n",
    "                test_input_masks=test_input_masks,\n",
    "            )\n",
    "\n",
    "            logits = gen_out[\"logits\"]  # (1, K_test, C_out, H, W)\n",
    "            B, K_test, C_out, H, W = logits.shape\n",
    "            assert B == 1, \"Phase 3 currently assumes batch_size=1 per task.\"\n",
    "\n",
    "            # -------------------------------\n",
    "            #   Critic updates (N_CRITIC)    #\n",
    "            # -------------------------------\n",
    "\n",
    "            for _ in range(N_CRITIC):\n",
    "                set_requires_grad(critic, True)\n",
    "                set_requires_grad(generator, False)\n",
    "\n",
    "                critic_opt.zero_grad()\n",
    "\n",
    "                # Use detached logits for critic training\n",
    "                logits_det = logits.detach().view(K_test, C_out, H, W)  # (B*K_test, C_out,H,W)\n",
    "\n",
    "                # Real outputs: one-hot\n",
    "                targets = test_outputs.view(K_test, H, W)               # (K_test,H,W)\n",
    "                O_real = one_hot_from_int(targets, NUM_CLASSES)        # (K_test, C_out, H, W)\n",
    "\n",
    "                # Inputs\n",
    "                I_real = test_inputs.view(K_test, H, W).unsqueeze(1).float()  # (K_test,1,H,W)\n",
    "\n",
    "                # Masks\n",
    "                mask_in = test_input_masks.view(K_test, H, W).bool()\n",
    "                mask_out = test_output_masks.view(K_test, H, W).bool()\n",
    "\n",
    "                # Critic scores\n",
    "                score_real = critic(\n",
    "                    I_in=I_real,\n",
    "                    O_pred=O_real,\n",
    "                    mask_in=mask_in,\n",
    "                    mask_out=mask_out,\n",
    "                    z=None,\n",
    "                    C=None\n",
    "                )  # (K_test,)\n",
    "\n",
    "                score_fake = critic(\n",
    "                    I_in=I_real,\n",
    "                    O_pred=logits_det,\n",
    "                    mask_in=mask_in,\n",
    "                    mask_out=mask_out,\n",
    "                    z=None,\n",
    "                    C=None\n",
    "                )  # (K_test,)\n",
    "\n",
    "                wasserstein = score_fake.mean() - score_real.mean()\n",
    "                gp = gradient_penalty(\n",
    "                    critic,\n",
    "                    I_real,\n",
    "                    O_real,\n",
    "                    I_real,\n",
    "                    logits_det,\n",
    "                    DEVICE\n",
    "                )\n",
    "\n",
    "                critic_loss = wasserstein + LAMBDA_GP * gp\n",
    "\n",
    "                critic_loss.backward()\n",
    "                critic_opt.step()\n",
    "\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                n_critic_steps += 1\n",
    "\n",
    "            ###############################\n",
    "            #   Generator update          #\n",
    "            ###############################\n",
    "\n",
    "            set_requires_grad(critic, False)\n",
    "            set_requires_grad(generator, True)\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            # Supervised CE loss across all test pairs\n",
    "            logits_flat = logits.view(B * K_test, C_out, H, W)         # (K_test,C,H,W)\n",
    "            targets_flat = test_outputs.view(B * K_test, H, W).long()  # (K_test,H,W)\n",
    "\n",
    "            ce_loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "            # Adversarial loss: - E[critic(I, O_fake)]\n",
    "            logits_for_adv = logits.view(K_test, C_out, H, W)          # (K_test,C,H,W)\n",
    "\n",
    "            I_for_adv = test_inputs.view(K_test, H, W).unsqueeze(1).float()\n",
    "            mask_in_adv = test_input_masks.view(K_test, H, W).bool()\n",
    "            mask_out_adv = test_output_masks.view(K_test, H, W).bool()\n",
    "\n",
    "            fake_scores = critic(\n",
    "                I_in=I_for_adv,\n",
    "                O_pred=logits_for_adv,\n",
    "                mask_in=mask_in_adv,\n",
    "                mask_out=mask_out_adv,\n",
    "                z=None,\n",
    "                C=None\n",
    "            )  # (K_test,)\n",
    "\n",
    "            gen_adv_loss = -fake_scores.mean()\n",
    "\n",
    "            gen_loss = ce_loss + LAMBDA_ADV * gen_adv_loss\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            total_gen_loss += gen_loss.item()\n",
    "            n_gen_steps += 1\n",
    "\n",
    "        avg_gen = total_gen_loss / max(n_gen_steps, 1)\n",
    "        avg_critic = total_critic_loss / max(n_critic_steps, 1)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Gen Loss = {avg_gen:.4f}, Critic Loss = {avg_critic:.4f}\")\n",
    "\n",
    "    return generator, critic\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Main Entrypoint           #\n",
    "###############################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generator = build_generator()\n",
    "    critic = build_critic()\n",
    "\n",
    "    generator, critic = train_phase3_adversarial(\n",
    "        generator=generator,\n",
    "        critic=critic,\n",
    "        data_loader=ARCDataModule\n",
    "    )\n",
    "\n",
    "    torch.save(generator.state_dict(), \"generator_phase3_adv.pt\")\n",
    "    torch.save(critic.state_dict(), \"critic_phase3_adv.pt\")\n",
    "\n",
    "    print(\"\\nSaved Phase 3 generator and critic checkpoints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.phase4\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.training.ppo_actor import PPOActor\n",
    "from src.training.ppo_value import PPOValuer\n",
    "from src.training.ppo_refiner import PPORefiner\n",
    "\n",
    "from src.architecture.context_encoding.example_pair_encoder import ExamplePairEncoder\n",
    "from src.architecture.context_encoding.example_pair_aggregator import ExamplePairAggregator\n",
    "from src.architecture.context_encoding.conditional_encoder import ConditionalTestInputEncoder\n",
    "from src.architecture.ViT.body import VisionTransformer\n",
    "from src.architecture.LViTM.body import LargeVisionTransformerModel\n",
    "from src.architecture.executor.executor import Executor\n",
    "from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "from src.data_pipeline.dataloader import ARCDataModule\n",
    "\n",
    "from src.inference.execution_controller import HybridExecuteController\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Hyperparameters           #\n",
    "###############################\n",
    "\n",
    "PPO_EPOCHS = 3\n",
    "PPO_STEPS = 3\n",
    "PPO_GAMMA = 0.99\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Build Generator Modules   #\n",
    "###############################\n",
    "\n",
    "def build_generator_components():\n",
    "    \"\"\"\n",
    "    Rebuilds generator components individually (not the ARCGenerator wrapper).\n",
    "    Used to compute context C and supply modules to the controller.\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################\n",
    "    #   Shared Hyperparams    #\n",
    "    ###########################\n",
    "\n",
    "    img_size   = 30\n",
    "    patch_size = 1\n",
    "    embed_dim  = 128\n",
    "    num_heads  = 4\n",
    "    depth_vit  = 6\n",
    "    mlp_dim    = 256\n",
    "    z_dim      = 64\n",
    "    num_props  = 4\n",
    "\n",
    "    ###############################\n",
    "    #   Vision Transformers       #\n",
    "    ###############################\n",
    "\n",
    "    # For example pairs (I, O) -> 2 channels\n",
    "    vit_pair = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=2\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # For test input I_test -> 1 channel\n",
    "    vit_test = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ###########################################\n",
    "    #   Context Encoders (Pair + Aggregator)  #\n",
    "    ###########################################\n",
    "\n",
    "    # Example pair encoder uses ViT with 2 input channels\n",
    "    example_encoder = ExamplePairEncoder(vit_pair).to(DEVICE)\n",
    "\n",
    "    # Aggregator uses the same embedding dimension as ViTs\n",
    "    aggregator = ExamplePairAggregator(\n",
    "        embed_dim=vit_pair.c_token.size(-1)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ###########################################\n",
    "    #   Conditional Test Input Encoder        #\n",
    "    ###########################################\n",
    "\n",
    "    # Conditional encoder uses the 1-channel ViT\n",
    "    cond_encoder = ConditionalTestInputEncoder(vit_test).to(DEVICE)\n",
    "\n",
    "    ###############################\n",
    "    #   Latent Proposal Model     #\n",
    "    ###############################\n",
    "\n",
    "    lvitm = LargeVisionTransformerModel(\n",
    "        embed_dim=vit_pair.c_token.size(-1),\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=8,\n",
    "        num_proposals=num_props,\n",
    "        z_dim=z_dim\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    ###############################\n",
    "    #   Executor                  #\n",
    "    ###############################\n",
    "\n",
    "    executor = Executor(\n",
    "        embed_dim=vit_pair.c_token.size(-1),\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        depth=4,\n",
    "        z_dim=z_dim,\n",
    "        hidden_channels=64,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return example_encoder, aggregator, cond_encoder, lvitm, executor\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Build Critic              #\n",
    "###############################\n",
    "\n",
    "def build_critic():\n",
    "    from src.architecture.ViT.body import VisionTransformer\n",
    "    from src.architecture.adViT.critic import AdversarialVisionTransformer\n",
    "\n",
    "    img_size   = 30\n",
    "    patch_size = 1\n",
    "    embed_dim  = 128\n",
    "    num_heads  = 4\n",
    "    depth_vit  = 6\n",
    "    mlp_dim    = 256\n",
    "\n",
    "    # IMPORTANT: ALWAYS 2 CHANNELS\n",
    "    #   ch1 = I_test  (1 channel)\n",
    "    #   ch2 = O_real or O_fake (1 channel)\n",
    "    vit_critic = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=depth_vit,\n",
    "        mlp_dim=mlp_dim,\n",
    "        in_channels=2      \n",
    "    ).to(DEVICE)\n",
    "\n",
    "    critic = AdversarialVisionTransformer(\n",
    "        vit_encoder=vit_critic,\n",
    "        z_dim=None,\n",
    "        c_dim=None,\n",
    "        hidden_dim=256\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return critic\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Compute Context C         #\n",
    "###############################\n",
    "\n",
    "def compute_context_C(\n",
    "        example_encoder: ExamplePairEncoder,\n",
    "        aggregator: ExamplePairAggregator,\n",
    "        train_inputs: torch.Tensor,        # (K_train,H,W)\n",
    "        train_outputs: torch.Tensor,       # (K_train,H,W)\n",
    "        train_input_masks: torch.Tensor,   # (K_train,H,W)\n",
    "        train_output_masks: torch.Tensor   # (K_train,H,W)\n",
    "):\n",
    "    \"\"\"\n",
    "    Encodes all training example pairs and aggregates into context C.\n",
    "    \"\"\"\n",
    "\n",
    "    if train_inputs.dim() == 3:\n",
    "        train_inputs = train_inputs.unsqueeze(0)\n",
    "        train_outputs = train_outputs.unsqueeze(0)\n",
    "        train_input_masks = train_input_masks.unsqueeze(0)\n",
    "        train_output_masks = train_output_masks.unsqueeze(0)\n",
    "\n",
    "    B, K_train, H, W = train_inputs.shape\n",
    "\n",
    "    h_list = []\n",
    "\n",
    "    for k in range(K_train):\n",
    "        I_k = train_inputs[:, k].unsqueeze(1).float()\n",
    "        O_k = train_outputs[:, k].unsqueeze(1).float()\n",
    "\n",
    "        mask_I_k = train_input_masks[:, k]\n",
    "        mask_O_k = train_output_masks[:, k]\n",
    "\n",
    "        h_k = example_encoder(\n",
    "            I_i=I_k,\n",
    "            O_i=O_k,\n",
    "            mask_I=mask_I_k,\n",
    "            mask_O=mask_O_k\n",
    "        )  # (B,D)\n",
    "        h_list.append(h_k)\n",
    "\n",
    "    h = torch.stack(h_list, dim=1)   # (B,K_train,D)\n",
    "    pair_mask = None\n",
    "\n",
    "    C = aggregator(h, mask=pair_mask)  # (B,D)\n",
    "    return C\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Phase 4 PPO Training      #\n",
    "###############################\n",
    "\n",
    "def train_phase4_ppo(data_loader: ARCDataModule):\n",
    "    # Build components\n",
    "    example_encoder, aggregator, cond_encoder, lvitm, executor = build_generator_components()\n",
    "    critic = build_critic()\n",
    "\n",
    "    # Load Phase 3 checkpoints if available\n",
    "    try:\n",
    "        gen_state = torch.load(\"generator_phase3_adv.pt\", map_location=DEVICE)\n",
    "        print(\"Loaded generator_phase3_adv.pt into generator components (partial load).\")\n",
    "        # You can do partial loads into submodules here if you saved them modularly.\n",
    "    except FileNotFoundError:\n",
    "        print(\"Phase 3 generator checkpoint not found. Using fresh generator components.\")\n",
    "\n",
    "    try:\n",
    "        critic.load_state_dict(torch.load(\"critic_phase3_adv.pt\", map_location=DEVICE))\n",
    "        print(\"Loaded critic_phase3_adv.pt.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Phase 3 critic checkpoint not found. Using fresh critic.\")\n",
    "\n",
    "    # Controller with critic\n",
    "    controller = HybridExecuteController(\n",
    "        lvitm=lvitm,\n",
    "        executor=executor,\n",
    "        cond_encoder=cond_encoder,\n",
    "        critic=critic\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # PPO modules\n",
    "    z_dim = 64   # must match LViTM z_dim\n",
    "    actor = PPOActor(z_dim=z_dim, embed_dim=256).to(DEVICE)\n",
    "    value_fn = PPOValuer(z_dim=z_dim, embed_dim=256).to(DEVICE)\n",
    "    ppo_refiner = PPORefiner(actor=actor, value_fn=value_fn, lr=1e-4)\n",
    "\n",
    "    for epoch in range(PPO_EPOCHS):\n",
    "        print(f\"\\n=== Phase 4 PPO Epoch {epoch + 1}/{PPO_EPOCHS} ===\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "            ###############################\n",
    "            #   Move batch to device      #\n",
    "            ###############################\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = v.to(DEVICE)\n",
    "\n",
    "            train_inputs       = batch[\"train_inputs\"]\n",
    "            train_outputs      = batch[\"train_outputs\"]\n",
    "            train_input_masks  = batch[\"train_input_masks\"]\n",
    "            train_output_masks = batch[\"train_output_masks\"]\n",
    "\n",
    "            test_inputs        = batch[\"test_inputs\"]\n",
    "            test_outputs       = batch[\"test_outputs\"]\n",
    "            test_input_masks   = batch[\"test_input_masks\"]\n",
    "\n",
    "            # Compute context C\n",
    "            C = compute_context_C(\n",
    "                example_encoder=example_encoder,\n",
    "                aggregator=aggregator,\n",
    "                train_inputs=train_inputs,\n",
    "                train_outputs=train_outputs,\n",
    "                train_input_masks=train_input_masks,\n",
    "                train_output_masks=train_output_masks\n",
    "            )  # (1,D)\n",
    "\n",
    "            # Pick first test input as initial grid\n",
    "            if test_inputs.dim() == 3:\n",
    "                test_inputs = test_inputs.unsqueeze(0)\n",
    "                test_input_masks = test_input_masks.unsqueeze(0)\n",
    "                test_outputs = test_outputs.unsqueeze(0)\n",
    "\n",
    "            B, K_test, H, W = test_inputs.shape\n",
    "            init_grid = test_inputs[:, 0].unsqueeze(1).float()       # (B,1,H,W)\n",
    "            init_mask = test_input_masks[:, 0]                        # (B,H,W)\n",
    "            target = test_outputs[:, 0]                               # (B,H,W)\n",
    "\n",
    "            ###############################\n",
    "            #   PPO Rollout & Update      #\n",
    "            ###############################\n",
    "\n",
    "            final_logits, ppo_stats = controller.ppo_rollout_and_update(\n",
    "                init_grid=init_grid,\n",
    "                init_mask=init_mask,\n",
    "                C=C,\n",
    "                ppo_refiner=ppo_refiner,\n",
    "                num_steps=PPO_STEPS,\n",
    "                gamma=PPO_GAMMA\n",
    "            )\n",
    "\n",
    "            # Optional: supervised signal on final prediction\n",
    "            pred_loss = F.cross_entropy(\n",
    "                final_logits,          # (B,C_out,H,W)\n",
    "                target.long()          # (B,H,W)\n",
    "            )\n",
    "\n",
    "            pred_loss.backward()\n",
    "            # NOTE: If you want to train executor/LViTM jointly with PPO, you can\n",
    "            # attach an optimizer here and step it. For now, PPORefiner.update()\n",
    "            # already steps the actor/value networks.\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}: \"\n",
    "                  f\"PPO loss={ppo_stats['loss']:.4f}, \"\n",
    "                  f\"policy={ppo_stats['policy_loss']:.4f}, \"\n",
    "                  f\"value={ppo_stats['value_loss']:.4f}, \"\n",
    "                  f\"entropy={ppo_stats['entropy']:.4f}\")\n",
    "            \n",
    "    return actor, value_fn\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_phase4_ppo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7aa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.training.train_orchestrator\n",
    "import argparse\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from src.training.checkpoints import save_checkpoint, load_checkpoint\n",
    "\n",
    "###############################\n",
    "#   Device                    #\n",
    "###############################\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Imports for each phase    #\n",
    "###############################\n",
    "\n",
    "# Phase 1\n",
    "from src.training.phase1 import train_phase1          # def train_phase1(arc_loader) -> generator\n",
    "\n",
    "# Phase 2\n",
    "from src.training.phase2 import train_critic_phase2, build_critic\n",
    "#   def build_critic() -> critic\n",
    "#   def train_critic_phase2(critic, data_loader) -> critic\n",
    "\n",
    "# Phase 3\n",
    "from src.training.phase3 import train_phase3_adversarial, build_generator, build_critic as build_critic_phase3\n",
    "#   def build_generator() -> generator\n",
    "#   def build_critic_phase3() -> critic\n",
    "#   def train_phase3_adversarial(generator, critic, data_loader) -> (generator, critic)\n",
    "\n",
    "# Phase 4\n",
    "from src.training.phase4 import train_phase4_ppo  # def train_phase4_ppo() -> None\n",
    "\n",
    "# Data loader\n",
    "from src.data_pipeline.dataloader import ARCDataModule\n",
    "\n",
    "# Point to your local folder named \"training\"\n",
    "folder_path = Path(\"./src/data_pipeline/ARC_data/data/training\")\n",
    "\n",
    "data_module = ARCDataModule(\n",
    "    dir_path=folder_path,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    pad_value=0,\n",
    ").prepare()\n",
    "\n",
    "# ========================\n",
    "# LIMIT DATASET FOR DEBUG\n",
    "# ========================\n",
    "LIMIT_SAMPLES = 1    # or 2\n",
    "\n",
    "if LIMIT_SAMPLES is not None:\n",
    "    data_module.dataset.data = data_module.dataset.data[:LIMIT_SAMPLES]\n",
    "\n",
    "arc_loader = data_module.get_loader()\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Orchestrator              #\n",
    "###############################\n",
    "\n",
    "def run_phase1():\n",
    "    \"\"\"\n",
    "    Phase 1: supervised pretraining of generator.\n",
    "    \"\"\"\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"   PHASE 1: Supervised Train   \")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    generator = train_phase1(arc_loader)\n",
    "\n",
    "    # Save generator weights\n",
    "    save_checkpoint(generator.state_dict(), \"checkpoints/phase1_generator.pt\")\n",
    "\n",
    "\n",
    "def run_phase2():\n",
    "    \"\"\"\n",
    "    Phase 2: critic warmup with WGAN-GP and random fake outputs.\n",
    "    \"\"\"\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"   PHASE 2: Critic Warmup      \")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    critic = build_critic()\n",
    "\n",
    "    critic = train_critic_phase2(critic, arc_loader)\n",
    "\n",
    "    save_checkpoint(critic.state_dict(), \"checkpoints/critic_phase2_warmup.pt\")\n",
    "\n",
    "\n",
    "def run_phase3():\n",
    "    \"\"\"\n",
    "    Phase 3: joint adversarial training of generator + critic.\n",
    "    Uses phase1 + phase2 checkpoints if available.\n",
    "    \"\"\"\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"   PHASE 3: Adversarial Train  \")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    # Build fresh models\n",
    "    generator = build_generator()\n",
    "    critic = build_critic_phase3()\n",
    "\n",
    "    # Optionally load phase 1 generator\n",
    "    ckpt_gen_p1 = load_checkpoint(\"checkpoints/phase1_generator.pt\", map_location=DEVICE)\n",
    "    if ckpt_gen_p1 is not None:\n",
    "        generator.load_state_dict(ckpt_gen_p1, strict=False)\n",
    "\n",
    "    # Optionally load phase 2 critic\n",
    "    ckpt_crit_p2 = load_checkpoint(\"checkpoints/critic_phase2_warmup.pt\", map_location=DEVICE)\n",
    "    if ckpt_crit_p2 is not None:\n",
    "        critic.load_state_dict(ckpt_crit_p2, strict=False)\n",
    "\n",
    "    generator, critic = train_phase3_adversarial(\n",
    "        generator=generator,\n",
    "        critic=critic,\n",
    "        data_loader=arc_loader\n",
    "    )\n",
    "\n",
    "    save_checkpoint(generator.state_dict(), \"checkpoints/generator_phase3_adv.pt\")\n",
    "    save_checkpoint(critic.state_dict(), \"checkpoints/critic_phase3_adv.pt\")\n",
    "\n",
    "\n",
    "def run_phase4():\n",
    "    \"\"\"\n",
    "    Phase 4: PPO refinement over latent proposals using HybridExecuteController.\n",
    "    Uses phase3 checkpoints if available.\n",
    "    \"\"\"\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"   PHASE 4: PPO Refinement     \")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    # train_phase4_ppo internally loads generator/critic if needed\n",
    "    actor, value_fn = train_phase4_ppo(data_loader=arc_loader)\n",
    "\n",
    "    # Save PPO modules\n",
    "    torch.save(actor.state_dict(), \"checkpoints/ppo_actor_phase4.pt\")\n",
    "    torch.save(value_fn.state_dict(), \"checkpoints/ppo_valuer_phase4.pt\")\n",
    "    print(\"\\nSaved PPO actor and valuer for Phase 4.\")\n",
    "\n",
    "\n",
    "###############################\n",
    "#   Main Entrypoint           #\n",
    "###############################\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"ARC-AGI-2 Training Orchestrator\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--start_phase\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"phase to start from (1-4)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--end_phase\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"phase to end at (1-4)\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    for phase in range(args.start_phase, args.end_phase + 1):\n",
    "        if phase == 1:\n",
    "            run_phase1()\n",
    "        elif phase == 2:\n",
    "            run_phase2()\n",
    "        elif phase == 3:\n",
    "            run_phase3()\n",
    "        elif phase == 4:\n",
    "            run_phase4()\n",
    "        else:\n",
    "            print(f\"[orchestrator] Unknown phase: {phase}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
