{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f156432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.adViT.critic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdversarialVisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vit_encoder: nn.Module,\n",
    "            z_dim: int | None = None,  # proposal dimension\n",
    "            c_dim: int | None = None,  # context dimension\n",
    "            hidden_dim: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vit = vit_encoder\n",
    "        self.z_dim = z_dim\n",
    "        self.c_dim = c_dim\n",
    "\n",
    "        embed_dim = self.vit.c_token.size(-1)\n",
    "\n",
    "        # Total feature dimension (one long vector)\n",
    "        in_dim = embed_dim\n",
    "        if z_dim is not None:\n",
    "            in_dim += z_dim\n",
    "        if c_dim is not None:\n",
    "            in_dim += c_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            I_in: torch.Tensor,        # (B, C_in, H, W)\n",
    "            O_pred: torch.Tensor,      # (B, C_out, H, W) or (B, T, C_out, H, W)\n",
    "            mask_in: torch.Tensor | None = None,\n",
    "            mask_out: torch.Tensor | None = None,\n",
    "            z: torch.Tensor | None = None,\n",
    "            C: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   P = num proposals         #\n",
    "        #   C_in = input channels     #\n",
    "        #   C_out = output channels   #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "\n",
    "        B, C_in, H, W = I_in.shape\n",
    "\n",
    "        #################################\n",
    "        #   MULTI-PROPOSAL BRANCH       #\n",
    "        #################################\n",
    "\n",
    "        if O_pred.dim() == 5:\n",
    "            B, T, C_out, H, W = O_pred.shape\n",
    "\n",
    "            # Expand inputs\n",
    "            I_exp = I_in.unsqueeze(1).expand(B, T, C_in, H, W)\n",
    "            I_flat = I_exp.reshape(B*T, C_in, H, W)\n",
    "            O_flat = O_pred.reshape(B*T, C_out, H, W)\n",
    "\n",
    "            # Convert O_flat to 1 channel if needed\n",
    "            if O_flat.size(1) > 1:\n",
    "                O_flat = torch.argmax(O_flat, dim=1, keepdim=True).float()\n",
    "\n",
    "            # Combine masks\n",
    "            if mask_in is not None or mask_out is not None:\n",
    "                if mask_in is None:\n",
    "                    mask_in = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "                if mask_out is None:\n",
    "                    mask_out = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "                mask = torch.logical_or(mask_in, mask_out)\n",
    "                mask = mask.unsqueeze(1).expand(B, T, H, W).reshape(B*T, H, W)\n",
    "            else:\n",
    "                mask = None\n",
    "\n",
    "            # Concatenate input + output\n",
    "            x = torch.cat([I_flat, O_flat], dim=1)  # (B*T, 2, H, W)\n",
    "\n",
    "            # Encode with ViT\n",
    "            h_flat = self.vit.forward_grid(x, mask=mask)\n",
    "            h = h_flat.view(B, T, -1)\n",
    "\n",
    "            # Collect features\n",
    "            feats = [h]\n",
    "\n",
    "            if z is not None:\n",
    "                if z.dim() == 2:\n",
    "                    z = z.unsqueeze(1).expand(B, T, -1)\n",
    "                feats.append(z)\n",
    "\n",
    "            if C is not None:\n",
    "                feats.append(C.unsqueeze(1).expand(B, T, -1))\n",
    "\n",
    "            feat = torch.cat(feats, dim=-1)\n",
    "            return self.mlp(feat).squeeze(-1)\n",
    "\n",
    "        #################################\n",
    "        #   SINGLE-PROPOSAL BRANCH      #\n",
    "        #################################\n",
    "\n",
    "        B, C_out, H, W = O_pred.shape\n",
    "\n",
    "        # Convert O_pred to 1 channel if needed\n",
    "        if O_pred.size(1) > 1:          # multi-channel class logits\n",
    "            classes = torch.arange(O_pred.size(1), device=O_pred.device).view(1, -1, 1, 1)\n",
    "            probs = O_pred.softmax(dim=1)\n",
    "            O_pred = (probs * classes).sum(dim=1, keepdim=True)   # differentiable\n",
    "\n",
    "\n",
    "        # Combine masks\n",
    "        if mask_in is not None or mask_out is not None:\n",
    "            if mask_in is None:\n",
    "                mask_in = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "            if mask_out is None:\n",
    "                mask_out = torch.zeros(B, H, W, dtype=torch.bool, device=O_pred.device)\n",
    "            mask = torch.logical_or(mask_in, mask_out)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # Concatenate input + output\n",
    "        x = torch.cat([I_in, O_pred], dim=1)  # (B, 2, H, W)\n",
    "\n",
    "        # Encode with ViT\n",
    "        h = self.vit.forward_grid(x, mask=mask)\n",
    "\n",
    "        # Combine features for MLP\n",
    "        feats = [h]\n",
    "        if z is not None:\n",
    "            feats.append(z)\n",
    "        if C is not None:\n",
    "            feats.append(C)\n",
    "        feat = torch.cat(feats, dim=-1)\n",
    "\n",
    "        return self.mlp(feat).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.context_encoding.example_pair_encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ExamplePairEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encods single example pair (I_i, O_i) into vector h_i\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vit_pair: nn.Module\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vit = vit_pair # generic\n",
    "        self.norm = nn.LayerNorm(self.vit.c_token.size(-1)) # normalize h_i\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            I_i: torch.Tensor,\n",
    "            O_i: torch.Tensor,\n",
    "            mask_I: torch.Tensor,\n",
    "            mask_O: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        B, _, H, W = I_i.shape\n",
    "        \n",
    "        # Concatenate input-output as different channels\n",
    "        x = torch.cat([I_i, O_i], dim=1)\n",
    "\n",
    "        # Combine masks\n",
    "        mask = torch.logical_or(mask_I, mask_O)\n",
    "        key_padded_mask = ~mask\n",
    "\n",
    "        # Pass through ViT for context embedding\n",
    "        h_i = self.vit.forward_grid(x, mask=key_padded_mask)  # (B, embed_dim)\n",
    "\n",
    "        # Normalize\n",
    "        h_i = self.norm(h_i)\n",
    "\n",
    "        return h_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714752eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.context_encoding.example_pair_aggregator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ExamplePairAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregates the context vectors from k example pairs (I_i, O_i) into a single embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            embed_dim: int,\n",
    "            hidden_dim: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # MLP to weigh context vectors\n",
    "        self.score_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.Tanh(), # [-1,1]\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Normalize context vector\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            h: torch.Tensor,  # all embeddings \n",
    "            mask: torch.Tensor | None = None  # where the tokens are valid\n",
    "    ) -> torch.Tensor:\n",
    "        ###############################\n",
    "        #   B = batch size            #\n",
    "        #   K = num example pairs     #\n",
    "        #   D = embedding dimension   #\n",
    "        ###############################\n",
    "\n",
    "        B, K, D = h.shape\n",
    "\n",
    "        # Infer scores\n",
    "        scores = self.score_mlp(h)\n",
    "\n",
    "        # Mask = where the tokens are\n",
    "        if mask is not None:\n",
    "\n",
    "            # Ensure boolean\n",
    "            mask = mask.to(dtype=torch.bool)\n",
    "\n",
    "            # Match to score shape\n",
    "            mask_expanded = mask.unsqueeze(-1) # (B, K, 1)\n",
    "\n",
    "            # Set where the mask is not to very negative\n",
    "            scores = scores.masked_fill(~mask_expanded, float(\"-inf\"))\n",
    "\n",
    "        # Attention weights over k example pairs\n",
    "        attn = F.softmax(scores, dim=1)\n",
    "\n",
    "        # Weighted sum\n",
    "        C = torch.sum(attn * h, dim=1)\n",
    "\n",
    "        # Final normalization\n",
    "        C = self.norm(C)\n",
    "\n",
    "        return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab898f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.context_encoding.conditional_encoder.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConditionalTestInputEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vit_test: nn.Module\n",
    "    ):\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   D = token embedding dim   #\n",
    "        #   S = num tokens            #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "\n",
    "        super().__init__()\n",
    "        self.vit = vit_test\n",
    "        self.embed_dim = self.vit.c_token.size(-1)\n",
    "\n",
    "        # Project context vector to embedding dim\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            I_test,  # (B, 1, H, W)\n",
    "            mask_test,  # (B, H, W) or None\n",
    "            C  # (B, D)\n",
    "    ):\n",
    "\n",
    "        B, _, H, W = I_test.shape\n",
    "\n",
    "        ######################\n",
    "        #   Encode with ViT  #\n",
    "        ######################\n",
    "\n",
    "        tokens = self.vit.patch_embedding(I_test)  # (B, S, D)\n",
    "\n",
    "        #######################\n",
    "        #   Build Test Mask   #\n",
    "        #######################\n",
    "\n",
    "        if mask_test is not None:\n",
    "            flat_mask = mask_test.reshape(B, -1)  # (B, S)\n",
    "            key_padding_mask = ~flat_mask         # True = pad\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "        \n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "        ##########################\n",
    "        #   Add Context Vector   #\n",
    "        ##########################\n",
    "\n",
    "        C_token = self.c_proj(C).unsqueeze(1)  # (B,1,D)\n",
    "        tokens = torch.cat([C_token, tokens], dim=1)  # (B,1+S,D)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # Add context to mask\n",
    "            c_pad = torch.zeros(B, 1, dtype=torch.bool, device=key_padding_mask.device)\n",
    "            key_padding_mask = torch.cat([c_pad, key_padding_mask], dim=1)  # (B,1+S)\n",
    "\n",
    "        #####################################\n",
    "        #   Positional Encoding + Dropout   #\n",
    "        #####################################\n",
    "\n",
    "        tokens = self.vit.pos_encoding(tokens)\n",
    "        tokens = self.vit.dropout(tokens)\n",
    "\n",
    "        return tokens, key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.attention.py\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "\n",
    "        out, _ = self.attn(\n",
    "            x, x, x,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.CNNBlock\n",
    "\n",
    "import torch.nn as nn\n",
    "from src.architecture.executor.FiLM import FiLM\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels: int,\n",
    "            z_dim: int | None = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            channels,  # in\n",
    "            channels,  # out\n",
    "            kernel_size=3, \n",
    "            padding=1\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(channels, channels)  # normalize each channel with respect to itself\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self.film = FiLM(channels, z_dim) if z_dim is not None else None\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            z=None\n",
    "    ):\n",
    "        # Feature extraction\n",
    "        x = self.norm(self.conv(x))\n",
    "\n",
    "        # Proposed feature modulation\n",
    "        if self.film is not None:\n",
    "            x = self.film(x, z)\n",
    "\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.executor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.architecture.executor.CNNBlock import CNNBlock\n",
    "from src.architecture.ViT.body import TransformerEncoderBlock\n",
    "\n",
    "\n",
    "# Hybrid ViT and CNN\n",
    "class Executor(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a latent transformation z to an input grid\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            mlp_dim,\n",
    "            depth,\n",
    "            z_dim,\n",
    "            hidden_channels=64,\n",
    "            num_classes=10  # ARC colors\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        ############################\n",
    "        #   CNN Feature Enricher   #\n",
    "        ############################\n",
    "\n",
    "        self.enricher = nn.Sequential(\n",
    "            nn.Conv2d(1, hidden_channels, 3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        ######################################\n",
    "        #   CNN Proposal Feature Detection   #\n",
    "        ######################################\n",
    "\n",
    "        self.cnn_blocks = nn.ModuleList([\n",
    "            CNNBlock(hidden_channels, z_dim=z_dim)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        ##################\n",
    "        #   Tokenizers   #\n",
    "        ##################\n",
    "\n",
    "        self.to_embedding = nn.Linear(hidden_channels, embed_dim)\n",
    "\n",
    "        # Interpret proposal \n",
    "        self.z_token = nn.Linear(z_dim, embed_dim)\n",
    "\n",
    "        ##################\n",
    "        #   ViT Layers   #\n",
    "        ##################\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        #######################\n",
    "        #   CNN Discretizer   #\n",
    "        #######################\n",
    "\n",
    "        self.discretizer = nn.Sequential(\n",
    "            nn.Conv2d(  # detect features in token\n",
    "                embed_dim, \n",
    "                hidden_channels, \n",
    "                kernel_size=3, \n",
    "                padding=1\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(  # convert token features to a classification\n",
    "                hidden_channels, \n",
    "                num_classes, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            grid, \n",
    "            z\n",
    "    ):\n",
    "        ###########################\n",
    "        #   grid = (B, 1, H, W)   #\n",
    "        #   z = (B, z_dim)        #\n",
    "        ###############################\n",
    "        #   B = batch size            #    \n",
    "        #   D = embedding dimension   #\n",
    "        #   H = height                #\n",
    "        #   W = width                 #\n",
    "        ###############################\n",
    "\n",
    "        B, _, H, W = grid.shape\n",
    "\n",
    "        #############################################\n",
    "        #   Enricher + Proposed Feature Modulator   #\n",
    "        #############################################\n",
    "\n",
    "        x = self.enricher(grid)\n",
    "\n",
    "        for block in self.cnn_blocks:\n",
    "            x = block(x, z)\n",
    "\n",
    "        ################\n",
    "        #   Tokenize   #\n",
    "        ################\n",
    "\n",
    "        # (B, C, H, W) -> (B, H*W, C)\n",
    "        x_flat = x.permute(0, 2, 3, 1).reshape(B, H*W, -1)\n",
    "\n",
    "        tokens = self.to_embedding(x_flat)  # (B, S, D)\n",
    "\n",
    "        # Add proposal z token\n",
    "        z_token = self.z_token(z).unsqueeze(1)  # (B, 1, D) one for each batch\n",
    "        tokens = torch.cat([z_token, tokens], dim=1)  # (B, 1+S, D)\n",
    "\n",
    "        ################################\n",
    "        #   ViT for Global Reasoning   #\n",
    "        ################################\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, None)\n",
    "\n",
    "        ###################\n",
    "        #   Un-tokenize   #\n",
    "        ###################\n",
    "\n",
    "        # Remove z token\n",
    "        x_tokens = tokens[:, 1:, :]  # (B, S, D)\n",
    "\n",
    "        # Reshape to (B, D, H, W)\n",
    "        x_feats = x_tokens.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "        ##################\n",
    "        #   Discretize   #\n",
    "        ##################\n",
    "\n",
    "        # Compute on the embedding dimension\n",
    "        logits = self.discretizer(x_feats)  # (B, num_classes, H, W)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.architecture.executor.FiLM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Feature-wise modulation\n",
    "class FiLM(nn.Module): \n",
    "    def __init__(\n",
    "            self,\n",
    "            feature_dim,\n",
    "            z_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.to_gamma = nn.Linear(z_dim, feature_dim)  # scale factor\n",
    "        self.to_beta = nn.Linear(z_dim, feature_dim)  # shift factor\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            z: torch.Tensor\n",
    "    ):\n",
    "        ########################\n",
    "        #   x = (B, C, H, W)   #\n",
    "        #   z = (B, z_dim)     #\n",
    "        ########################\n",
    "        #   B = batch size     #       \n",
    "        #   C = channels       #\n",
    "        #   H = height         #\n",
    "        #   W = width          #\n",
    "        ########################\n",
    "\n",
    "        ############################\n",
    "        #   Compute Coefficients   #\n",
    "        ############################\n",
    "\n",
    "        # Expand across input\n",
    "        gamma = self.to_gamma(z).unsqueeze(-1).unsqueeze(-1)  # (B, C, 1, 1)\n",
    "        beta = self.to_beta(z).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        ###############################\n",
    "        #   Apply Feture Modulation   #\n",
    "        ###############################\n",
    "\n",
    "        return x * (1 + gamma) + beta"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
