{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdabd1-f773-4e93-a53b-3cf36412472b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No .json files found under: C:\\Users\\brand\\ARC-AGI-Model\\src\\data_pipeline\\ARC_data\\data\\training",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 448\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# Point to your local folder named \"training\"\u001b[39;00m\n\u001b[32m    439\u001b[39m     folder_path = Path(\u001b[33m\"\u001b[39m\u001b[33m~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    441\u001b[39m     data_module = \u001b[43mARCDataModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_value\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     arc_loader = data_module.get_loader()\n\u001b[32m    452\u001b[39m     \u001b[38;5;66;03m# Concise, single-sample summary to avoid IOPub overflow\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 402\u001b[39m, in \u001b[36mARCDataModule.prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    401\u001b[39m     \u001b[38;5;66;03m# load + preprocess\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     data = \u001b[43mload_jsons_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m     _add_one_to_all_values_in_place(data)\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# pad each sample independently (metric_dict unused)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mload_jsons_from_folder\u001b[39m\u001b[34m(dir_path)\u001b[39m\n\u001b[32m     22\u001b[39m files = \u001b[38;5;28msorted\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m root.rglob(\u001b[33m\"\u001b[39m\u001b[33m*.json\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m p.is_file())\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo .json files found under: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m data = {}\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No .json files found under: C:\\Users\\brand\\ARC-AGI-Model\\src\\data_pipeline\\ARC_data\\data\\training"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Load all JSONs (recursive)\n",
    "# ----------------------------\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Read every .json file under dir_path (recursively) and return a dict\n",
    "    keyed by the file's relative path (without the .json extension).\n",
    "    \"\"\"\n",
    "    root = Path(dir_path).expanduser().resolve()\n",
    "    files = sorted(p for p in root.rglob(\"*.json\") if p.is_file())\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found under: {root}\")\n",
    "\n",
    "    data = {}\n",
    "    for p in files:\n",
    "        key = str(p.relative_to(root).with_suffix(\"\"))  # e.g. \"subdir/file\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data[key] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"Unable to load any .json files under: {root}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocess helpers\n",
    "# ----------------------------\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Torch dataset\n",
    "# ----------------------------\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # masks\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,\n",
    "            \"train_output_masks\": train_output_masks,\n",
    "            \"test_input_masks\": test_input_masks,\n",
    "            \"test_output_masks\": test_output_masks,\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# NEW: Data module wrapper\n",
    "# ----------------------------\n",
    "class ARCDataModule:\n",
    "    \"\"\"\n",
    "    Simple wrapper to produce a DataLoader from your folder.\n",
    "    Usage:\n",
    "        dm = ARCDataModule(\"~/path/to/training\").prepare()\n",
    "        loader = dm.get_loader()\n",
    "        for batch in loader: ...\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ):\n",
    "        self.dir_path = Path(dir_path).expanduser().resolve()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.dataset = None\n",
    "        self._loader = None\n",
    "\n",
    "    def prepare(self):\n",
    "        # load + preprocess\n",
    "        data = load_jsons_from_folder(self.dir_path)\n",
    "        _add_one_to_all_values_in_place(data)\n",
    "\n",
    "        # pad each sample independently (metric_dict unused)\n",
    "        padded = pad_data(data, metric_dict=None, pad_value=self.pad_value)\n",
    "        sample_list = build_sample_level_dataset(padded, pad_value=self.pad_value)\n",
    "\n",
    "        # build dataset + loader\n",
    "        self.dataset = ARCSampleDataset(sample_list=sample_list)\n",
    "        self._loader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            collate_fn=arc_collate_fn_bs1,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "        return self  # allow chaining\n",
    "\n",
    "    def get_loader(self):\n",
    "        if self._loader is None:\n",
    "            self.prepare()\n",
    "        return self._loader\n",
    "\n",
    "    # convenience so the module itself is iterable\n",
    "    def __iter__(self):\n",
    "        return iter(self.get_loader())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) if self.dataset is not None else 0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Point to your local folder named \"training\"\n",
    "    folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "\n",
    "    data_module = ARCDataModule(\n",
    "        dir_path=folder_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ).prepare()\n",
    "\n",
    "    arc_loader = data_module.get_loader()\n",
    "\n",
    "    # Concise, single-sample summary to avoid IOPub overflow\n",
    "    for batch in arc_loader:\n",
    "        num_train = int(batch[\"train_inputs\"].shape[0])\n",
    "        num_test  = int(batch[\"test_inputs\"].shape[0])\n",
    "\n",
    "        # original (max over pairs before padding)\n",
    "        train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "        test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "        # new padded sizes (actual tensor shapes)\n",
    "        train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "        train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "        test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "        test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "        print(\"=== SUMMARY (single sample) ===\")\n",
    "        print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "        print(f\"Train original size (max): ({train_orig_h}, {train_orig_w})\")\n",
    "        print(f\"Train padded sizes — input: ({train_in_h}, {train_in_w}), output: ({train_out_h}, {train_out_w})\")\n",
    "        print(f\"Test  original size (max): ({test_orig_h}, {test_orig_w})\")\n",
    "        print(f\"Test  padded sizes — input: ({test_in_h}, {test_in_w}), output: ({test_out_h}, {test_out_w})\")\n",
    "\n",
    "        if num_train > 0:\n",
    "            print(\"\\n--- Example TRAIN pair [0] ---\")\n",
    "            print(\"input:\", batch[\"train_inputs\"][0].tolist())\n",
    "            print(\"output:\", batch[\"train_outputs\"][0].tolist())\n",
    "        if num_test > 0:\n",
    "            print(\"\\n--- Example TEST pair [0] ---\")\n",
    "            print(\"input:\", batch[\"test_inputs\"][0].tolist())\n",
    "            print(\"output:\", batch[\"test_outputs\"][0].tolist())\n",
    "        break\n",
    "\n",
    "    print(type(arc_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace43ad-e906-4e64-81d3-b4c3cc53bcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-agi-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
