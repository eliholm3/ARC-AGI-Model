{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d04f648-8e98-4047-b793-2dc1847b3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be4509c9-2944-4eaf-a99f-1b460400b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 7e0986d6\n",
      "train_inputs: torch.Size([2, 30, 30])\n",
      "train_outputs: torch.Size([2, 30, 30])\n",
      "test_inputs: torch.Size([1, 30, 30])\n",
      "test_outputs: torch.Size([1, 30, 30])\n",
      "train_original_size: tensor([13, 16])\n",
      "test_original_size: tensor([12, 17])\n"
     ]
    }
   ],
   "source": [
    "def load_jsons_from_folder(folder_path):\n",
    "    folder_path = os.path.expanduser(folder_path)\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    if (not os.path.isdir(folder_path)):\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "    file_names = os.listdir(folder_path)\n",
    "    file_names.sort()\n",
    "\n",
    "    for name in file_names:\n",
    "        if (not name.endswith(\".json\")):\n",
    "            continue\n",
    "\n",
    "        full_path = os.path.join(folder_path, name)\n",
    "        try:\n",
    "            with open(full_path, \"r\") as f:\n",
    "                obj = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read:\", full_path)\n",
    "            print(\" Error:\", e)\n",
    "            continue\n",
    "\n",
    "        if ('train' not in obj) or ('test' not in obj):\n",
    "            print(\"Skipping (no train/test):\", full_path)\n",
    "            continue\n",
    "\n",
    "        # basic shape check\n",
    "        ok = True\n",
    "        for split in ['train', 'test']:\n",
    "            if (not isinstance(obj[split], list)):\n",
    "                ok = False\n",
    "                break\n",
    "            for pairs in obj[split]:\n",
    "                if ('input' not in pairs) or ('output' not in pairs):\n",
    "                    ok = False\n",
    "                    break\n",
    "        if (not ok):\n",
    "            print(\"Skipping (bad format):\", full_path)\n",
    "            continue\n",
    "\n",
    "        key = os.path.splitext(name)[0]\n",
    "        data[key] = obj\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict, pad_value=0):\n",
    "    for sample in data.values():\n",
    "        # define max square sizes (per split)\n",
    "        max_train_size = max(metric_dict['max_train_input_height'], metric_dict['max_train_input_width'],\n",
    "                             metric_dict['max_train_output_height'], metric_dict['max_train_output_width'])\n",
    "        max_test_size = max(metric_dict['max_test_input_height'], metric_dict['max_test_input_width'],\n",
    "                            metric_dict['max_test_output_height'], metric_dict['max_test_output_width'])\n",
    "\n",
    "        # pad training samples (grids only)\n",
    "        for pairs in sample['train']:\n",
    "            # pad input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # pad output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for output in pairs['output']:\n",
    "                while len(output) < max_train_size:\n",
    "                    output.append(pad_value)\n",
    "\n",
    "        # pad test samples (grids only)\n",
    "        for pairs in sample['test']:\n",
    "            # pad input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # pad output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for output in pairs['output']:\n",
    "                while len(output) < max_test_size:\n",
    "                    output.append(pad_value)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def print_padded_data(data):\n",
    "    for sample_name, sample in data.items():\n",
    "        print(\"==================================================\")\n",
    "        print(\"SAMPLE:\", sample_name)\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "        print(\"TRAINING PAIRS:\")\n",
    "        for idx, pairs in enumerate(sample['train']):\n",
    "            print(f\"  Train Pair {idx + 1}\")\n",
    "            print(\"  INPUT:\")\n",
    "            for row in pairs['input']:\n",
    "                print(\"   \", row)\n",
    "            print(\"  OUTPUT:\")\n",
    "            for row in pairs['output']:\n",
    "                print(\"   \", row)\n",
    "            print()\n",
    "\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"TEST PAIRS:\")\n",
    "        for idx, pairs in enumerate(sample['test']):\n",
    "            print(f\"  Test Pair {idx + 1}\")\n",
    "            print(\"  INPUT:\")\n",
    "            for row in pairs['input']:\n",
    "                print(\"   \", row)\n",
    "            print(\"  OUTPUT:\")\n",
    "            for row in pairs['output']:\n",
    "                print(\"   \", row)\n",
    "            print()\n",
    "\n",
    "        print(\"==================================================\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])   # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]]) # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])     # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])   # [num_test, H, W]\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "\n",
    "folder_path = f\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\"\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "metrics = get_metrics(data)\n",
    "padded_data = pad_data(data, metrics)\n",
    "sample_level = build_sample_level_dataset(padded_data, pad_value=0)\n",
    "arc_dataset = ARCSampleDataset(sample_list=sample_level)\n",
    "\n",
    "arc_loader = DataLoader(\n",
    "    arc_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=arc_collate_fn_bs1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "for batch in arc_loader:\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(\"train_inputs:\", batch[\"train_inputs\"].shape)   # [num_train, H, W]\n",
    "    print(\"train_outputs:\", batch[\"train_outputs\"].shape) # [num_train, H, W]\n",
    "    print(\"test_inputs:\", batch[\"test_inputs\"].shape)     # [num_test, H, W]\n",
    "    print(\"test_outputs:\", batch[\"test_outputs\"].shape)   # [num_test, H, W]\n",
    "    print(\"train_original_size:\", batch[\"train_original_size\"])\n",
    "    print(\"test_original_size:\", batch[\"test_original_size\"])\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdabd1-f773-4e93-a53b-3cf36412472b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
