{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bdabd1-f773-4e93-a53b-3cf36412472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET-WIDE PAD SIZES ===\n",
      "TRAIN -> 30x30\n",
      "TEST  -> 30x30\n",
      "\n",
      "=== SUMMARY (sample 1) ===\n",
      "id: 1acc24af\n",
      "#train: 4 | #test: 1\n",
      "Train original max: (12, 12)\n",
      "Test  original max: (12, 12)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 2, 2, 2, 2, 1, 1, 2, 2] ... \n",
      "[1, 1, 2, 1, 1, 2, 1, 1, 2, 1] ... \n",
      "[2, 2, 2, 1, 1, 2, 2, 2, 2, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 2, 2, 2, 2, 1, 1, 2, 2] ... \n",
      "[1, 1, 2, 1, 1, 2, 1, 1, 2, 1] ... \n",
      "[2, 2, 2, 1, 1, 2, 2, 2, 2, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 2, 2, 2, 1, 1, 1, 2, 2, 2] ... \n",
      "[1, 2, 1, 2, 2, 1, 1, 2, 1, 2] ... \n",
      "[2, 2, 1, 1, 2, 2, 2, 2, 1, 2] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 2, 2, 2, 1, 1, 1, 2, 2, 2] ... \n",
      "[1, 2, 1, 2, 2, 1, 1, 2, 1, 2] ... \n",
      "[2, 2, 1, 1, 2, 2, 2, 2, 1, 2] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 2) ===\n",
      "id: d255d7a7\n",
      "#train: 3 | #test: 1\n",
      "Train original max: (16, 16)\n",
      "Test  original max: (20, 20)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[10, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[1, 1, 1, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[10, 8, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "...\n",
      "output:\n",
      "[10, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[10, 1, 8, 1, 10, 8, 1, 10, 1, 8] ... \n",
      "[8, 1, 8, 1, 8, 8, 1, 8, 1, 8] ... \n",
      "[8, 1, 1, 1, 8, 8, 1, 1, 1, 8] ... \n",
      "[8, 8, 1, 8, 8, 8, 8, 1, 8, 8] ... \n",
      "[8, 8, 1, 8, 8, 8, 8, 1, 8, 8] ... \n",
      "[8, 8, 1, 8, 8, 8, 8, 1, 8, 8] ... \n",
      "...\n",
      "output:\n",
      "[10, 8, 8, 8, 10, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 3) ===\n",
      "id: f8c80d96\n",
      "#train: 3 | #test: 1\n",
      "Train original max: (10, 10)\n",
      "Test  original max: (10, 10)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 2, 1, 1, 2, 1, 1] ... \n",
      "[1, 1, 1, 1, 2, 1, 1, 2, 1, 1] ... \n",
      "[1, 1, 1, 1, 2, 1, 1, 2, 2, 2] ... \n",
      "[1, 1, 1, 1, 2, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 2, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 2, 2, 2, 2, 2, 2] ... \n",
      "...\n",
      "output:\n",
      "[6, 2, 6, 6, 2, 6, 6, 2, 6, 6] ... \n",
      "[6, 2, 6, 6, 2, 6, 6, 2, 6, 6] ... \n",
      "[6, 2, 6, 6, 2, 6, 6, 2, 2, 2] ... \n",
      "[6, 2, 6, 6, 2, 6, 6, 6, 6, 6] ... \n",
      "[6, 2, 6, 6, 2, 6, 6, 6, 6, 6] ... \n",
      "[6, 2, 6, 6, 2, 2, 2, 2, 2, 2] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[5, 5, 5, 5, 5, 5, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 5, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 5, 1, 1, 1, 1] ... \n",
      "[5, 5, 5, 1, 1, 5, 1, 1, 1, 1] ... \n",
      "[1, 1, 5, 1, 1, 5, 1, 1, 1, 1] ... \n",
      "[1, 1, 5, 1, 1, 5, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[5, 5, 5, 5, 5, 5, 6, 6, 5, 6] ... \n",
      "[6, 6, 6, 6, 6, 5, 6, 6, 5, 6] ... \n",
      "[6, 6, 6, 6, 6, 5, 6, 6, 5, 6] ... \n",
      "[5, 5, 5, 6, 6, 5, 6, 6, 5, 6] ... \n",
      "[6, 6, 5, 6, 6, 5, 6, 6, 5, 6] ... \n",
      "[6, 6, 5, 6, 6, 5, 6, 6, 5, 6] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 4) ===\n",
      "id: 42f83767\n",
      "#train: 3 | #test: 2\n",
      "Train original max: (30, 30)\n",
      "Test  original max: (30, 30)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[2, 2, 1, 4, 4, 1, 3, 3, 1, 6] ... \n",
      "[2, 2, 1, 4, 4, 1, 3, 3, 1, 6] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 6] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[2, 2, 2, 3, 3, 3, 4, 4, 4, 2] ... \n",
      "[2, 1, 2, 3, 1, 1, 1, 4, 1, 2] ... \n",
      "[2, 2, 2, 3, 3, 3, 4, 4, 4, 2] ... \n",
      "[3, 3, 3, 2, 2, 2, 4, 4, 4, 3] ... \n",
      "[3, 1, 1, 2, 1, 2, 1, 4, 1, 3] ... \n",
      "[3, 3, 3, 2, 2, 2, 4, 4, 4, 3] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[2, 2, 1, 3, 3, 1, 4, 4, 1, 9] ... \n",
      "[2, 2, 1, 3, 3, 1, 4, 4, 1, 9] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[3, 3, 3, 3, 3, 3, 2, 2, 2, 9] ... \n",
      "[1, 1, 3, 1, 1, 3, 2, 1, 1, 1] ... \n",
      "[3, 3, 3, 3, 3, 3, 2, 2, 2, 9] ... \n",
      "[3, 3, 3, 2, 2, 2, 2, 2, 2, 9] ... \n",
      "[1, 1, 3, 2, 1, 1, 2, 1, 1, 1] ... \n",
      "[3, 3, 3, 2, 2, 2, 2, 2, 2, 9] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 5) ===\n",
      "id: 94be5b80\n",
      "#train: 2 | #test: 1\n",
      "Train original max: (18, 14)\n",
      "Test  original max: (18, 15)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 8, 3, 2] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 8, 3, 2] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 8, 3, 2] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 8, 1, 1, 8, 1, 1, 1] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 3, 3, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 3, 1, 1, 3, 1, 1] ... \n",
      "[1, 1, 1, 3, 3, 3, 3, 3, 3, 1] ... \n",
      "[1, 1, 1, 1, 1, 4, 4, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 4, 1, 1, 4, 1, 1] ... \n",
      "[1, 1, 1, 4, 4, 4, 4, 4, 4, 1] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 6) ===\n",
      "id: df8cc377\n",
      "#train: 3 | #test: 1\n",
      "Train original max: (20, 23)\n",
      "Test  original max: (22, 26)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 3, 3, 3, 3, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 1, 1, 1, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 1, 1, 1, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 1, 1, 1, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 3, 3, 3, 3, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 3, 3, 3, 3, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 9, 1, 9, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 1, 9, 1, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 9, 1, 9, 3, 1, 1, 1] ... \n",
      "[1, 1, 3, 3, 3, 3, 3, 1, 1, 1] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 6, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 2, 2, 2, 2, 2, 2, 2] ... \n",
      "[6, 1, 1, 2, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 2, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 6, 1, 2, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 2, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 2, 2, 2, 2, 2, 2, 2] ... \n",
      "[1, 1, 1, 2, 5, 1, 5, 1, 5, 1] ... \n",
      "[1, 1, 1, 2, 1, 5, 1, 5, 1, 5] ... \n",
      "[1, 1, 1, 2, 5, 1, 5, 1, 5, 1] ... \n",
      "[1, 1, 1, 2, 1, 5, 1, 5, 1, 5] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 7) ===\n",
      "id: 0e671a1a\n",
      "#train: 4 | #test: 1\n",
      "Train original max: (13, 13)\n",
      "Test  original max: (13, 13)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 5, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 5, 6, 6, 6, 6, 6, 6, 6, 6] ... \n",
      "[1, 6, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 6, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 6, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 6, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 3, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 6, 6, 6] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 6, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 6, 1, 1] ... \n",
      "[1, 3, 6, 6, 6, 6, 6, 6, 6, 6] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 8) ===\n",
      "id: 4acc7107\n",
      "#train: 4 | #test: 1\n",
      "Train original max: (10, 10)\n",
      "Test  original max: (10, 10)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[9, 9, 1, 1, 1, 1, 5, 1, 1, 1] ... \n",
      "[9, 1, 1, 1, 5, 5, 5, 5, 1, 1] ... \n",
      "[9, 9, 9, 1, 1, 1, 5, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 5, 5, 5, 5, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 9, 9, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[9, 9, 9, 9, 1, 5, 5, 5, 5, 1] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[3, 3, 3, 3, 1, 1, 4, 4, 4, 1] ... \n",
      "[1, 1, 1, 3, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 3, 3, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 4, 4, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[3, 3, 3, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[3, 3, 3, 1, 1, 4, 4, 4, 1, 1] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 9) ===\n",
      "id: e3497940\n",
      "#train: 3 | #test: 1\n",
      "Train original max: (10, 9)\n",
      "Test  original max: (10, 9)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 6, 1, 1, 1, 1, 0] ... \n",
      "[1, 1, 1, 3, 6, 1, 1, 1, 1, 0] ... \n",
      "[1, 1, 1, 3, 6, 3, 7, 1, 1, 0] ... \n",
      "[1, 1, 1, 3, 6, 1, 1, 1, 1, 0] ... \n",
      "[1, 1, 1, 3, 6, 3, 3, 3, 1, 0] ... \n",
      "[1, 1, 7, 7, 6, 7, 1, 1, 1, 0] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 1, 1, 3, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 1, 7, 3, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 1, 1, 3, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 3, 3, 3, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 1, 7, 7, 0, 0, 0, 0, 0, 0] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 1, 6, 1, 1, 1, 1, 0] ... \n",
      "[1, 1, 1, 2, 6, 1, 1, 1, 1, 0] ... \n",
      "[1, 1, 1, 2, 6, 2, 1, 1, 1, 0] ... \n",
      "[1, 2, 2, 2, 6, 2, 2, 2, 7, 0] ... \n",
      "[1, 1, 1, 7, 6, 7, 7, 1, 1, 0] ... \n",
      "[1, 1, 1, 1, 6, 2, 2, 2, 1, 0] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 1, 1, 2, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 1, 1, 2, 0, 0, 0, 0, 0, 0] ... \n",
      "[7, 2, 2, 2, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 1, 7, 7, 0, 0, 0, 0, 0, 0] ... \n",
      "[1, 2, 2, 2, 0, 0, 0, 0, 0, 0] ... \n",
      "...\n",
      "\n",
      "=== SUMMARY (sample 10) ===\n",
      "id: 55059096\n",
      "#train: 3 | #test: 1\n",
      "Train original max: (14, 16)\n",
      "Test  original max: (17, 13)\n",
      "Padded sizes — train_in: (30, 30), train_out: (30, 30), test_in: (30, 30), test_out: (30, 30)\n",
      "Matches global TRAIN size? True | Matches global TEST size? True\n",
      "\n",
      "--- Example TRAIN pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 1, 4, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 4, 4, 4, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 4, 1, 1, 1, 1, 1, 4] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 4, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 4, 4, 4, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 1, 4, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 4, 4, 4, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 4, 3, 1, 1, 1, 1, 4] ... \n",
      "[1, 1, 1, 1, 1, 3, 1, 1, 1, 3] ... \n",
      "[1, 1, 1, 1, 1, 1, 3, 4, 3, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 4, 4, 4, 1] ... \n",
      "...\n",
      "\n",
      "--- Example TEST pair [0] (cropped) ---\n",
      "input:\n",
      "[1, 1, 4, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 4, 4, 4, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 4, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "...\n",
      "output:\n",
      "[1, 1, 4, 1, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 4, 4, 4, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 4, 3, 1, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 3, 1, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 3, 1, 1, 1, 1] ... \n",
      "[1, 1, 1, 1, 1, 1, 3, 1, 1, 1] ... \n",
      "...\n",
      "\n",
      "DataLoader type: <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Load all JSONs (recursive)\n",
    "# ----------------------------\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Read every .json file under dir_path (recursively) and return a dict\n",
    "    keyed by the file's relative path (without the .json extension).\n",
    "    \"\"\"\n",
    "    root = Path(dir_path).expanduser().resolve()\n",
    "    files = sorted(p for p in root.rglob(\"*.json\") if p.is_file())\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found under: {root}\")\n",
    "\n",
    "    data = {}\n",
    "    for p in files:\n",
    "        key = str(p.relative_to(root).with_suffix(\"\"))  # e.g. \"subdir/file\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data[key] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"Unable to load any .json files under: {root}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocess helpers\n",
    "# ----------------------------\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads the ENTIRE dataset so that:\n",
    "      • all TRAIN pairs are square-padded to the same dataset-wide size, and\n",
    "      • all TEST  pairs are square-padded to the same dataset-wide size.    # CHANGED\n",
    "    If metric_dict is None, it will be computed from the data.               # NEW\n",
    "    \"\"\"\n",
    "    # ----- compute global (dataset-wide) sizes -----                         # NEW\n",
    "    if metric_dict is None:\n",
    "        metric_dict = get_metrics(data)\n",
    "\n",
    "    max_train_size = max(\n",
    "        metric_dict[\"max_train_input_height\"],\n",
    "        metric_dict[\"max_train_input_width\"],\n",
    "        metric_dict[\"max_train_output_height\"],\n",
    "        metric_dict[\"max_train_output_width\"]\n",
    "    )\n",
    "    max_test_size = max(\n",
    "        metric_dict[\"max_test_input_height\"],\n",
    "        metric_dict[\"max_test_input_width\"],\n",
    "        metric_dict[\"max_test_output_height\"],\n",
    "        metric_dict[\"max_test_output_width\"]\n",
    "    )\n",
    "\n",
    "    # ----- pad EVERY sample to the global split sizes -----                  # CHANGED\n",
    "    for sample in data.values():\n",
    "        # TRAIN -> global train size\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # TEST -> global test size\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Torch dataset\n",
    "# ----------------------------\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # masks\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,\n",
    "            \"train_output_masks\": train_output_masks,\n",
    "            \"test_input_masks\": test_input_masks,\n",
    "            \"test_output_masks\": test_output_masks,\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# NEW: Small pretty-printer for grids (cropped)\n",
    "# ----------------------------\n",
    "def _pretty_grid(tensor, max_rows=6, max_cols=10):  # NEW\n",
    "    arr = tensor.tolist()\n",
    "    lines = []\n",
    "    r = 0\n",
    "    while r < min(len(arr), max_rows):\n",
    "        row = arr[r]\n",
    "        row_disp = row[:max_cols]\n",
    "        row_txt = str(row_disp) + (\" ... \" if len(row) > max_cols else \"\")\n",
    "        lines.append(row_txt)\n",
    "        r += 1\n",
    "    if len(arr) > max_rows:\n",
    "        lines.append(\"...\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# NEW: Data module wrapper\n",
    "# ----------------------------\n",
    "class ARCDataModule:\n",
    "    \"\"\"\n",
    "    Simple wrapper to produce a DataLoader from your folder.\n",
    "    Usage:\n",
    "        dm = ARCDataModule(\"~/path/to/training\").prepare()\n",
    "        loader = dm.get_loader()\n",
    "        for batch in loader: ...\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ):\n",
    "        self.dir_path = Path(dir_path).expanduser().resolve()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.dataset = None\n",
    "        self._loader = None\n",
    "        self.metrics = None  # NEW\n",
    "\n",
    "    def prepare(self):\n",
    "        # load + preprocess\n",
    "        data = load_jsons_from_folder(self.dir_path)\n",
    "        _add_one_to_all_values_in_place(data)\n",
    "\n",
    "        # compute dataset-wide metrics + pad globally                        # CHANGED\n",
    "        self.metrics = get_metrics(data)                                     # NEW\n",
    "        padded = pad_data(data, metric_dict=self.metrics, pad_value=self.pad_value)\n",
    "\n",
    "        sample_list = build_sample_level_dataset(padded, pad_value=self.pad_value)\n",
    "\n",
    "        # build dataset + loader\n",
    "        self.dataset = ARCSampleDataset(sample_list=sample_list)\n",
    "        self._loader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            collate_fn=arc_collate_fn_bs1,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "        return self  # allow chaining\n",
    "\n",
    "    def get_loader(self):\n",
    "        if self._loader is None:\n",
    "            self.prepare()\n",
    "        return self._loader\n",
    "\n",
    "    # convenience so the module itself is iterable\n",
    "    def __iter__(self):\n",
    "        return iter(self.get_loader())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) if self.dataset is not None else 0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Point to your local folder named \"training\"\n",
    "    folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "\n",
    "    data_module = ARCDataModule(\n",
    "        dir_path=folder_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ).prepare()\n",
    "\n",
    "    arc_loader = data_module.get_loader()\n",
    "\n",
    "    # Expected global sizes (dataset-wide)                                    # NEW\n",
    "    M = data_module.metrics\n",
    "    GLOBAL_TRAIN_SIZE = max(\n",
    "        M[\"max_train_input_height\"], M[\"max_train_input_width\"],\n",
    "        M[\"max_train_output_height\"], M[\"max_train_output_width\"]\n",
    "    )\n",
    "    GLOBAL_TEST_SIZE = max(\n",
    "        M[\"max_test_input_height\"], M[\"max_test_input_width\"],\n",
    "        M[\"max_test_output_height\"], M[\"max_test_output_width\"]\n",
    "    )\n",
    "    print(\"=== DATASET-WIDE PAD SIZES ===\")\n",
    "    print(f\"TRAIN -> {GLOBAL_TRAIN_SIZE}x{GLOBAL_TRAIN_SIZE}\")\n",
    "    print(f\"TEST  -> {GLOBAL_TEST_SIZE}x{GLOBAL_TEST_SIZE}\")\n",
    "\n",
    "    # Print up to 10 concise, readable examples\n",
    "    printed = 0\n",
    "    for batch in arc_loader:\n",
    "        num_train = int(batch[\"train_inputs\"].shape[0])\n",
    "        num_test  = int(batch[\"test_inputs\"].shape[0])\n",
    "\n",
    "        # original (max over pairs before padding, per sample)\n",
    "        train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "        test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "        # padded sizes (actual tensor shapes)\n",
    "        train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "        train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "        test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "        test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "        # Validate against global expectations\n",
    "        train_ok = (train_in_h == GLOBAL_TRAIN_SIZE == train_out_h) and (train_in_w == GLOBAL_TRAIN_SIZE == train_out_w)\n",
    "        test_ok  = (test_in_h  == GLOBAL_TEST_SIZE  == test_out_h)  and (test_in_w  == GLOBAL_TEST_SIZE  == test_out_w)\n",
    "\n",
    "        print(f\"\\n=== SUMMARY (sample {printed+1}) ===\")\n",
    "        print(f\"id: {batch['id']}\")\n",
    "        print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "        print(f\"Train original max: ({train_orig_h}, {train_orig_w})\")\n",
    "        print(f\"Test  original max: ({test_orig_h}, {test_orig_w})\")\n",
    "        print(f\"Padded sizes — train_in: ({train_in_h}, {train_in_w}), \"\n",
    "              f\"train_out: ({train_out_h}, {train_out_w}), \"\n",
    "              f\"test_in: ({test_in_h}, {test_in_w}), \"\n",
    "              f\"test_out: ({test_out_h}, {test_out_w})\")\n",
    "        print(f\"Matches global TRAIN size? {train_ok} | Matches global TEST size? {test_ok}\")\n",
    "\n",
    "        if num_train > 0:\n",
    "            print(\"\\n--- Example TRAIN pair [0] (cropped) ---\")\n",
    "            print(\"input:\\n\"  + _pretty_grid(batch[\"train_inputs\"][0], 6, 10))\n",
    "            print(\"output:\\n\" + _pretty_grid(batch[\"train_outputs\"][0], 6, 10))\n",
    "        if num_test > 0:\n",
    "            print(\"\\n--- Example TEST pair [0] (cropped) ---\")\n",
    "            print(\"input:\\n\"  + _pretty_grid(batch[\"test_inputs\"][0], 6, 10))\n",
    "            print(\"output:\\n\" + _pretty_grid(batch[\"test_outputs\"][0], 6, 10))\n",
    "\n",
    "        printed += 1\n",
    "        if printed >= 10:\n",
    "            break\n",
    "\n",
    "    print(\"\\nDataLoader type:\", type(arc_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace43ad-e906-4e64-81d3-b4c3cc53bcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
