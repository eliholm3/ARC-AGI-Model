{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4509c9-2944-4eaf-a99f-1b460400b86b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3881276199.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThought for 22s\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "Skip to content\n",
    "Chat history\n",
    "\n",
    "You said:\n",
    "how do I change the permissions for the user when launching the studio\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 22s\n",
    "Gotcha — “Studio” means different things in different platforms, and permissions live in different places. Here are the exact clicks for the most common ones. Pick the section that matches what you’re using and you should be set.\n",
    "\n",
    "Azure Machine Learning Studio\n",
    "Azure Portal → your ML workspace → Access control (IAM) → Add role assignment.\n",
    "\n",
    "Assign a role (common picks):\n",
    "\n",
    "AzureML Data Scientist / Contributor (full workspace use)\n",
    "\n",
    "Reader (view-only)\n",
    "\n",
    "Also grant data/secret access so Studio works end-to-end:\n",
    "\n",
    "On the linked Storage account: Storage Blob Data Reader or Contributor\n",
    "\n",
    "On the linked Key Vault: Key Vault Secrets User (if they use secrets)\n",
    "\n",
    "Have the user sign out/in and re-open Studio from the workspace (Launch studio).\n",
    "\n",
    "AWS SageMaker Studio\n",
    "AWS Console → SageMaker → Domains → your domain → User profiles → select the user → Edit.\n",
    "\n",
    "Set the Execution role (IAM role) that has the needed permissions (S3, ECR, KMS, SageMaker APIs, etc.).\n",
    "\n",
    "If you changed IAM policies, have the user pick them up by restarting apps: Studio Control Panel → user’s Apps (JupyterServer / KernelGateway) → Delete/Stop, then Re-launch. (Their home EFS volume persists.)\n",
    "\n",
    "Google Vertex AI Studio (Generative AI)\n",
    "Google Cloud Console → IAM & Admin → grant roles at the project:\n",
    "\n",
    "Vertex AI User (use resources)\n",
    "\n",
    "Service Account User (if they need to run with a SA)\n",
    "\n",
    "For data, grant Storage Object Viewer/Creator on the relevant GCS buckets.\n",
    "\n",
    "Reopen Vertex AI Studio.\n",
    "\n",
    "OpenAI Studio\n",
    "In your OpenAI workspace/org, go to Members & roles → set Owner / Editor / Viewer.\n",
    "\n",
    "If you’re using Projects, adjust the user’s project role too (project-level sharing is separate).\n",
    "\n",
    "Looker Studio (formerly Data Studio)\n",
    "Open the report → Share → Manage access → add the user as Viewer or Editor.\n",
    "\n",
    "Also share the Data Source (Data Source → Share) if the report doesn’t use owner’s credentials.\n",
    "\n",
    "Quick checks if changes don’t “stick”\n",
    "Close/reopen the Studio (or sign out/in).\n",
    "\n",
    "In SageMaker, restart/delete the user’s Studio Apps.\n",
    "\n",
    "In Azure, confirm the role is on the workspace (and not just the resource group), and that Storage/Key Vault roles are set.\n",
    "\n",
    "Propagation usually requires a fresh session.\n",
    "\n",
    "If you tell me which Studio you’re on (Azure ML, SageMaker, Vertex AI, OpenAI, Looker, something else), I’ll tailor the steps precisely to your setup (including which exact roles your user needs for their tasks).\n",
    "\n",
    "You said:\n",
    "dont I need to use this on the user?\n",
    "\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": \"s3:ListBucket\",\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::arc-data-training\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": [\n",
    "\t\t\t\t\"s3:GetObject\",\n",
    "\t\t\t\t\"s3:GetObjectVersion\"\n",
    "\t\t\t],\n",
    "\t\t\t\"Resource\": \"arn:aws:s3:::arc-data-training/*\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 12s\n",
    "Short answer: attach that policy to the SageMaker Studio execution role, not to the human IAM user. That’s the role your Studio apps (JupyterServer/KernelGateway) assume when they run, so S3 access needs to live there.\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "Find the role Studio uses\n",
    "\n",
    "AWS Console → SageMaker → Domains → your domain → User profiles → select the user.\n",
    "\n",
    "Note the Execution role ARN.\n",
    "\n",
    "Attach the policy to that role\n",
    "\n",
    "AWS Console → IAM → Roles → open that execution role → Add permissions → Create inline policy → JSON → paste your policy → Save.\n",
    "\n",
    "Your read-only policy (as posted) is fine for reading from s3://arc-data-training:\n",
    "\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\"Effect\": \"Allow\",\"Action\": \"s3:ListBucket\",\"Resource\": \"arn:aws:s3:::arc-data-training\"},\n",
    "    {\"Effect\": \"Allow\",\"Action\": [\"s3:GetObject\",\"s3:GetObjectVersion\"],\"Resource\": \"arn:aws:s3:::arc-data-training/*\"}\n",
    "  ]\n",
    "}\n",
    "Common add-ons (use if needed)\n",
    "Write data back to the bucket\n",
    "\n",
    "{\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\",\"s3:AbortMultipartUpload\"],\"Resource\":\"arn:aws:s3:::arc-data-training/*\"}\n",
    "List object versions (if the bucket is versioned and you need to browse versions)\n",
    "\n",
    "{\"Effect\":\"Allow\",\"Action\":\"s3:ListBucketVersions\",\"Resource\":\"arn:aws:s3:::arc-data-training\"}\n",
    "SSE-KMS encryption (if objects use a CMK)\n",
    "\n",
    "Add to the role policy:\n",
    "\n",
    "{\n",
    "  \"Effect\":\"Allow\",\n",
    "  \"Action\":[\"kms:Decrypt\",\"kms:Encrypt\",\"kms:GenerateDataKey*\",\"kms:DescribeKey\"],\n",
    "  \"Resource\":\"arn:aws:kms:<region>:<account>:key/<key-id-or-alias>\"\n",
    "}\n",
    "Ensure the KMS key policy allows this role as a principal.\n",
    "\n",
    "If access still fails\n",
    "Bucket policy denies override role permissions. Check arc-data-training’s Bucket policy for any Deny statements or allow-lists. If it’s locked down, add an allow for the role:\n",
    "\n",
    "{\n",
    "  \"Version\":\"2012-10-17\",\n",
    "  \"Statement\":[\n",
    "    {\n",
    "      \"Sid\":\"AllowStudioRoleRead\",\n",
    "      \"Effect\":\"Allow\",\n",
    "      \"Principal\":{\"AWS\":\"arn:aws:iam::<account-id>:role/<studio-execution-role-name>\"},\n",
    "      \"Action\":[\"s3:ListBucket\"],\n",
    "      \"Resource\":\"arn:aws:s3:::arc-data-training\"\n",
    "    },\n",
    "    {\n",
    "      \"Sid\":\"AllowStudioRoleReadObjects\",\n",
    "      \"Effect\":\"Allow\",\n",
    "      \"Principal\":{\"AWS\":\"arn:aws:iam::<account-id>:role/<studio-execution-role-name>\"},\n",
    "      \"Action\":[\"s3:GetObject\",\"s3:GetObjectVersion\"],\n",
    "      \"Resource\":\"arn:aws:s3:::arc-data-training/*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "Credentials caching: After changing the role, in Studio go to Control Panel → (your user) → Apps, stop/delete the running JupyterServer/KernelGateway apps, then reopen Studio so it picks up the new permissions.\n",
    "\n",
    "If you paste me the execution role ARN (or confirm whether the bucket uses KMS or needs write access), I’ll hand you the exact, ready-to-paste policy.\n",
    "\n",
    "You said:\n",
    "change this code not to access a bucket but a folder in my directory called training\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_jsons_from_folder(s3_uri, profile_name='arcprofile'):\n",
    "    if not str(s3_uri).startswith(\"s3://\"):\n",
    "        raise ValueError(\"Expected an S3 URI like s3://bucket/prefix/\")\n",
    "\n",
    "    o = urlparse(s3_uri)\n",
    "    bucket = o.netloc\n",
    "    prefix = o.path.lstrip(\"/\")\n",
    "    if prefix and not prefix.endswith(\"/\"):\n",
    "        prefix += \"/\"\n",
    "\n",
    "    # use default creds, or pick a profile\n",
    "    session = boto3.Session(profile_name=profile_name) if profile_name else boto3.Session()\n",
    "    s3 = session.client(\"s3\")\n",
    "\n",
    "    # list only the current \"folder\" (non-recursive)\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter=\"/\")\n",
    "\n",
    "    json_keys = []\n",
    "    for page in pages:\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\".json\"):\n",
    "                json_keys.append(key)\n",
    "\n",
    "    if not json_keys:\n",
    "        raise FileNotFoundError(f\"No .json objects found under s3://{bucket}/{prefix}\")\n",
    "\n",
    "    # sort by filename to mirror your local behavior\n",
    "    json_keys.sort(key=lambda k: os.path.basename(k))\n",
    "\n",
    "    data = {}\n",
    "    for key in json_keys:\n",
    "        uri = f\"s3://{bucket}/{key}\"\n",
    "        try:\n",
    "            body = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read().decode(\"utf-8\")\n",
    "            obj = json.loads(body)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read:\", uri)\n",
    "            print(\" Error:\", e)\n",
    "            continue\n",
    "\n",
    "        # same validation as your original\n",
    "        if (\"train\" not in obj) or (\"test\" not in obj):\n",
    "            print(\"Skipping (no train/test):\", uri)\n",
    "            continue\n",
    "\n",
    "        ok = True\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            if not isinstance(obj[split], list):\n",
    "                ok = False\n",
    "                break\n",
    "            for pairs in obj[split]:\n",
    "                if (\"input\" not in pairs) or (\"output\" not in pairs):\n",
    "                    ok = False\n",
    "                    break\n",
    "            if not ok:\n",
    "                break\n",
    "        if not ok:\n",
    "            print(\"Skipping (bad format):\", uri)\n",
    "            continue\n",
    "\n",
    "        name = os.path.splitext(os.path.basename(key))[0]\n",
    "        data[name] = obj\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # NEW: stack masks alongside data\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])   # [num_train, H, W]\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]]) # [num_train, H, W]\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])    # [num_test, H, W]\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])   # [num_test, H, W]\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,   # NEW\n",
    "            \"train_output_masks\": train_output_masks, # NEW\n",
    "            \"test_input_masks\": test_input_masks,     # NEW\n",
    "            \"test_output_masks\": test_output_masks,   # NEW\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------- main -----------\n",
    "folder_path = f\"s3://arc-data-training/training/\"\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "\n",
    "# add +1 to every grid value BEFORE padding\n",
    "_add_one_to_all_values_in_place(data)\n",
    "\n",
    "metrics = get_metrics(data)\n",
    "padded_data = pad_data(data, metrics, pad_value=0)\n",
    "sample_level = build_sample_level_dataset(padded_data, pad_value=0)\n",
    "arc_dataset = ARCSampleDataset(sample_list=sample_level)\n",
    "\n",
    "arc_loader = DataLoader(\n",
    "    arc_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=arc_collate_fn_bs1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# NEW: print *every* array for *every* sample (can be large!)\n",
    "for batch in arc_loader:\n",
    "    print(\"=== SAMPLE (FULL) ===\")\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(\"train_inputs:\", batch[\"train_inputs\"].tolist())\n",
    "    print(\"train_outputs:\", batch[\"train_outputs\"].tolist())\n",
    "    print(\"train_input_masks:\", batch[\"train_input_masks\"].tolist())\n",
    "    print(\"train_output_masks:\", batch[\"train_output_masks\"].tolist())\n",
    "    print(\"test_inputs:\", batch[\"test_inputs\"].tolist())\n",
    "    print(\"test_outputs:\", batch[\"test_outputs\"].tolist())\n",
    "    print(\"test_input_masks:\", batch[\"test_input_masks\"].tolist())\n",
    "    print(\"test_output_masks:\", batch[\"test_output_masks\"].tolist())\n",
    "    print(\"train_original_size:\", batch[\"train_original_size\"].tolist())\n",
    "    print(\"test_original_size:\", batch[\"test_original_size\"].tolist())\n",
    "    print()\n",
    "\n",
    "print(type(arc_loader))\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 12s\n",
    "Here’s a local-filesystem version (no S3/boto3). It reads .json files directly from a folder named training (non-recursive), keeps your validation and downstream logic the same, and fixes a small import issue.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .json files from a local directory (non-recursive).\n",
    "    Mirrors the S3 behavior: list current folder only, sort by filename,\n",
    "    validate schema, and return a dict[name] = parsed_json.\n",
    "    \"\"\"\n",
    "    p = Path(dir_path)\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        raise FileNotFoundError(f\"Directory not found: {p}\")\n",
    "\n",
    "    # list only current folder, non-recursive, .json only\n",
    "    json_files = [f for f in p.iterdir() if f.is_file() and f.suffix.lower() == \".json\"]\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No .json files found in {p}\")\n",
    "\n",
    "    # sort by filename to mirror previous behavior\n",
    "    json_files.sort(key=lambda f: f.name)\n",
    "\n",
    "    data = {}\n",
    "    for f in json_files:\n",
    "        try:\n",
    "            body = f.read_text(encoding=\"utf-8\")\n",
    "            obj = json.loads(body)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read:\", f)\n",
    "            print(\" Error:\", e)\n",
    "            continue\n",
    "\n",
    "        # same validation as your original\n",
    "        if (\"train\" not in obj) or (\"test\" not in obj):\n",
    "            print(\"Skipping (no train/test):\", f)\n",
    "            continue\n",
    "\n",
    "        ok = True\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            if not isinstance(obj[split], list):\n",
    "                ok = False\n",
    "                break\n",
    "            for pairs in obj[split]:\n",
    "                if (\"input\" not in pairs) or (\"output\" not in pairs):\n",
    "                    ok = False\n",
    "                    break\n",
    "            if not ok:\n",
    "                break\n",
    "        if not ok:\n",
    "            print(\"Skipping (bad format):\", f)\n",
    "            continue\n",
    "\n",
    "        name = f.stem  # filename without extension\n",
    "        data[name] = obj\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # NEW: stack masks alongside data\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])   # [num_train, H, W]\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]]) # [num_train, H, W]\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])    # [num_test, H, W]\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])   # [num_test, H, W]\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,   # NEW\n",
    "            \"train_output_masks\": train_output_masks, # NEW\n",
    "            \"test_input_masks\": test_input_masks,     # NEW\n",
    "            \"test_output_masks\": test_output_masks,   # NEW\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------- main -----------\n",
    "# Point to your local folder named \"training\" (sibling/relative to where you run the script)\n",
    "folder_path = Path(\"training\")\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "\n",
    "# add +1 to every grid value BEFORE padding\n",
    "_add_one_to_all_values_in_place(data)\n",
    "\n",
    "metrics = get_metrics(data)\n",
    "padded_data = pad_data(data, metrics, pad_value=0)\n",
    "sample_level = build_sample_level_dataset(padded_data, pad_value=0)\n",
    "arc_dataset = ARCSampleDataset(sample_list=sample_level)\n",
    "\n",
    "arc_loader = DataLoader(\n",
    "    arc_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=arc_collate_fn_bs1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# NEW: print *every* array for *every* sample (can be large!)\n",
    "for batch in arc_loader:\n",
    "    print(\"=== SAMPLE (FULL) ===\")\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(\"train_inputs:\", batch[\"train_inputs\"].tolist())\n",
    "    print(\"train_outputs:\", batch[\"train_outputs\"].tolist())\n",
    "    print(\"train_input_masks:\", batch[\"train_input_masks\"].tolist())\n",
    "    print(\"train_output_masks:\", batch[\"train_output_masks\"].tolist())\n",
    "    print(\"test_inputs:\", batch[\"test_inputs\"].tolist())\n",
    "    print(\"test_outputs:\", batch[\"test_outputs\"].tolist())\n",
    "    print(\"test_input_masks:\", batch[\"test_input_masks\"].tolist())\n",
    "    print(\"test_output_masks:\", batch[\"test_output_masks\"].tolist())\n",
    "    print(\"train_original_size:\", batch[\"train_original_size\"].tolist())\n",
    "    print(\"test_original_size:\", batch[\"test_original_size\"].tolist())\n",
    "    print()\n",
    "\n",
    "print(type(arc_loader))\n",
    "Notes:\n",
    "\n",
    "If your training directory is elsewhere, set folder_path = Path(\"/full/path/to/training\").\n",
    "\n",
    "To search subfolders too, replace the listing with p.glob(\"**/*.json\") and sort those paths.\n",
    "\n",
    "You said:\n",
    "its not a directory its a folder\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 10s\n",
    "Totally fair—UI calls it a “folder.” In code, folder == directory. Your function already expects a folder named training. If the error wording threw you off, here’s a tiny patch that changes the message and makes the path handling a bit friendlier:\n",
    "\n",
    "Patch (minimal change):\n",
    "\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .json files from a local folder (non-recursive).\n",
    "    \"\"\"\n",
    "    p = Path(dir_path).expanduser().resolve()\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        raise FileNotFoundError(f\"Folder not found (expected a folder containing .json files): {p}\")\n",
    "\n",
    "    json_files = [f for f in p.iterdir() if f.is_file() and f.suffix.lower() == \".json\"]\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No .json files found in folder: {p}\")\n",
    "\n",
    "    json_files.sort(key=lambda f: f.name)\n",
    "\n",
    "    data = {}\n",
    "    for f in json_files:\n",
    "        try:\n",
    "            body = f.read_text(encoding=\"utf-8\")\n",
    "            obj = json.loads(body)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read:\", f)\n",
    "            print(\" Error:\", e)\n",
    "            continue\n",
    "        # ... (rest unchanged)\n",
    "Point the code at your folder named training:\n",
    "\n",
    "# If the folder sits next to your script:\n",
    "folder_path = (Path(__file__).parent / \"training\").resolve()\n",
    "\n",
    "# If you're in a notebook/REPL and your CWD already contains the folder:\n",
    "# folder_path = Path(\"training\").resolve()\n",
    "\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "Quick sanity check (optional)\n",
    "\n",
    "print(\"Reading from:\", folder_path)\n",
    "print(\"Found:\", [p.name for p in Path(folder_path).iterdir() if p.suffix.lower()==\".json\"])\n",
    "If your “training” folder is elsewhere, just give the absolute path, e.g.:\n",
    "\n",
    "folder_path = Path(r\"C:\\Users\\you\\projects\\myapp\\training\").resolve()  # Windows\n",
    "# or\n",
    "folder_path = Path(\"/home/you/projects/myapp/training\").resolve()      # macOS/Linux\n",
    "That’s it—no S3, just a local folder.\n",
    "\n",
    "You said:\n",
    "your file read isn't working make a simple load jsons that reads all of the data\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .json files from a local folder (non-recursive).\n",
    "    \"\"\"\n",
    "    p = Path(dir_path).expanduser().resolve()\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        raise FileNotFoundError(f\"Folder not found (expected a folder containing .json files): {p}\")\n",
    "\n",
    "    json_files = [f for f in p.iterdir() if f.is_file() and f.suffix.lower() == \".json\"]\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No .json files found in folder: {p}\")\n",
    "\n",
    "    json_files.sort(key=lambda f: f.name)\n",
    "\n",
    "    data = {}\n",
    "    for f in json_files:\n",
    "        try:\n",
    "            body = f.read_text(encoding=\"utf-8\")\n",
    "            obj = json.loads(body)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read:\", f)\n",
    "            print(\" Error:\", e)\n",
    "            continue\n",
    "        # ... (rest unchanged)\n",
    "\n",
    "\n",
    "\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # NEW: stack masks alongside data\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])   # [num_train, H, W]\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]]) # [num_train, H, W]\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])    # [num_test, H, W]\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])   # [num_test, H, W]\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,   # NEW\n",
    "            \"train_output_masks\": train_output_masks, # NEW\n",
    "            \"test_input_masks\": test_input_masks,     # NEW\n",
    "            \"test_output_masks\": test_output_masks,   # NEW\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------- main -----------\n",
    "# Point to your local folder named \"training\" (sibling/relative to where you run the script)\n",
    "folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "\n",
    "# add +1 to every grid value BEFORE padding\n",
    "_add_one_to_all_values_in_place(data)\n",
    "\n",
    "metrics = get_metrics(data)\n",
    "padded_data = pad_data(data, metrics, pad_value=0)\n",
    "sample_level = build_sample_level_dataset(padded_data, pad_value=0)\n",
    "arc_dataset = ARCSampleDataset(sample_list=sample_level)\n",
    "\n",
    "arc_loader = DataLoader(\n",
    "    arc_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=arc_collate_fn_bs1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# NEW: print *every* array for *every* sample (can be large!)\n",
    "for batch in arc_loader:\n",
    "    print(\"=== SAMPLE (FULL) ===\")\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(\"train_inputs:\", batch[\"train_inputs\"].tolist())\n",
    "    print(\"train_outputs:\", batch[\"train_outputs\"].tolist())\n",
    "    print(\"train_input_masks:\", batch[\"train_input_masks\"].tolist())\n",
    "    print(\"train_output_masks:\", batch[\"train_output_masks\"].tolist())\n",
    "    print(\"test_inputs:\", batch[\"test_inputs\"].tolist())\n",
    "    print(\"test_outputs:\", batch[\"test_outputs\"].tolist())\n",
    "    print(\"test_input_masks:\", batch[\"test_input_masks\"].tolist())\n",
    "    print(\"test_output_masks:\", batch[\"test_output_masks\"].tolist())\n",
    "    print(\"train_original_size:\", batch[\"train_original_size\"].tolist())\n",
    "    print(\"test_original_size:\", batch[\"test_original_size\"].tolist())\n",
    "    print()\n",
    "\n",
    "print(type(arc_loader))\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 13s\n",
    "Got it — here’s a drop-in, super simple loader that just reads all .json files from your training folder (recursively) and returns them in a dict. No schema checks, no skipping.\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Read every .json file under dir_path (recursively) and return a dict\n",
    "    keyed by the file's relative path (without the .json extension).\n",
    "    \"\"\"\n",
    "    root = Path(dir_path).expanduser().resolve()\n",
    "    files = sorted(p for p in root.rglob(\"*.json\") if p.is_file())\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found under: {root}\")\n",
    "\n",
    "    data = {}\n",
    "    for p in files:\n",
    "        key = str(p.relative_to(root).with_suffix(\"\"))  # e.g. \"subdir/file\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data[key] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"Unable to load any .json files under: {root}\")\n",
    "\n",
    "    return data\n",
    "Use it exactly like you are:\n",
    "folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "Note: This reads everything. If any JSONs don’t have the expected train/test shape, your later steps will error. If you want, I can add a one-liner filter to keep only files that contain those keys.\n",
    "\n",
    "You said:\n",
    "add its actual size also at the end\n",
    "\n",
    "\n",
    "# NEW: print *every* array for *every* sample (can be large!)\n",
    "for batch in arc_loader:\n",
    "    print(\"=== SAMPLE (FULL) ===\")\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(\"train_inputs:\", batch[\"train_inputs\"].tolist())\n",
    "    print(\"train_outputs:\", batch[\"train_outputs\"].tolist())\n",
    "    print(\"train_input_masks:\", batch[\"train_input_masks\"].tolist())\n",
    "    print(\"train_output_masks:\", batch[\"train_output_masks\"].tolist())\n",
    "    print(\"test_inputs:\", batch[\"test_inputs\"].tolist())\n",
    "    print(\"test_outputs:\", batch[\"test_outputs\"].tolist())\n",
    "    print(\"test_input_masks:\", batch[\"test_input_masks\"].tolist())\n",
    "    print(\"test_output_masks:\", batch[\"test_output_masks\"].tolist())\n",
    "    print(\"train_original_size:\", batch[\"train_original_size\"].tolist())\n",
    "    print(\"test_original_size:\", batch[\"test_original_size\"].tolist())\n",
    "    print()\n",
    "\n",
    "print(type(arc_loader))\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 13s\n",
    "You got it — here’s a minimal patch that prints the actual tensor shapes for each sample and a summary at the very end with total samples/batches.\n",
    "\n",
    "Replace your print section with this:\n",
    "\n",
    "# NEW: print *every* array for *every* sample (can be large!)\n",
    "for batch in arc_loader:\n",
    "    print(\"=== SAMPLE (FULL) ===\")\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(\"train_inputs:\", batch[\"train_inputs\"].tolist())\n",
    "    print(\"train_outputs:\", batch[\"train_outputs\"].tolist())\n",
    "    print(\"train_input_masks:\", batch[\"train_input_masks\"].tolist())\n",
    "    print(\"train_output_masks:\", batch[\"train_output_masks\"].tolist())\n",
    "    print(\"test_inputs:\", batch[\"test_inputs\"].tolist())\n",
    "    print(\"test_outputs:\", batch[\"test_outputs\"].tolist())\n",
    "    print(\"test_input_masks:\", batch[\"test_input_masks\"].tolist())\n",
    "    print(\"test_output_masks:\", batch[\"test_output_masks\"].tolist())\n",
    "    print(\"train_original_size:\", batch[\"train_original_size\"].tolist())\n",
    "    print(\"test_original_size:\", batch[\"test_original_size\"].tolist())\n",
    "\n",
    "    # >>> Added: actual tensor sizes (after padding)\n",
    "    print(\"train_inputs.shape:\", tuple(batch[\"train_inputs\"].shape))      # (num_train, H, W)\n",
    "    print(\"train_outputs.shape:\", tuple(batch[\"train_outputs\"].shape))    # (num_train, H, W)\n",
    "    print(\"test_inputs.shape:\", tuple(batch[\"test_inputs\"].shape))        # (num_test, H, W)\n",
    "    print(\"test_outputs.shape:\", tuple(batch[\"test_outputs\"].shape))      # (num_test, H, W)\n",
    "    print()\n",
    "\n",
    "# >>> Added: end-of-run summary\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(\"Total samples:\", len(arc_dataset))\n",
    "print(\"Total batches per epoch:\", len(arc_loader))  # batch_size=1 so this equals samples\n",
    "print(\"Batch size:\", 1)\n",
    "This keeps your detailed lists but also shows the actual shapes of the tensors per sample (which reflect the padded square sizes), and prints a concise dataset summary at the end.\n",
    "\n",
    "You said:\n",
    "okay reset the prints I just want number of test, number of train, original and new size of the inputs and outputs of them, and one example of them printed once\n",
    "\n",
    "for batch in arc_loader:\n",
    "    print(\"=== SAMPLE (FULL) ===\")\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(\"train_inputs:\", batch[\"train_inputs\"].tolist())\n",
    "    print(\"train_outputs:\", batch[\"train_outputs\"].tolist())\n",
    "    print(\"train_input_masks:\", batch[\"train_input_masks\"].tolist())\n",
    "    print(\"train_output_masks:\", batch[\"train_output_masks\"].tolist())\n",
    "    print(\"test_inputs:\", batch[\"test_inputs\"].tolist())\n",
    "    print(\"test_outputs:\", batch[\"test_outputs\"].tolist())\n",
    "    print(\"test_input_masks:\", batch[\"test_input_masks\"].tolist())\n",
    "    print(\"test_output_masks:\", batch[\"test_output_masks\"].tolist())\n",
    "    print(\"train_original_size:\", batch[\"train_original_size\"].tolist())\n",
    "    print(\"test_original_size:\", batch[\"test_original_size\"].tolist())\n",
    "    print(\"train_inputs.shape:\", tuple(batch[\"train_inputs\"].shape))      # (num_train, H, W)\n",
    "    print(\"train_outputs.shape:\", tuple(batch[\"train_outputs\"].shape))    # (num_train, H, W)\n",
    "    print(\"test_inputs.shape:\", tuple(batch[\"test_inputs\"].shape))        # (num_test, H, W)\n",
    "    print(\"test_outputs.shape:\", tuple(batch[\"test_outputs\"].shape))      # (num_test, H, W)\n",
    "    print()\n",
    "\n",
    "print(type(arc_loader))\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 15s\n",
    "Here’s a concise print block that shows exactly what you asked: #train, #test, the original (max) sizes and new padded sizes of inputs/outputs for each split, plus one example pair from each split.\n",
    "\n",
    "# -------- concise reporting --------\n",
    "for batch in arc_loader:\n",
    "    num_train = batch[\"train_inputs\"].shape[0]\n",
    "    num_test  = batch[\"test_inputs\"].shape[0]\n",
    "\n",
    "    # original (max over pairs) sizes recorded earlier\n",
    "    train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "    test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "    # new padded sizes (actual tensor shapes)\n",
    "    train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "    train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "    test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "    test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "    print(\"=== SAMPLE ===\")\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "\n",
    "    print(f\"Train original size (max over pairs): ({train_orig_h}, {train_orig_w})\")\n",
    "    print(f\"Train padded sizes — input: ({train_in_h}, {train_in_w}), output: ({train_out_h}, {train_out_w})\")\n",
    "\n",
    "    print(f\"Test  original size (max over pairs): ({test_orig_h}, {test_orig_w})\")\n",
    "    print(f\"Test  padded sizes — input: ({test_in_h}, {test_in_w}), output: ({test_out_h}, {test_out_w})\")\n",
    "\n",
    "    # One example pair from each split (if present)\n",
    "    if num_train > 0:\n",
    "        print(\"--- Example TRAIN pair [0] ---\")\n",
    "        print(\"input:\", batch[\"train_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"train_outputs\"][0].tolist())\n",
    "\n",
    "    if num_test > 0:\n",
    "        print(\"--- Example TEST pair [0] ---\")\n",
    "        print(\"input:\", batch[\"test_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"test_outputs\"][0].tolist())\n",
    "\n",
    "    print()\n",
    "You said:\n",
    "how to change this in my env\n",
    "\n",
    "--NotebookApp.iopub_data_rate_limit=1.0e10 --NotebookApp.rate_limit_window=10\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 17s\n",
    "Here are the easiest ways to set those Jupyter limits in your environment—pick the one that matches how you launch notebooks.\n",
    "\n",
    "Quick (one-time) for the current run\n",
    "Classic Notebook\n",
    "\n",
    "jupyter notebook \\\n",
    "  --NotebookApp.iopub_data_rate_limit=1.0e10 \\\n",
    "  --NotebookApp.rate_limit_window=10\n",
    "JupyterLab / Jupyter Server\n",
    "\n",
    "jupyter lab \\\n",
    "  --ServerApp.iopub_msg_rate_limit=1.0e10 \\\n",
    "  --ServerApp.rate_limit_window=10\n",
    "If you see a warning like “iopub_data_rate_limit not recognized by ServerApp”, use ServerApp.iopub_msg_rate_limit (Jupyter Server uses msg rate, not data).\n",
    "\n",
    "Persistent (all future runs)\n",
    "Create or edit your Jupyter config files in ~/.jupyter/:\n",
    "\n",
    "Classic Notebook (~/.jupyter/jupyter_notebook_config.py)\n",
    "\n",
    "c = get_config()\n",
    "c.NotebookApp.iopub_data_rate_limit = 1.0e10\n",
    "c.NotebookApp.rate_limit_window = 10.0\n",
    "JupyterLab / Jupyter Server (~/.jupyter/jupyter_server_config.py)\n",
    "\n",
    "c = get_config()\n",
    "c.ServerApp.iopub_msg_rate_limit = 1.0e10\n",
    "c.ServerApp.rate_limit_window = 10.0\n",
    "If those files don’t exist:\n",
    "\n",
    "jupyter notebook --generate-config\n",
    "jupyter server --generate-config   # for Jupyter Server / JupyterLab\n",
    "Per–Conda/venv (keep it scoped to your env)\n",
    "Put the config files inside the env and point Jupyter to them:\n",
    "\n",
    "# Activate your env first...\n",
    "export JUPYTER_CONFIG_DIR=\"$CONDA_PREFIX/etc/jupyter\"   # or path to your venv\n",
    "mkdir -p \"$JUPYTER_CONFIG_DIR\"\n",
    "jupyter server --generate-config\n",
    "# then edit $JUPYTER_CONFIG_DIR/jupyter_server_config.py as above\n",
    "SageMaker Studio (if that’s your setup)\n",
    "Open a terminal in Studio.\n",
    "\n",
    "Edit ~/.jupyter/jupyter_server_config.py:\n",
    "\n",
    "c = get_config()\n",
    "c.ServerApp.iopub_msg_rate_limit = 1.0e10\n",
    "c.ServerApp.rate_limit_window = 10.0\n",
    "Control Panel → Apps → Stop the JupyterServer, then start it again.\n",
    "\n",
    "Tip: You trimmed your prints (nice!)—that’s the best long-term fix. The huge limits above will suppress the “IOPub data rate exceeded” errors, but it’s still smart to avoid dumping giant tensors in loops.\n",
    "\n",
    "You said:\n",
    "IOPub data rate exceeded.\n",
    "The Jupyter server will temporarily stop sending output\n",
    "to the client in order to avoid crashing it.\n",
    "To change this limit, set the config variable\n",
    "--ServerApp.iopub_data_rate_limit.\n",
    "\n",
    "Current values:\n",
    "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
    "ServerApp.rate_limit_window=3.0 (secs)\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 11s\n",
    "You’re hitting the Jupyter Server limits (not the classic Notebook app). Use the ServerApp settings.\n",
    "\n",
    "Quick (for this run only)\n",
    "From a terminal in your env, start Jupyter with higher limits:\n",
    "\n",
    "jupyter lab \\\n",
    "  --ServerApp.iopub_data_rate_limit=1.0e9 \\\n",
    "  --ServerApp.rate_limit_window=10\n",
    "# (or if you run classic notebook)\n",
    "jupyter notebook \\\n",
    "  --NotebookApp.iopub_data_rate_limit=1.0e9 \\\n",
    "  --NotebookApp.rate_limit_window=10\n",
    "Tip: You can also raise the message-count limiter if needed:\n",
    "--ServerApp.iopub_msg_rate_limit=1.0e9\n",
    "\n",
    "Persistent (scoped to your env)\n",
    "Point Jupyter’s config to live inside your env and generate it:\n",
    "\n",
    "# conda\n",
    "export JUPYTER_CONFIG_DIR=\"$CONDA_PREFIX/etc/jupyter\"\n",
    "mkdir -p \"$JUPYTER_CONFIG_DIR\"\n",
    "jupyter server --generate-config\n",
    "Edit $JUPYTER_CONFIG_DIR/jupyter_server_config.py and add:\n",
    "\n",
    "c = get_config()\n",
    "c.ServerApp.iopub_data_rate_limit = 1.0e9   # bytes/sec\n",
    "c.ServerApp.rate_limit_window = 10.0        # seconds\n",
    "c.ServerApp.iopub_msg_rate_limit = 1.0e9    # optional, msgs/sec\n",
    "Restart Jupyter Lab/Server.\n",
    "\n",
    "SageMaker Studio (if that’s what you’re on)\n",
    "Open a terminal and edit ~/.jupyter/jupyter_server_config.py with the same lines above.\n",
    "\n",
    "Control Panel → Apps → Stop the JupyterServer (and KernelGateway if needed), then start again.\n",
    "\n",
    "Even with higher limits, it’s best to avoid huge prints. Your new concise logging should help a lot.\n",
    "\n",
    "You said:\n",
    "can I set it in my notebook?\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 16s\n",
    "Short answer: not for the current session.\n",
    "ServerApp.iopub_data_rate_limit is a server startup setting. You can write the config from a notebook, but you must restart the Jupyter server for it to take effect.\n",
    "\n",
    "Do it from a notebook (persists for next restart)\n",
    "Run this in a cell to create/update your config inside the active env:\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from jupyter_core.paths import jupyter_config_dir\n",
    "\n",
    "DATA_RATE = 1.0e9   # bytes/sec\n",
    "WINDOW    = 10.0    # seconds\n",
    "\n",
    "cfg_dir = Path(os.environ.get(\"JUPYTER_CONFIG_DIR\") or jupyter_config_dir())\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg_path = cfg_dir / \"jupyter_server_config.py\"  # JupyterLab / Jupyter Server\n",
    "text = (\n",
    "    \"c = get_config()\\n\"\n",
    "    f\"c.ServerApp.iopub_data_rate_limit = {DATA_RATE}\\n\"\n",
    "    f\"c.ServerApp.rate_limit_window = {WINDOW}\\n\"\n",
    "    # optional, sometimes useful:\n",
    "    f\"# c.ServerApp.iopub_msg_rate_limit = {DATA_RATE}\\n\"\n",
    ")\n",
    "\n",
    "cfg_path.write_text(text, encoding=\"utf-8\")\n",
    "print(f\"Wrote config to: {cfg_path}\\nRestart Jupyter Server to apply.\")\n",
    "If you’re on classic Notebook instead of Jupyter Server, write jupyter_notebook_config.py with:\n",
    "\n",
    "c = get_config()\n",
    "c.NotebookApp.iopub_data_rate_limit = 1.0e9\n",
    "c.NotebookApp.rate_limit_window = 10.0\n",
    "After running the cell, restart Jupyter Lab/Server (or in SageMaker Studio: Control Panel → Apps → Stop/Start JupyterServer).\n",
    "\n",
    "If you can’t restart right now\n",
    "Use one of these in-notebook workarounds to avoid hitting the limit:\n",
    "\n",
    "Print less: keep your trimmed logger; avoid large .tolist() dumps.\n",
    "\n",
    "Write to a file instead of the browser:\n",
    "\n",
    "with open(\"debug_dump.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    import json\n",
    "    json.dump(batch[\"train_inputs\"][0].tolist(), f)\n",
    "Silence output for a cell:\n",
    "\n",
    "%%capture\n",
    "# code that would spam output\n",
    "Limit tensor/array printing:\n",
    "\n",
    "import torch, numpy as np\n",
    "torch.set_printoptions(edgeitems=2, threshold=1000, linewidth=120)\n",
    "np.set_printoptions(edgeitems=2, threshold=1000, linewidth=120)\n",
    "If you tell me whether you’re on JupyterLab locally, VS Code, or SageMaker Studio, I can tailor the exact restart steps.\n",
    "\n",
    "You said:\n",
    "is there some workaround way I can do?\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 24s\n",
    "Yep—there are a few workarounds you can do right now in-notebook that avoid touching the server config:\n",
    "\n",
    "1) Send the heavy output to a file (best fix)\n",
    "No output goes over IOPub, so you won’t hit the limit.\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(\"logs/batch_dump.txt\", \"w\", encoding=\"utf-8\") as f, redirect_stdout(f):\n",
    "    # >>> put the noisy prints here <<<\n",
    "    for batch in arc_loader:\n",
    "        print(\"ID:\", batch[\"id\"])\n",
    "        # ... any other huge prints ...\n",
    "\n",
    "print(\"Wrote logs/batch_dump.txt\")\n",
    "Then preview a tiny tail in the notebook:\n",
    "\n",
    "print(\"\\n\".join(Path(\"logs/batch_dump.txt\").read_text(encoding=\"utf-8\").splitlines()[-50:]))\n",
    "2) Capture cell output, then show just the tail\n",
    "This keeps the flood off the browser during the run.\n",
    "\n",
    "%%capture cap --no-stderr\n",
    "# >>> put noisy prints here <<<\n",
    "for batch in arc_loader:\n",
    "    print(\"ID:\", batch[\"id\"])\n",
    "    # ... huge prints ...\n",
    "\n",
    "# New cell:\n",
    "from pathlib import Path\n",
    "Path(\"logs\").mkdir(exist_ok=True)\n",
    "Path(\"logs/run.log\").write_text(cap.stdout, encoding=\"utf-8\")\n",
    "print(\"\\n\".join(cap.stdout.splitlines()[-50:]))\n",
    "3) Throttle printing to stay under the limit\n",
    "If you must print a big blob, print in chunks with a small sleep so bytes/sec < limit.\n",
    "\n",
    "import time, json\n",
    "\n",
    "def rate_limited_print(obj, limit_bps=1_000_000, chunk=50_000):\n",
    "    \"\"\"Print JSON/text in chunks so we stay under ~limit_bps.\"\"\"\n",
    "    s = obj if isinstance(obj, str) else json.dumps(obj)\n",
    "    pause = chunk / float(limit_bps)  # seconds between chunks\n",
    "    for i in range(0, len(s), chunk):\n",
    "        print(s[i:i+chunk], end=\"\", flush=True)\n",
    "        time.sleep(pause)\n",
    "    print()\n",
    "\n",
    "# example\n",
    "# rate_limited_print(batch[\"train_inputs\"][0].tolist())\n",
    "4) Overwrite instead of appending (much less output)\n",
    "Show only a small summary and update it each loop:\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for i, batch in enumerate(arc_loader, 1):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Sample {i}/{len(arc_dataset)}\")\n",
    "    print(f\"#train={batch['train_inputs'].shape[0]}  #test={batch['test_inputs'].shape[0]}\")\n",
    "5) Make big objects smaller before printing\n",
    "import numpy as np, torch\n",
    "np.set_printoptions(edgeitems=2, threshold=200, linewidth=120)\n",
    "torch.set_printoptions(edgeitems=2, threshold=200, linewidth=120)\n",
    "Pick (1) or (2) if you just need the data saved; they’re the most reliable. Use (3) only if you truly have to stream a large blob to the notebook UI.\n",
    "\n",
    "You said:\n",
    "IOPub data rate exceeded.\n",
    "The Jupyter server will temporarily stop sending output\n",
    "to the client in order to avoid crashing it.\n",
    "To change this limit, set the config variable\n",
    "--ServerApp.iopub_data_rate_limit.\n",
    "\n",
    "Current values:\n",
    "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
    "ServerApp.rate_limit_window=3.0 (secs)\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 18s\n",
    "Yep — print it once and keep it tiny. Replace your loop with this single-sample summary:\n",
    "\n",
    "# Print ONCE: counts, original sizes, padded sizes, and one example pair\n",
    "for batch in arc_loader:\n",
    "    num_train = int(batch[\"train_inputs\"].shape[0])\n",
    "    num_test  = int(batch[\"test_inputs\"].shape[0])\n",
    "\n",
    "    # original (max over pairs before padding)\n",
    "    train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "    test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "    # new padded sizes (actual tensor shapes)\n",
    "    train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "    train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "    test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "    test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "    print(\"=== SUMMARY (single sample) ===\")\n",
    "    print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "    print(f\"Train original size (max): ({train_orig_h}, {train_orig_w})\")\n",
    "    print(f\"Train padded sizes — input: ({train_in_h}, {train_in_w}), output: ({train_out_h}, {train_out_w})\")\n",
    "    print(f\"Test  original size (max): ({test_orig_h}, {test_orig_w})\")\n",
    "    print(f\"Test  padded sizes — input: ({test_in_h}, {test_in_w}), output: ({test_out_h}, {test_out_w})\")\n",
    "\n",
    "    # one example from each split (if present)\n",
    "    if num_train > 0:\n",
    "        print(\"\\n--- Example TRAIN pair [0] ---\")\n",
    "        print(\"input:\", batch[\"train_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"train_outputs\"][0].tolist())\n",
    "    if num_test > 0:\n",
    "        print(\"\\n--- Example TEST pair [0] ---\")\n",
    "        print(\"input:\", batch[\"test_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"test_outputs\"][0].tolist())\n",
    "\n",
    "    break  # <<< ensure we only print once\n",
    "If you still hit the IOPub limit, dump to a file and print a tiny tail:\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"logs\").mkdir(exist_ok=True)\n",
    "with open(\"logs/summary.txt\", \"w\", encoding=\"utf-8\") as f, redirect_stdout(f):\n",
    "    # same loop as above (keep the break)\n",
    "\n",
    "print(\"\\n\".join(Path(\"logs/summary.txt\").read_text(encoding=\"utf-8\").splitlines()[-60:]))\n",
    "You said:\n",
    "make the dataloader come from a class\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Read every .json file under dir_path (recursively) and return a dict\n",
    "    keyed by the file's relative path (without the .json extension).\n",
    "    \"\"\"\n",
    "    root = Path(dir_path).expanduser().resolve()\n",
    "    files = sorted(p for p in root.rglob(\"*.json\") if p.is_file())\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found under: {root}\")\n",
    "\n",
    "    data = {}\n",
    "    for p in files:\n",
    "        key = str(p.relative_to(root).with_suffix(\"\"))  # e.g. \"subdir/file\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data[key] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"Unable to load any .json files under: {root}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # NEW: stack masks alongside data\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])   # [num_train, H, W]\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]]) # [num_train, H, W]\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])    # [num_test, H, W]\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])   # [num_test, H, W]\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,   # NEW\n",
    "            \"train_output_masks\": train_output_masks, # NEW\n",
    "            \"test_input_masks\": test_input_masks,     # NEW\n",
    "            \"test_output_masks\": test_output_masks,   # NEW\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------- main -----------\n",
    "# Point to your local folder named \"training\" (sibling/relative to where you run the script)\n",
    "folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "\n",
    "# add +1 to every grid value BEFORE padding\n",
    "_add_one_to_all_values_in_place(data)\n",
    "\n",
    "metrics = get_metrics(data)\n",
    "padded_data = pad_data(data, metrics, pad_value=0)\n",
    "sample_level = build_sample_level_dataset(padded_data, pad_value=0)\n",
    "arc_dataset = ARCSampleDataset(sample_list=sample_level)\n",
    "\n",
    "arc_loader = DataLoader(\n",
    "    arc_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=arc_collate_fn_bs1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# NEW: print *every* array for *every* sample (can be large!)\n",
    "for batch in arc_loader:\n",
    "    num_train = int(batch[\"train_inputs\"].shape[0])\n",
    "    num_test  = int(batch[\"test_inputs\"].shape[0])\n",
    "\n",
    "    # original (max over pairs before padding)\n",
    "    train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "    test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "    # new padded sizes (actual tensor shapes)\n",
    "    train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "    train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "    test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "    test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "    print(\"=== SUMMARY (single sample) ===\")\n",
    "    print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "    print(f\"Train original size (max): ({train_orig_h}, {train_orig_w})\")\n",
    "    print(f\"Train padded sizes — input: ({train_in_h}, {train_in_w}), output: ({train_out_h}, {train_out_w})\")\n",
    "    print(f\"Test  original size (max): ({test_orig_h}, {test_orig_w})\")\n",
    "    print(f\"Test  padded sizes — input: ({test_in_h}, {test_in_w}), output: ({test_out_h}, {test_out_w})\")\n",
    "\n",
    "    # one example from each split (if present)\n",
    "    if num_train > 0:\n",
    "        print(\"\\n--- Example TRAIN pair [0] ---\")\n",
    "        print(\"input:\", batch[\"train_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"train_outputs\"][0].tolist())\n",
    "    if num_test > 0:\n",
    "        print(\"\\n--- Example TEST pair [0] ---\")\n",
    "        print(\"input:\", batch[\"test_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"test_outputs\"][0].tolist())\n",
    "\n",
    "    break  # <<< ensure we only print once\n",
    "\n",
    "print(type(arc_loader))\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 7s\n",
    "Here’s a tiny wrapper class that builds the DataLoader for you. Drop it in, then use ARCDataModule(...).prepare().get_loader() (or iterate over the module directly).\n",
    "\n",
    "# --- NEW: simple data module that returns a DataLoader ---\n",
    "class ARCDataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ):\n",
    "        self.dir_path = Path(dir_path).expanduser().resolve()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.dataset = None\n",
    "        self._loader = None\n",
    "\n",
    "    def prepare(self):\n",
    "        # load + preprocess\n",
    "        data = load_jsons_from_folder(self.dir_path)\n",
    "        _add_one_to_all_values_in_place(data)\n",
    "        # pad_data ignores metric_dict, so we can skip get_metrics()\n",
    "        padded = pad_data(data, metric_dict=None, pad_value=self.pad_value)\n",
    "        sample_list = build_sample_level_dataset(padded, pad_value=self.pad_value)\n",
    "\n",
    "        # build dataset + loader\n",
    "        self.dataset = ARCSampleDataset(sample_list=sample_list)\n",
    "        self._loader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            collate_fn=arc_collate_fn_bs1,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "        return self  # allow chaining\n",
    "\n",
    "    def get_loader(self):\n",
    "        if self._loader is None:\n",
    "            self.prepare()\n",
    "        return self._loader\n",
    "\n",
    "    # optional niceties: use the module like a loader directly\n",
    "    def __iter__(self):\n",
    "        return iter(self.get_loader())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) if self.dataset is not None else 0\n",
    "Use it in your main section\n",
    "# ----------- main -----------\n",
    "folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "\n",
    "data_module = ARCDataModule(\n",
    "    dir_path=folder_path,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    pad_value=0,\n",
    ").prepare()\n",
    "\n",
    "arc_loader = data_module.get_loader()   # or: for batch in data_module: ...\n",
    "\n",
    "# concise summary print (once)\n",
    "for batch in arc_loader:\n",
    "    num_train = int(batch[\"train_inputs\"].shape[0])\n",
    "    num_test  = int(batch[\"test_inputs\"].shape[0])\n",
    "\n",
    "    train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "    test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "    train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "    train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "    test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "    test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "    print(\"=== SUMMARY (single sample) ===\")\n",
    "    print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "    print(f\"Train original size (max): ({train_orig_h}, {train_orig_w})\")\n",
    "    print(f\"Train padded sizes — input: ({train_in_h}, {train_in_w}), output: ({train_out_h}, {train_out_w})\")\n",
    "    print(f\"Test  original size (max): ({test_orig_h}, {test_orig_w})\")\n",
    "    print(f\"Test  padded sizes — input: ({test_in_h}, {test_in_w}), output: ({test_out_h}, {test_out_w})\")\n",
    "\n",
    "    if num_train > 0:\n",
    "        print(\"\\n--- Example TRAIN pair [0] ---\")\n",
    "        print(\"input:\", batch[\"train_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"train_outputs\"][0].tolist())\n",
    "    if num_test > 0:\n",
    "        print(\"\\n--- Example TEST pair [0] ---\")\n",
    "        print(\"input:\", batch[\"test_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"test_outputs\"][0].tolist())\n",
    "    break\n",
    "This keeps your existing functions intact, but now the DataLoader “comes from a class.” If you want a train/val/test split at the sample level later, we can extend the module with separate loaders.\n",
    "\n",
    "You said:\n",
    "recreate my entire file to include this\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Read every .json file under dir_path (recursively) and return a dict\n",
    "    keyed by the file's relative path (without the .json extension).\n",
    "    \"\"\"\n",
    "    root = Path(dir_path).expanduser().resolve()\n",
    "    files = sorted(p for p in root.rglob(\"*.json\") if p.is_file())\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found under: {root}\")\n",
    "\n",
    "    data = {}\n",
    "    for p in files:\n",
    "        key = str(p.relative_to(root).with_suffix(\"\"))  # e.g. \"subdir/file\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data[key] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"Unable to load any .json files under: {root}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,    # NEW\n",
    "                \"output_mask\": out_mask    # NEW\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # NEW: stack masks alongside data\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])   # [num_train, H, W]\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]]) # [num_train, H, W]\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])    # [num_test, H, W]\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])   # [num_test, H, W]\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,   # NEW\n",
    "            \"train_output_masks\": train_output_masks, # NEW\n",
    "            \"test_input_masks\": test_input_masks,     # NEW\n",
    "            \"test_output_masks\": test_output_masks,   # NEW\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------- main -----------\n",
    "# Point to your local folder named \"training\" (sibling/relative to where you run the script)\n",
    "folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "data = load_jsons_from_folder(folder_path)\n",
    "\n",
    "# add +1 to every grid value BEFORE padding\n",
    "_add_one_to_all_values_in_place(data)\n",
    "\n",
    "metrics = get_metrics(data)\n",
    "padded_data = pad_data(data, metrics, pad_value=0)\n",
    "sample_level = build_sample_level_dataset(padded_data, pad_value=0)\n",
    "arc_dataset = ARCSampleDataset(sample_list=sample_level)\n",
    "\n",
    "arc_loader = DataLoader(\n",
    "    arc_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=arc_collate_fn_bs1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# NEW: print *every* array for *every* sample (can be large!)\n",
    "for batch in arc_loader:\n",
    "    num_train = int(batch[\"train_inputs\"].shape[0])\n",
    "    num_test  = int(batch[\"test_inputs\"].shape[0])\n",
    "\n",
    "    # original (max over pairs before padding)\n",
    "    train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "    test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "    # new padded sizes (actual tensor shapes)\n",
    "    train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "    train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "    test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "    test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "    print(\"=== SUMMARY (single sample) ===\")\n",
    "    print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "    print(f\"Train original size (max): ({train_orig_h}, {train_orig_w})\")\n",
    "    print(f\"Train padded sizes — input: ({train_in_h}, {train_in_w}), output: ({train_out_h}, {train_out_w})\")\n",
    "    print(f\"Test  original size (max): ({test_orig_h}, {test_orig_w})\")\n",
    "    print(f\"Test  padded sizes — input: ({test_in_h}, {test_in_w}), output: ({test_out_h}, {test_out_w})\")\n",
    "\n",
    "    # one example from each split (if present)\n",
    "    if num_train > 0:\n",
    "        print(\"\\n--- Example TRAIN pair [0] ---\")\n",
    "        print(\"input:\", batch[\"train_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"train_outputs\"][0].tolist())\n",
    "    if num_test > 0:\n",
    "        print(\"\\n--- Example TEST pair [0] ---\")\n",
    "        print(\"input:\", batch[\"test_inputs\"][0].tolist())\n",
    "        print(\"output:\", batch[\"test_outputs\"][0].tolist())\n",
    "\n",
    "    break  # <<< ensure we only print once\n",
    "\n",
    "print(type(arc_loader))\n",
    "ChatGPT said:\n",
    "\n",
    "Thought for 10s\n",
    "Here’s your file rebuilt so the DataLoader comes from a class (ARCDataModule), while keeping your existing logic intact and the concise, single-sample summary at the end.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Load all JSONs (recursive)\n",
    "# ----------------------------\n",
    "def load_jsons_from_folder(dir_path):\n",
    "    \"\"\"\n",
    "    Read every .json file under dir_path (recursively) and return a dict\n",
    "    keyed by the file's relative path (without the .json extension).\n",
    "    \"\"\"\n",
    "    root = Path(dir_path).expanduser().resolve()\n",
    "    files = sorted(p for p in root.rglob(\"*.json\") if p.is_file())\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found under: {root}\")\n",
    "\n",
    "    data = {}\n",
    "    for p in files:\n",
    "        key = str(p.relative_to(root).with_suffix(\"\"))  # e.g. \"subdir/file\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data[key] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"Unable to load any .json files under: {root}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocess helpers\n",
    "# ----------------------------\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    NEW: also stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        idx = 0\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # ----- TEST -----\n",
    "        idx = 0\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # NEW: masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w)\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Torch dataset\n",
    "# ----------------------------\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # masks\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,\n",
    "            \"train_output_masks\": train_output_masks,\n",
    "            \"test_input_masks\": test_input_masks,\n",
    "            \"test_output_masks\": test_output_masks,\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# NEW: Data module wrapper\n",
    "# ----------------------------\n",
    "class ARCDataModule:\n",
    "    \"\"\"\n",
    "    Simple wrapper to produce a DataLoader from your folder.\n",
    "    Usage:\n",
    "        dm = ARCDataModule(\"~/path/to/training\").prepare()\n",
    "        loader = dm.get_loader()\n",
    "        for batch in loader: ...\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ):\n",
    "        self.dir_path = Path(dir_path).expanduser().resolve()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.dataset = None\n",
    "        self._loader = None\n",
    "\n",
    "    def prepare(self):\n",
    "        # load + preprocess\n",
    "        data = load_jsons_from_folder(self.dir_path)\n",
    "        _add_one_to_all_values_in_place(data)\n",
    "\n",
    "        # pad each sample independently (metric_dict unused)\n",
    "        padded = pad_data(data, metric_dict=None, pad_value=self.pad_value)\n",
    "        sample_list = build_sample_level_dataset(padded, pad_value=self.pad_value)\n",
    "\n",
    "        # build dataset + loader\n",
    "        self.dataset = ARCSampleDataset(sample_list=sample_list)\n",
    "        self._loader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            collate_fn=arc_collate_fn_bs1,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "        return self  # allow chaining\n",
    "\n",
    "    def get_loader(self):\n",
    "        if self._loader is None:\n",
    "            self.prepare()\n",
    "        return self._loader\n",
    "\n",
    "    # convenience so the module itself is iterable\n",
    "    def __iter__(self):\n",
    "        return iter(self.get_loader())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) if self.dataset is not None else 0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Point to your local folder named \"training\"\n",
    "    folder_path = Path(\"~/ARC-AGI-Model/src/data_pipeline/ARC_data/data/training\")\n",
    "\n",
    "    data_module = ARCDataModule(\n",
    "        dir_path=folder_path,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        pad_value=0,\n",
    "    ).prepare()\n",
    "\n",
    "    arc_loader = data_module.get_loader()\n",
    "\n",
    "    # Concise, single-sample summary to avoid IOPub overflow\n",
    "    for batch in arc_loader:\n",
    "        num_train = int(batch[\"train_inputs\"].shape[0])\n",
    "        num_test  = int(batch[\"test_inputs\"].shape[0])\n",
    "\n",
    "        # original (max over pairs before padding)\n",
    "        train_orig_h, train_orig_w = map(int, batch[\"train_original_size\"].tolist())\n",
    "        test_orig_h,  test_orig_w  = map(int, batch[\"test_original_size\"].tolist())\n",
    "\n",
    "        # new padded sizes (actual tensor shapes)\n",
    "        train_in_h, train_in_w   = batch[\"train_inputs\"].shape[1], batch[\"train_inputs\"].shape[2]\n",
    "        train_out_h, train_out_w = batch[\"train_outputs\"].shape[1], batch[\"train_outputs\"].shape[2]\n",
    "        test_in_h,  test_in_w    = batch[\"test_inputs\"].shape[1], batch[\"test_inputs\"].shape[2]\n",
    "        test_out_h, test_out_w   = batch[\"test_outputs\"].shape[1], batch[\"test_outputs\"].shape[2]\n",
    "\n",
    "        print(\"=== SUMMARY (single sample) ===\")\n",
    "        print(f\"#train: {num_train} | #test: {num_test}\")\n",
    "        print(f\"Train original size (max): ({train_orig_h}, {train_orig_w})\")\n",
    "        print(f\"Train padded sizes — input: ({train_in_h}, {train_in_w}), output: ({train_out_h}, {train_out_w})\")\n",
    "        print(f\"Test  original size (max): ({test_orig_h}, {test_orig_w})\")\n",
    "        print(f\"Test  padded sizes — input: ({test_in_h}, {test_in_w}), output: ({test_out_h}, {test_out_w})\")\n",
    "\n",
    "        if num_train > 0:\n",
    "            print(\"\\n--- Example TRAIN pair [0] ---\")\n",
    "            print(\"input:\", batch[\"train_inputs\"][0].tolist())\n",
    "            print(\"output:\", batch[\"train_outputs\"][0].tolist())\n",
    "        if num_test > 0:\n",
    "            print(\"\\n--- Example TEST pair [0] ---\")\n",
    "            print(\"input:\", batch[\"test_inputs\"][0].tolist())\n",
    "            print(\"output:\", batch[\"test_outputs\"][0].tolist())\n",
    "        break\n",
    "\n",
    "    print(type(arc_loader))\n",
    "\n",
    "\n",
    "No file chosenNo file chosen\n",
    "ChatGPT can make mistakes. OpenAI doesn't use Veterans United - Enterprise workspace data to train its models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdabd1-f773-4e93-a53b-3cf36412472b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
