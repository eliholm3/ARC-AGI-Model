{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98f0399-6eb6-4897-91ef-96dcb181f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train TRAIN] [--model-dir MODEL_DIR]\n",
      "                             [--batch-size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--pad-value PAD_VALUE] [--add-one]\n",
      "                             [--workers WORKERS] [--log-n LOG_N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/eliholm/.local/share/jupyter/runtime/kernel-60ca2500-d71d-4405-ac7e-3cd5eb96dfac.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eliholm/ARC-AGI-Model/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script adapts your original notebook code to run as a SageMaker *training entry point*.\n",
    "It works from a single notebook by saving this file (train.py) and launching a SageMaker\n",
    "PyTorch Estimator job. It supports:\n",
    "  • Reading data from either a LOCAL directory (recommended with input_mode=File/FastFile)\n",
    "    or directly from S3 (s3://bucket/prefix) using your job's IAM role credentials.\n",
    "  • Single- or multi-GPU/multi-node (PyTorch DDP) when used with SageMaker's distribution\n",
    "    configs. Only rank 0 prints to stdout to keep logs clean.\n",
    "  • Writing artifacts/metrics to SM_MODEL_DIR so they show up in the job's model tarball.\n",
    "\n",
    "Minimal launcher (in your notebook):\n",
    "\n",
    "    from sagemaker.pytorch import PyTorch\n",
    "    from sagemaker.inputs import TrainingInput\n",
    "\n",
    "    est = PyTorch(\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir=\".\",\n",
    "        role=role,\n",
    "        framework_version=\"2.3\",\n",
    "        py_version=\"py310\",\n",
    "        instance_type=\"ml.g5.2xlarge\",\n",
    "        instance_count=1,                       # >1 for multi-node\n",
    "        distribution={\"torch_distributed\": {\"enabled\": True}},  # enable DDP when count>1\n",
    "        hyperparameters={\"batch_size\": 4, \"epochs\": 1, \"add_one\": True, \"pad_value\": 0},\n",
    "        enable_sagemaker_metrics=True,\n",
    "    )\n",
    "    est.fit({\"train\": TrainingInput(s3_uri_to_your_jsons, input_mode=\"File\")})\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# --------------------- I/O helpers ---------------------\n",
    "\n",
    "def _is_s3_uri(p: str) -> bool:\n",
    "    return str(p).startswith(\"s3://\")\n",
    "\n",
    "\n",
    "def _list_local_jsons(dir_path: str):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        raise FileNotFoundError(f\"Local directory not found: {dir_path}\")\n",
    "    # non-recursive, mirror original behavior\n",
    "    files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.json') and os.path.isfile(os.path.join(dir_path, f))]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .json files found in {dir_path}\")\n",
    "    files.sort(key=lambda p: os.path.basename(p))\n",
    "    return files\n",
    "\n",
    "\n",
    "def _list_s3_jsons(s3_uri: str):\n",
    "    if not _is_s3_uri(s3_uri):\n",
    "        raise ValueError(\"Expected an S3 URI like s3://bucket/prefix/\")\n",
    "    o = urlparse(s3_uri)\n",
    "    bucket = o.netloc\n",
    "    prefix = o.path.lstrip('/')\n",
    "    if prefix and not prefix.endswith('/'):\n",
    "        prefix += '/'\n",
    "\n",
    "    s3 = boto3.client(\"s3\")  # uses job's IAM role\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter=\"/\")\n",
    "\n",
    "    keys = []\n",
    "    for page in pages:\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\".json\"):\n",
    "                keys.append(key)\n",
    "    if not keys:\n",
    "        raise FileNotFoundError(f\"No .json objects found under s3://{bucket}/{prefix}\")\n",
    "    keys.sort(key=lambda k: os.path.basename(k))\n",
    "    return [(bucket, k) for k in keys]\n",
    "\n",
    "\n",
    "def load_jsons_from_folder(path_or_s3: str):\n",
    "    \"\"\"\n",
    "    Load and validate your ARC-style JSONs from either a local directory or s3:// prefix.\n",
    "    Returns: dict[name -> parsed_json]\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "\n",
    "    if _is_s3_uri(path_or_s3):\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        for bucket, key in _list_s3_jsons(path_or_s3):\n",
    "            name = os.path.splitext(os.path.basename(key))[0]\n",
    "            uri = f\"s3://{bucket}/{key}\"\n",
    "            try:\n",
    "                body = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read().decode(\"utf-8\")\n",
    "                obj = json.loads(body)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to read:\", uri)\n",
    "                print(\" Error:\", e)\n",
    "                continue\n",
    "            if _valid_arc_obj(obj):\n",
    "                data[name] = obj\n",
    "            else:\n",
    "                print(\"Skipping (bad format):\", uri)\n",
    "    else:\n",
    "        for fpath in _list_local_jsons(path_or_s3):\n",
    "            name = os.path.splitext(os.path.basename(fpath))[0]\n",
    "            try:\n",
    "                with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    obj = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to read:\", fpath)\n",
    "                print(\" Error:\", e)\n",
    "                continue\n",
    "            if _valid_arc_obj(obj):\n",
    "                data[name] = obj\n",
    "            else:\n",
    "                print(\"Skipping (bad format):\", fpath)\n",
    "\n",
    "    if not data:\n",
    "        raise FileNotFoundError(\"No valid ARC jsons loaded.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def _valid_arc_obj(obj) -> bool:\n",
    "    if (\"train\" not in obj) or (\"test\" not in obj):\n",
    "        return False\n",
    "    for split in (\"train\", \"test\"):\n",
    "        if not isinstance(obj[split], list):\n",
    "            return False\n",
    "        for pairs in obj[split]:\n",
    "            if (\"input\" not in pairs) or (\"output\" not in pairs):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# --------------------- your original transforms ---------------------\n",
    "\n",
    "def _add_one_to_all_values_in_place(data):\n",
    "    \"\"\"\n",
    "    Adds +1 to every scalar value in each input/output grid across all samples.\n",
    "    Done BEFORE padding so pad_value=0 remains 0.\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for pairs in sample.get(split, []):\n",
    "                # input grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"input\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"input\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "                # output grid\n",
    "                r = 0\n",
    "                while r < len(pairs[\"output\"]):\n",
    "                    c = 0\n",
    "                    row = pairs[\"output\"][r]\n",
    "                    while c < len(row):\n",
    "                        row[c] = row[c] + 1\n",
    "                        c += 1\n",
    "                    r += 1\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    metric_dict = {\n",
    "        \"max_train_len\": 0,\n",
    "        \"max_test_len\": 0,\n",
    "        \"max_train_input_height\": 0,\n",
    "        \"max_test_input_height\": 0,\n",
    "        \"max_train_output_height\": 0,\n",
    "        \"max_test_output_height\": 0,\n",
    "        \"max_train_input_width\": 0,\n",
    "        \"max_test_input_width\": 0,\n",
    "        \"max_train_output_width\": 0,\n",
    "        \"max_test_output_width\": 0,\n",
    "    }\n",
    "\n",
    "    for sample in data.values():\n",
    "        if (len(sample['train']) > metric_dict['max_train_len']):\n",
    "            metric_dict['max_train_len'] = len(sample['train'])\n",
    "        if (len(sample['test']) > metric_dict['max_test_len']):\n",
    "            metric_dict['max_test_len'] = len(sample['test'])\n",
    "        for pairs in sample['train']:\n",
    "            if (len(pairs['input']) > metric_dict['max_train_input_height']):\n",
    "                metric_dict['max_train_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_train_output_height']):\n",
    "                metric_dict['max_train_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_train_input_width']):\n",
    "                    metric_dict['max_train_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_train_output_width']):\n",
    "                    metric_dict['max_train_output_width'] = len(output)\n",
    "        for pairs in sample['test']:\n",
    "            if (len(pairs['input']) > metric_dict['max_test_input_height']):\n",
    "                metric_dict['max_test_input_height'] = len(pairs['input'])\n",
    "            if (len(pairs['output']) > metric_dict['max_test_output_height']):\n",
    "                metric_dict['max_test_output_height'] = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if (len(inp) > metric_dict['max_test_input_width']):\n",
    "                    metric_dict['max_test_input_width'] = len(inp)\n",
    "            for output in pairs['output']:\n",
    "                if (len(output) > metric_dict['max_test_output_width']):\n",
    "                    metric_dict['max_test_output_width'] = len(output)\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def pad_data(data, metric_dict=None, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each sample independently to its own max square size.\n",
    "    metric_dict is ignored (kept for backward compatibility).\n",
    "    \"\"\"\n",
    "    for sample in data.values():\n",
    "        # ----- compute per-sample maxima for TRAIN -----\n",
    "        max_train_input_height = 0\n",
    "        max_train_input_width  = 0\n",
    "        max_train_output_height = 0\n",
    "        max_train_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('train', []):\n",
    "            if len(pairs['input'])  > max_train_input_height:  max_train_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_train_output_height: max_train_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_train_input_width:  max_train_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_train_output_width: max_train_output_width = len(outp)\n",
    "\n",
    "        # ----- compute per-sample maxima for TEST -----\n",
    "        max_test_input_height = 0\n",
    "        max_test_input_width  = 0\n",
    "        max_test_output_height = 0\n",
    "        max_test_output_width  = 0\n",
    "\n",
    "        for pairs in sample.get('test', []):\n",
    "            if len(pairs['input'])  > max_test_input_height:  max_test_input_height  = len(pairs['input'])\n",
    "            if len(pairs['output']) > max_test_output_height: max_test_output_height = len(pairs['output'])\n",
    "            for inp in pairs['input']:\n",
    "                if len(inp) > max_test_input_width:  max_test_input_width  = len(inp)\n",
    "            for outp in pairs['output']:\n",
    "                if len(outp) > max_test_output_width: max_test_output_width = len(outp)\n",
    "\n",
    "        # ----- per-sample square sizes -----\n",
    "        max_train_size = max(\n",
    "            max_train_input_height,\n",
    "            max_train_input_width,\n",
    "            max_train_output_height,\n",
    "            max_train_output_width\n",
    "        )\n",
    "        max_test_size = max(\n",
    "            max_test_input_height,\n",
    "            max_test_input_width,\n",
    "            max_test_output_height,\n",
    "            max_test_output_width\n",
    "        )\n",
    "\n",
    "        # ----- pad TRAIN for this sample -----\n",
    "        for pairs in sample.get('train', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_train_size:\n",
    "                pairs['input'].append([pad_value] * max_train_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_train_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_train_size:\n",
    "                pairs['output'].append([pad_value] * max_train_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_train_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "        # ----- pad TEST for this sample -----\n",
    "        for pairs in sample.get('test', []):\n",
    "            # input\n",
    "            while len(pairs['input']) < max_test_size:\n",
    "                pairs['input'].append([pad_value] * max_test_size)\n",
    "            for inp in pairs['input']:\n",
    "                while len(inp) < max_test_size:\n",
    "                    inp.append(pad_value)\n",
    "            # output\n",
    "            while len(pairs['output']) < max_test_size:\n",
    "                pairs['output'].append([pad_value] * max_test_size)\n",
    "            for outp in pairs['output']:\n",
    "                while len(outp) < max_test_size:\n",
    "                    outp.append(pad_value)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _infer_original_size_from_padded(grid, pad_value=0):\n",
    "    h = 0\n",
    "    w = 0\n",
    "    r = 0\n",
    "    while r < len(grid):\n",
    "        row = grid[r]\n",
    "        any_nonpad = False\n",
    "        last_nonpad = -1\n",
    "        c = 0\n",
    "        while c < len(row):\n",
    "            if row[c] != pad_value:\n",
    "                any_nonpad = True\n",
    "                last_nonpad = c\n",
    "            c += 1\n",
    "        if any_nonpad:\n",
    "            if (r + 1) > h:\n",
    "                h = r + 1\n",
    "            if (last_nonpad + 1) > w:\n",
    "                w = last_nonpad + 1\n",
    "        r += 1\n",
    "    return (h, w)\n",
    "\n",
    "\n",
    "def build_sample_level_dataset(data, pad_value=0):\n",
    "    \"\"\"\n",
    "    Build a list of per-sample records.\n",
    "    Stores per-pair masks: 1 where value != pad_value, else 0.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for sample_name, sample in data.items():\n",
    "        # containers\n",
    "        train_pairs = []\n",
    "        test_pairs = []\n",
    "\n",
    "        # track original (unpadded) sizes per split\n",
    "        train_max_h = 0\n",
    "        train_max_w = 0\n",
    "        test_max_h = 0\n",
    "        test_max_w = 0\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        for pairs in sample['train']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            # original sizes (prefer stored, else infer)\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            # update split-wide original size (max over inputs/outputs)\n",
    "            if in_h > train_max_h: train_max_h = in_h\n",
    "            if out_h > train_max_h: train_max_h = out_h\n",
    "            if in_w > train_max_w: train_max_w = in_w\n",
    "            if out_w > train_max_w: train_max_w = out_w\n",
    "\n",
    "            # tensors\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            # masks (1 for non-pad, 0 for pad)\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            # store pair\n",
    "            train_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask,\n",
    "            })\n",
    "\n",
    "        # ----- TEST -----\n",
    "        for pairs in sample['test']:\n",
    "            inp_grid = pairs['input']\n",
    "            out_grid = pairs['output']\n",
    "\n",
    "            if ('orig_input_size' in pairs):\n",
    "                in_h, in_w = pairs['orig_input_size']\n",
    "            else:\n",
    "                in_h, in_w = _infer_original_size_from_padded(inp_grid, pad_value)\n",
    "            if ('orig_output_size' in pairs):\n",
    "                out_h, out_w = pairs['orig_output_size']\n",
    "            else:\n",
    "                out_h, out_w = _infer_original_size_from_padded(out_grid, pad_value)\n",
    "\n",
    "            if in_h > test_max_h: test_max_h = in_h\n",
    "            if out_h > test_max_h: test_max_h = out_h\n",
    "            if in_w > test_max_w: test_max_w = in_w\n",
    "            if out_w > test_max_w: test_max_w = out_w\n",
    "\n",
    "            inp_tensor = torch.tensor(inp_grid).long()\n",
    "            out_tensor = torch.tensor(out_grid).long()\n",
    "\n",
    "            inp_mask = (inp_tensor != pad_value).long()\n",
    "            out_mask = (out_tensor != pad_value).long()\n",
    "\n",
    "            test_pairs.append({\n",
    "                \"input\": inp_tensor,\n",
    "                \"output\": out_tensor,\n",
    "                \"input_mask\": inp_mask,\n",
    "                \"output_mask\": out_mask,\n",
    "            })\n",
    "\n",
    "        # assemble sample-level record\n",
    "        item = {\n",
    "            \"id\": str(sample_name),\n",
    "            \"train_pairs\": train_pairs,\n",
    "            \"test_pairs\": test_pairs,\n",
    "            \"train_original_size\": (train_max_h, train_max_w),\n",
    "            \"test_original_size\": (test_max_h, test_max_w),\n",
    "        }\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ARCSampleDataset(Dataset):\n",
    "    def __init__(self, sample_list):\n",
    "        self.data = sample_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # stack per-sample pairs into tensors\n",
    "        train_inputs = torch.stack([p[\"input\"] for p in sample[\"train_pairs\"]])      # [num_train, H, W]\n",
    "        train_outputs = torch.stack([p[\"output\"] for p in sample[\"train_pairs\"]])    # [num_train, H, W]\n",
    "        test_inputs = torch.stack([p[\"input\"] for p in sample[\"test_pairs\"]])        # [num_test, H, W]\n",
    "        test_outputs = torch.stack([p[\"output\"] for p in sample[\"test_pairs\"]])      # [num_test, H, W]\n",
    "\n",
    "        # masks\n",
    "        train_input_masks = torch.stack([p[\"input_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        train_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"train_pairs\"]])\n",
    "        test_input_masks  = torch.stack([p[\"input_mask\"] for p in sample[\"test_pairs\"]])\n",
    "        test_output_masks = torch.stack([p[\"output_mask\"] for p in sample[\"test_pairs\"]])\n",
    "\n",
    "        return {\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"train_inputs\": train_inputs,\n",
    "            \"train_outputs\": train_outputs,\n",
    "            \"test_inputs\": test_inputs,\n",
    "            \"test_outputs\": test_outputs,\n",
    "            \"train_input_masks\": train_input_masks,\n",
    "            \"train_output_masks\": train_output_masks,\n",
    "            \"test_input_masks\": test_input_masks,\n",
    "            \"test_output_masks\": test_output_masks,\n",
    "            \"train_original_size\": torch.tensor(sample[\"train_original_size\"], dtype=torch.long),\n",
    "            \"test_original_size\": torch.tensor(sample[\"test_original_size\"], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def arc_collate_fn_bs1(batch):\n",
    "    # batch size is guaranteed to be 1; return the single dict unchanged\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "# --------------------- training entry point ---------------------\n",
    "\n",
    "def init_distributed_if_needed():\n",
    "    \"\"\"Initialize torch.distributed if launched with torchrun (SageMaker DDP).\"\"\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "    if world_size > 1:\n",
    "        backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(local_rank)\n",
    "        torch.distributed.init_process_group(backend=backend, init_method=\"env://\")\n",
    "    return world_size, rank, local_rank\n",
    "\n",
    "\n",
    "def is_primary(rank: int) -> bool:\n",
    "    return rank == 0\n",
    "\n",
    "\n",
    "def save_json(obj, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", \"\"),\n",
    "                        help=\"Path to training data directory (local) or s3:// prefix\")\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=1)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--pad-value\", type=int, default=0)\n",
    "    parser.add_argument(\"--add-one\", dest=\"add_one\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--workers\", type=int, default=max(0, (os.cpu_count() or 2)//2))\n",
    "    parser.add_argument(\"--log-n\", type=int, default=2, help=\"Log first N samples only (rank 0)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size, rank, local_rank = init_distributed_if_needed()\n",
    "\n",
    "    def log(*a, **k):\n",
    "        if is_primary(rank):\n",
    "            print(*a, **k, flush=True)\n",
    "\n",
    "    log(\"=== Config ===\")\n",
    "    log(vars(args))\n",
    "    log(f\"world_size={world_size} rank={rank} local_rank={local_rank} gpu={torch.cuda.is_available()}\")\n",
    "\n",
    "    # ----------------- Load & preprocess -----------------\n",
    "    if not args.train:\n",
    "        raise ValueError(\"--train path was not provided and SM_CHANNEL_TRAIN is empty.\")\n",
    "\n",
    "    data = load_jsons_from_folder(args.train)\n",
    "\n",
    "    if args.add_one:\n",
    "        _add_one_to_all_values_in_place(data)\n",
    "\n",
    "    metrics = get_metrics(data)\n",
    "    padded_data = pad_data(data, metrics, pad_value=args.pad_value)\n",
    "    sample_level = build_sample_level_dataset(padded_data, pad_value=args.pad_value)\n",
    "\n",
    "    # Persist a tiny metrics file for inspection\n",
    "    if is_primary(rank):\n",
    "        save_json({\"metrics\": metrics, \"num_samples\": len(sample_level)}, os.path.join(args.model_dir, \"preprocess_metrics.json\"))\n",
    "\n",
    "    # ----------------- DataLoader -----------------\n",
    "    ds = ARCSampleDataset(sample_list=sample_level)\n",
    "    # Pin memory helps if using GPU; persistent workers when workers>0\n",
    "    pin = torch.cuda.is_available()\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=arc_collate_fn_bs1,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=pin,\n",
    "        persistent_workers=(args.workers > 0),\n",
    "        prefetch_factor=2 if args.workers > 0 else None,\n",
    "    )\n",
    "\n",
    "    # ----------------- (Placeholder) Training Loop -----------------\n",
    "    # This script focuses on data preparation/ingest; plug in your model below.\n",
    "    # We just iterate a few samples to validate the pipeline and save shapes.\n",
    "    seen = 0\n",
    "    shapes = []\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in loader:\n",
    "            if seen < args.log_n and is_primary(rank):\n",
    "                log(\"=== SAMPLE ===\")\n",
    "                log(\"ID:\", batch[\"id\"]) \n",
    "                log(\"train_inputs:\", tuple(batch[\"train_inputs\"].shape))\n",
    "                log(\"train_outputs:\", tuple(batch[\"train_outputs\"].shape))\n",
    "                log(\"test_inputs:\", tuple(batch[\"test_inputs\"].shape))\n",
    "                log(\"test_outputs:\", tuple(batch[\"test_outputs\"].shape))\n",
    "                log(\"train_original_size:\", batch[\"train_original_size\"].tolist())\n",
    "                log(\"test_original_size:\", batch[\"test_original_size\"].tolist())\n",
    "            shapes.append({\n",
    "                \"id\": batch[\"id\"],\n",
    "                \"train_inputs\": list(batch[\"train_inputs\"].shape),\n",
    "                \"train_outputs\": list(batch[\"train_outputs\"].shape),\n",
    "                \"test_inputs\": list(batch[\"test_inputs\"].shape),\n",
    "                \"test_outputs\": list(batch[\"test_outputs\"].shape),\n",
    "            })\n",
    "            seen += 1\n",
    "\n",
    "    if is_primary(rank):\n",
    "        save_json({\"sample_shapes\": shapes[: args.log_n]}, os.path.join(args.model_dir, \"sample_shapes.json\"))\n",
    "        # drop a tiny file to ensure model artifact is produced\n",
    "        with open(os.path.join(args.model_dir, \"_SUCCESS\"), \"w\") as f:\n",
    "            f.write(\"ok\\n\")\n",
    "\n",
    "    # Clean up DDP\n",
    "    if world_size > 1:\n",
    "        torch.distributed.barrier()\n",
    "        torch.distributed.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85de6f-b89c-43c5-a0a0-2d0a9d51c0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
